{
  "hash": "d29596a26fddb72a8f523eedc4a1932e",
  "result": {
    "markdown": "\n\n\n# Boosted gradient trees\n\nBoosted gradient machines achieve currently state-of-the-art performance for structured (tabular) data which makes them probably one of the most important algorithms for E&E where structured data dominates the field.\n\nIn the following, we use the 'xgboost' package (@xgboost).\n\n## Classification\n\n::: panel-tabset\n### R\n\n\n::: {.cell hash='brt_cache/html/unnamed-chunk-1_6c4389d6cfd0ca6b2127f3da8a1bff85'}\n\n```{.r .cell-code}\nlibrary(xgboost)\nX = as.matrix(iris[,1:4])\nY = as.integer(iris[,5]) - 1 # classes must be integers starting from 0\n\nxgdata = xgb.DMatrix(X, label = Y)\n\n# nrounds = number of trees in the ensemble\nbrt = xgboost(data = xgdata, \n              objective=\"multi:softprob\", \n              nrounds = 50, \n              num_class = 3,\n              verbose = 0)\n```\n:::\n\n\nShow feature importances:\n\n\n::: {.cell hash='brt_cache/html/unnamed-chunk-2_d719ec374fc2683a247daa1d83575949'}\n\n```{.r .cell-code}\nxgb.importance(model = brt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Feature        Gain      Cover Frequency\n1: Petal.Length 0.671879438 0.57441039 0.3792049\n2:  Petal.Width 0.311535837 0.29261084 0.3088685\n3:  Sepal.Width 0.010177107 0.04910115 0.1162080\n4: Sepal.Length 0.006407618 0.08387763 0.1957187\n```\n:::\n:::\n\n\nMake predictions (class probabilities):\n\n\n::: {.cell hash='brt_cache/html/unnamed-chunk-3_206e27fa2a6d544315d06fe9d5d60ef2'}\n\n```{.r .cell-code}\nhead(matrix(predict(brt, newdata = xgb.DMatrix(X)), ncol =3), n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]        [,2]        [,3]\n[1,] 0.995287061 0.002195822 0.001027058\n[2,] 0.003323558 0.995396435 0.001592265\n[3,] 0.001389398 0.002407764 0.997380674\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell hash='brt_cache/html/unnamed-chunk-4_28712719e6ebe5d8898bfb6bf452e98f'}\n\n```{.python .cell-code}\nimport xgboost as xgb\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\n# Parameters:\nparam = {\n  'max_depth':2, \n  'eta':1, \n  'objective':'multi:softmax' }\nnum_round = 50\n\nmodel = xgb.XGBClassifier(param, num_round, verbosity = 0).fit(X, Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n/home/maxpichler/miniconda3/envs/r-sjsdm/lib/python3.7/site-packages/xgboost/core.py:502: FutureWarning: Pass `objective, use_label_encoder` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n  format(\", \".join(args_msg)), FutureWarning\n/home/maxpichler/miniconda3/envs/r-sjsdm/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n```\n:::\n:::\n\n\nFeature importance\n\n\n::: {.cell hash='brt_cache/html/unnamed-chunk-5_4ac43594395affe8df6a3f65b3a0c723'}\n\n```{.python .cell-code}\nmodel.feature_importances_\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([0.00959796, 0.01645038, 0.6765859 , 0.29736578], dtype=float32)\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell hash='brt_cache/html/unnamed-chunk-6_7fd0f6458ab6464a3c2ea3ee6586f686'}\n\n```{.python .cell-code}\nmodel.predict_proba(X)[0:10,:]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9636227e-01, 2.3820533e-03, 1.2557388e-03],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9679452e-01, 2.3830866e-03, 8.2237815e-04],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9636227e-01, 2.3820533e-03, 1.2557388e-03],\n       [9.9679452e-01, 2.3830866e-03, 8.2237815e-04]], dtype=float32)\n```\n:::\n:::\n\n:::\n\n## Regression\n\n::: panel-tabset\n### R\n\n\n::: {.cell hash='brt_cache/html/unnamed-chunk-7_fb45fe7c86b579fc51449743c23aae3b'}\n\n```{.r .cell-code}\nlibrary(xgboost)\nX = as.matrix(iris[,2:4])\nY = iris[,1]\n\nxgdata = xgb.DMatrix(X, label = Y)\n\n# nrounds = number of trees in the ensemble\nbrt = xgboost(data = xgdata, \n              objective=\"reg:squarederror\", \n              nrounds = 50, \n              verbose = 0)\n```\n:::\n\n\nShow feature importances:\n\n\n::: {.cell hash='brt_cache/html/unnamed-chunk-8_4310c1e6a9065de65b8fc526de88e62a'}\n\n```{.r .cell-code}\nxgb.importance(model = brt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Feature       Gain     Cover Frequency\n1: Petal.Length 0.86781219 0.4789538 0.3789062\n2:  Petal.Width 0.06987880 0.2128402 0.2626953\n3:  Sepal.Width 0.06230901 0.3082060 0.3583984\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell hash='brt_cache/html/unnamed-chunk-9_b58025580387ff36670b630d53b67b25'}\n\n```{.r .cell-code}\nhead(predict(brt, newdata = xgb.DMatrix(X), n = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[10:50:56] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.506606 3.506606 3.506606 3.506606 3.506606 3.506606\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell hash='brt_cache/html/unnamed-chunk-10_c45490d0938217cc368a1df78fb9d9a2'}\n\n```{.python .cell-code}\nimport xgboost as xgb\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\n# Parameters:\nmodel = xgb.XGBRegressor(\n  objective = 'reg:squarederror',\n  max_depth = 2,  \n  n_estimators = 50, \n  verbosity = 0).fit(X, Y)\n```\n:::\n\n\nFeature importance:\n\n\n::: {.cell hash='brt_cache/html/unnamed-chunk-11_e7b994104045ae51fa63e97f0dc94c88'}\n\n```{.python .cell-code}\nprint(model.feature_importances_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.08471056 0.835755   0.07953447]\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell hash='brt_cache/html/unnamed-chunk-12_ac54bdca11f760968e0bef8d94a6eb1f'}\n\n```{.python .cell-code}\nmodel.predict(X)[0:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([5.0407157, 4.6844926, 4.711238 , 4.917956 , 5.0407157, 5.450946 ,\n       4.928966 , 4.986462 , 4.6750975, 4.917956 ], dtype=float32)\n```\n:::\n:::\n\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}