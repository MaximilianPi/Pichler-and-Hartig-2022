{
  "hash": "1bdfebca8d34bc4c89038c68ae6cffe3",
  "result": {
    "markdown": "# Boosted gradient trees\n\n\n\n\n\n\nBoosted gradient machines achieve currently state-of-the-art performance for structured (tabular) data which makes them probably one of the most important algorithms for E&E where structured data dominates the field.\n\nIn the following, we use the 'xgboost' package (@xgboost) (Python: 'xgboost' (@xgboost), Julia: 'MLJ' (@anthony_blaom_2019_3541506))\n\n## Classification\n\n::: panel-tabset\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(xgboost)\nX = as.matrix(iris[,1:4])\nY = as.integer(iris[,5]) - 1 # classes must be integers starting from 0\n\nxgdata = xgb.DMatrix(X, label = Y)\n\n# nrounds = number of trees in the ensemble\nbrt = xgboost(data = xgdata, \n              objective=\"multi:softprob\", \n              nrounds = 50, \n              num_class = 3,\n              verbose = 0)\n```\n:::\n\n\nShow feature importances:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb.importance(model = brt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Feature        Gain      Cover Frequency\n1: Petal.Length 0.671879438 0.57441039 0.3792049\n2:  Petal.Width 0.311535837 0.29261084 0.3088685\n3:  Sepal.Width 0.010177107 0.04910115 0.1162080\n4: Sepal.Length 0.006407618 0.08387763 0.1957187\n```\n:::\n:::\n\n\nMake predictions (class probabilities):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(matrix(predict(brt, newdata = xgb.DMatrix(X)), ncol =3), n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]        [,2]        [,3]\n[1,] 0.995287061 0.002195822 0.001027058\n[2,] 0.003323558 0.995396435 0.001592265\n[3,] 0.001389398 0.002407764 0.997380674\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport xgboost as xgb\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\n# Parameters:\nparam = {\n  'max_depth':2, \n  'eta':1, \n  'objective':'multi:softmax' }\nnum_round = 50\n\nmodel = xgb.XGBClassifier(param, num_round, verbosity = 0).fit(X, Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n/home/max/miniconda3/envs/r-sjsdm/lib/python3.9/site-packages/xgboost/core.py:568: FutureWarning: Pass `objective, use_label_encoder` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n  warnings.warn(\n```\n:::\n:::\n\n\nFeature importance\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel.feature_importances_\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([0.00959796, 0.01645038, 0.6765859 , 0.29736578], dtype=float32)\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel.predict_proba(X)[0:10,:]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9636227e-01, 2.3820533e-03, 1.2557388e-03],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9679452e-01, 2.3830866e-03, 8.2237815e-04],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9636227e-01, 2.3820533e-03, 1.2557388e-03],\n       [9.9679452e-01, 2.3830866e-03, 8.2237815e-04]], dtype=float32)\n```\n:::\n:::\n\n\n### Julia\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nimport StatsBase;\nusing MLJ;\nBRT_classifier = @load XGBoostClassifier pkg=XGBoost;\nusing RDatasets;\nusing StatsBase;\nusing DataFrames;\n```\n:::\n\n::: {.cell}\n\n```{.julia .cell-code}\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 1:4]);\nY = iris[:, 5];\n```\n:::\n\n\nModels:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nmodel = fit!(machine(BRT_classifier(), X, Y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrained Machine; caches model-specific representations of data\n  model: XGBoostClassifier(num_round = 100, …)\n  args: \n    1:\tSource @086 ⏎ Table{AbstractVector{Continuous}}\n    2:\tSource @683 ⏎ AbstractVector{Multiclass{3}}\n```\n:::\n:::\n\n\n\nPredictions:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nMLJ.predict(model, X)[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float32}:\n UnivariateFinite{Multiclass{3}}(setosa=>0.997, versicolor=>0.00238, virginica=>0.000814)\n UnivariateFinite{Multiclass{3}}(setosa=>0.996, versicolor=>0.00238, virginica=>0.00126)\n UnivariateFinite{Multiclass{3}}(setosa=>0.997, versicolor=>0.00238, virginica=>0.000814)\n UnivariateFinite{Multiclass{3}}(setosa=>0.997, versicolor=>0.00238, virginica=>0.000822)\n UnivariateFinite{Multiclass{3}}(setosa=>0.997, versicolor=>0.00238, virginica=>0.000814)\n```\n:::\n:::\n\n\n\n:::\n\n## Regression\n\n::: panel-tabset\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(xgboost)\nX = as.matrix(iris[,2:4])\nY = iris[,1]\n\nxgdata = xgb.DMatrix(X, label = Y)\n\n# nrounds = number of trees in the ensemble\nbrt = xgboost(data = xgdata, \n              objective=\"reg:squarederror\", \n              nrounds = 50, \n              verbose = 0)\n```\n:::\n\n\nShow feature importances:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb.importance(model = brt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Feature       Gain     Cover Frequency\n1: Petal.Length 0.86781219 0.4789538 0.3789062\n2:  Petal.Width 0.06987880 0.2128402 0.2626953\n3:  Sepal.Width 0.06230901 0.3082060 0.3583984\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(predict(brt, newdata = xgb.DMatrix(X), n = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[15:06:18] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.506606 3.506606 3.506606 3.506606 3.506606 3.506606\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport xgboost as xgb\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\n# Parameters:\nmodel = xgb.XGBRegressor(\n  objective = 'reg:squarederror',\n  max_depth = 2,  \n  n_estimators = 50, \n  verbosity = 0).fit(X, Y)\n```\n:::\n\n\nFeature importance:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(model.feature_importances_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.08471056 0.835755   0.07953447]\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel.predict(X)[0:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([5.0407157, 4.6844926, 4.711238 , 4.917956 , 5.0407157, 5.450946 ,\n       4.928966 , 4.986462 , 4.6750975, 4.917956 ], dtype=float32)\n```\n:::\n:::\n\n\n### Julia\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nimport StatsBase;\nusing MLJ;\nBRT_regressor =  @load XGBoostRegressor pkg=XGBoost;\nusing RDatasets;\nusing DataFrames;\n```\n:::\n\n::: {.cell}\n\n```{.julia .cell-code}\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 2:4]);\nY = iris[:, 1];\n```\n:::\n\n\nModel:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nmodel = fit!(machine(BRT_regressor(), X, Y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrained Machine; caches model-specific representations of data\n  model: XGBoostRegressor(num_round = 100, …)\n  args: \n    1:\tSource @893 ⏎ Table{AbstractVector{Continuous}}\n    2:\tSource @522 ⏎ AbstractVector{Continuous}\n```\n:::\n:::\n\n\n\nPredictions:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nMLJ.predict(model, X)[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5-element Vector{Float32}:\n 5.1509466\n 4.8569074\n 4.551141\n 4.7587333\n 4.999504\n```\n:::\n:::\n\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}