{
  "hash": "55561e3337554cf99b84187074aa3c41",
  "result": {
    "markdown": "# Deep neural networks\n\n\n\n\n\n\nDeep neural networks, or more precisely here fully connected neural networks, can be flexibly built which makes their application more challenging than other ML algorithms.\n\nIn the following, we use the 'keras' (@kerasR; @chollet2015keras) (Python: 'keras' (@chollet2015keras); Julia: 'Flux' (@Flux)) package which is a higher level API on the python 'tensorflow' framework (@abadi2016tensorflow).\n\n## Classification\n\n::: panel-tabset\n### R\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-2_bdcaff32021054a807f3abadc0f2b2c9'}\n\n```{.r .cell-code}\nlibrary(keras)\nX = scale(as.matrix(iris[,1:4]))\nY = as.integer(iris$Species)\n# We need to one hot encode our response classes\nYT = k_one_hot(Y-1L, num_classes = 3)\n\nDNN = keras_model_sequential() %>% \n  # first hidden layer\n  layer_dense(input_shape = ncol(X), \n              units = 10, \n              activation = \"relu\") %>% \n  # second hidden layer with regularization\n  layer_dense(units = 20, \n              activation = \"relu\",\n              kernel_regularizer = regularizer_l1()) %>% \n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\n  layer_dense(units = 3, \n              activation = \"softmax\")\n\n# print architecture\nsummary(DNN)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_2 (Dense)                    (None, 10)                      50          \n dense_1 (Dense)                    (None, 20)                      220         \n dense (Dense)                      (None, 3)                       63          \n================================================================================\nTotal params: 333\nTrainable params: 333\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# add loss function and optimizer\nDNN %>% \n  compile(loss = loss_categorical_crossentropy,\n          optimizer = optimizer_adamax(0.01))\n\n# train model\nDNN %>% \n  fit(X, YT, epochs = 50, verbose = 0)\n```\n:::\n\n\nMake predictions (class probabilities):\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-3_f818dc2714c7ba77323ce62630e1ba9d'}\n\n```{.r .cell-code}\nhead(predict(DNN, X), n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]        [,2]         [,3]\n[1,] 0.9943717 0.004811807 0.0008164378\n[2,] 0.9866450 0.011190814 0.0021641329\n[3,] 0.9952909 0.003913700 0.0007954266\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-4_df8289051237c9f7e024872ae18390e3'}\n\n```{.python .cell-code}\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\n# We need to one hot encode our response classes\nYT = keras.utils.to_categorical(Y, num_classes = 3)\n\nDNN = keras.Sequential()\n  # first hidden layer\nDNN.add(Dense(\n  input_shape=[X.shape[1]], \n  units = 10, \n  activation = \"relu\")) \n  # second hidden layer with regularization\nDNN.add(Dense(\n  units = 20, \n  activation = \"relu\",\n  kernel_regularizer = keras.regularizers.l1()))\n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\nDNN.add(Dense(\n  units = 3, \n  activation = \"softmax\"))\n\n# print architecture\nDNN.summary()\n\n# add loss function and optimizer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_3 (Dense)             (None, 10)                50        \n                                                                 \n dense_4 (Dense)             (None, 20)                220       \n                                                                 \n dense_5 (Dense)             (None, 3)                 63        \n                                                                 \n=================================================================\nTotal params: 333\nTrainable params: 333\nNon-trainable params: 0\n_________________________________________________________________\n```\n:::\n\n```{.python .cell-code}\nDNN.compile(loss = keras.losses.categorical_crossentropy,\n            optimizer = keras.optimizers.Adamax(0.01))\n\n# train model\nDNN.fit(X, YT, epochs = 50, verbose = 0)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.callbacks.History object at 0x7fb0896b0850>\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-5_cb8ecdf04ca5c9aec17796088f6a7a9c'}\n\n```{.python .cell-code}\nDNN.predict(X)[0:10,:]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n1/5 [=====>........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n5/5 [==============================] - 0s 680us/step\narray([[9.9583131e-01, 4.1106688e-03, 5.8080172e-05],\n       [9.8362643e-01, 1.6319780e-02, 5.3694650e-05],\n       [9.9434531e-01, 5.6184102e-03, 3.6299647e-05],\n       [9.9109250e-01, 8.8622421e-03, 4.5296307e-05],\n       [9.9725479e-01, 2.6933698e-03, 5.1819748e-05],\n       [9.9600965e-01, 3.8014781e-03, 1.8886165e-04],\n       [9.9641293e-01, 3.5384791e-03, 4.8511523e-05],\n       [9.9425328e-01, 5.6840330e-03, 6.2594314e-05],\n       [9.8860162e-01, 1.1364626e-02, 3.3743974e-05],\n       [9.8900944e-01, 1.0945212e-02, 4.5381825e-05]], dtype=float32)\n```\n:::\n:::\n\n\n### Julia\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-6_e71920c76a09487951dc7c59b5694814'}\n\n```{.julia .cell-code}\nimport StatsBase\nusing RDatasets\nusing StatsBase\nusing DataFrames\nimport MLJBase.int\nusing Flux, Statistics\nusing Flux.Data: DataLoader\nusing Flux: onehotbatch, onecold, @epochs\nusing Flux.Losses: logitcrossentropy\n```\n:::\n\n\nData preparation:\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-7_a9b35f0cc181e23e333fb9d1a23718de'}\n\n```{.julia .cell-code}\niris = dataset(\"datasets\", \"iris\");\nX = transpose(Matrix(mapcols(StatsBase.zscore, iris[:, 1:4])));\nY = int(iris[:, 5], type = Int);\nclasses = sort(unique(Y));\nYT = onehotbatch(Y, classes);\ndata_loader = DataLoader((X, YT), batchsize=10, shuffle=true);\n```\n:::\n\n\nCreate model (similar to Keras):\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-8_0bb2311dfdfaa6ddf3d81ca0bb937ff1'}\n\n```{.julia .cell-code}\nmodel = Chain(\n  Dense(4, 20, relu),\n  Dense(20, 20, relu),\n  Dense(20, 3)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChain(\n  Dense(4 => 20, relu),                 # 100 parameters\n  Dense(20 => 20, relu),                # 420 parameters\n  Dense(20 => 3),                       # 63 parameters\n)                   # Total: 6 arrays, 583 parameters, 2.652 KiB.\n```\n:::\n:::\n\n\nTrain/optimize Model:\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-9_def010d895779ec0cc2d942727853b9c'}\n\n```{.julia .cell-code}\nparameters = Flux.params(model);\noptimizer = ADAM(0.01);\n\n# Help functions\nloss(x, y) = logitcrossentropy(model(x), y);\n\nget_loss() = @show sum(logitcrossentropy(model(X), YT));\n\n## Training\nfor epoch in 1:20\n  Flux.train!(loss, parameters, data_loader, optimizer, cb = Flux.throttle(get_loss, 5))\nend\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsum(logitcrossentropy(model(X), YT)) = 1.0847621827469989\nsum(logitcrossentropy(model(X), YT)) = 0.41861586893929087\nsum(logitcrossentropy(model(X), YT)) = 0.28409296777747656\nsum(logitcrossentropy(model(X), YT)) = 0.19468215242911596\nsum(logitcrossentropy(model(X), YT)) = 0.13160069597338106\nsum(logitcrossentropy(model(X), YT)) = 0.09292801366692326\nsum(logitcrossentropy(model(X), YT)) = 0.08476726510840268\nsum(logitcrossentropy(model(X), YT)) = 0.07357130215647495\nsum(logitcrossentropy(model(X), YT)) = 0.05955705216439545\nsum(logitcrossentropy(model(X), YT)) = 0.06489229857135734\nsum(logitcrossentropy(model(X), YT)) = 0.04552515538160692\nsum(logitcrossentropy(model(X), YT)) = 0.04941019442329357\nsum(logitcrossentropy(model(X), YT)) = 0.04051517193299854\nsum(logitcrossentropy(model(X), YT)) = 0.05666814742618713\nsum(logitcrossentropy(model(X), YT)) = 0.05791907364710918\nsum(logitcrossentropy(model(X), YT)) = 0.04238850341678316\nsum(logitcrossentropy(model(X), YT)) = 0.08791328847980895\nsum(logitcrossentropy(model(X), YT)) = 0.047533954738316425\nsum(logitcrossentropy(model(X), YT)) = 0.044352655498664274\nsum(logitcrossentropy(model(X), YT)) = 0.0397672009806662\n```\n:::\n:::\n\n\nPredictions:\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-10_ad869418105438cc29354026ae25a9dd'}\n\n```{.julia .cell-code}\ntranspose(softmax(model(X)))[1:5,:]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5Ã—3 Matrix{Float64}:\n 0.999997  2.61079e-6  1.15129e-11\n 0.999971  2.85092e-5  2.56738e-10\n 0.999999  1.29144e-6  1.78573e-11\n 0.999998  2.14865e-6  5.09192e-11\n 0.999999  9.7007e-7   3.61251e-12\n```\n:::\n:::\n\n\n\n:::\n\n## Regression\n\n::: panel-tabset\n### R\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-11_64ea00cb1beb237958cac1b1413693b0'}\n\n```{.r .cell-code}\nlibrary(keras)\nX = scale(as.matrix(iris[,2:4]))\nY = as.matrix(iris[,1,drop=FALSE])\n\nDNN = keras_model_sequential() %>% \n  # first hidden layer\n  layer_dense(input_shape = ncol(X), \n              units = 10, \n              activation = \"relu\") %>% \n  # second hidden layer with regularization\n  layer_dense(units = 20, \n              activation = \"relu\",\n              kernel_regularizer = regularizer_l1()) %>% \n  # output layer, one output neuron for one response\n  # and no activation function\n  layer_dense(units = 1)\n\n# print architecture\nsummary(DNN)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_2\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_8 (Dense)                    (None, 10)                      40          \n dense_7 (Dense)                    (None, 20)                      220         \n dense_6 (Dense)                    (None, 1)                       21          \n================================================================================\nTotal params: 281\nTrainable params: 281\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# add loss function and optimizer\nDNN %>% \n  compile(loss = loss_mean_squared_error,\n          optimizer = optimizer_adamax(0.01))\n\n# train model\nDNN %>% \n  fit(X, YT, epochs = 50, verbose = 0)\n```\n:::\n\n\nMake predictions:\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-12_879630ad864b5177f320c495e5ef9d7f'}\n\n```{.r .cell-code}\nhead(predict(DNN, X), n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]\n[1,] 0.3332271\n[2,] 0.3336180\n[3,] 0.3334625\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-13_95d296202b5dc3b23642604d977f5618'}\n\n```{.python .cell-code}\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\nDNN = keras.Sequential()\n  # first hidden layer\nDNN.add(Dense(\n  input_shape=[X.shape[1]], \n  units = 10, \n  activation = \"relu\")) \n  # second hidden layer with regularization\nDNN.add(Dense(\n  units = 20, \n  activation = \"relu\",\n  kernel_regularizer = keras.regularizers.l1()))\n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\nDNN.add(Dense(\n  units = 1, \n  activation = None))\n\n# print architecture\nDNN.summary()\n\n# add loss function and optimizer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_9 (Dense)             (None, 10)                40        \n                                                                 \n dense_10 (Dense)            (None, 20)                220       \n                                                                 \n dense_11 (Dense)            (None, 1)                 21        \n                                                                 \n=================================================================\nTotal params: 281\nTrainable params: 281\nNon-trainable params: 0\n_________________________________________________________________\n```\n:::\n\n```{.python .cell-code}\nDNN.compile(loss = keras.losses.mean_squared_error,\n            optimizer = keras.optimizers.Adamax(0.01))\n\n# train model\nDNN.fit(X, Y, epochs = 50, verbose = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.callbacks.History object at 0x7faf45b6fac0>\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-14_a33f2fdd6a1209b265c895affa749bba'}\n\n```{.python .cell-code}\nDNN.predict(X)[0:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n1/5 [=====>........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n5/5 [==============================] - 0s 626us/step\narray([[5.153783 ],\n       [4.8413796],\n       [4.947369 ],\n       [4.906414 ],\n       [5.2335167],\n       [5.212913 ],\n       [4.928632 ],\n       [5.086681 ],\n       [4.784372 ],\n       [5.047194 ]], dtype=float32)\n```\n:::\n:::\n\n\n\n### Julia\n\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-15_67431682b8fdbf525ae6cf799ed94718'}\n\n```{.julia .cell-code}\nimport StatsBase\nusing RDatasets\nusing StatsBase\nusing DataFrames\nimport MLJBase.int\nusing Flux, Statistics\nusing Flux.Data: DataLoader\nusing Flux: onehotbatch, onecold, @epochs\nusing Flux.Losses: mse\n```\n:::\n\n\nData preparation:\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-16_4107a190971f6f9891e313825bc36eae'}\n\n```{.julia .cell-code}\niris = dataset(\"datasets\", \"iris\");\nX = transpose(Matrix(mapcols(StatsBase.zscore, iris[:, 2:4])));\nYT = iris[:, 1];\nYT = reshape(YT, 1, length(YT));\n\ndata_loader = DataLoader((X, YT), batchsize=10, shuffle=true);\n```\n:::\n\n\nCreate model (similar to Keras):\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-17_a62eeafcd275bfe399bec3a72c176241'}\n\n```{.julia .cell-code}\nmodel = Chain(\n  Dense(3, 20, relu),\n  Dense(20, 20, relu),\n  Dense(20, 1)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChain(\n  Dense(3 => 20, relu),                 # 80 parameters\n  Dense(20 => 20, relu),                # 420 parameters\n  Dense(20 => 1),                       # 21 parameters\n)                   # Total: 6 arrays, 521 parameters, 2.410 KiB.\n```\n:::\n:::\n\n\nTrain/optimize Model:\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-18_baeaa7877c77fe944f8c5fde3b869fa9'}\n\n```{.julia .cell-code}\nparameters = Flux.params(model);\noptimizer = ADAM(0.01);\n\n# Help functions\nloss(x, y) = mse(model(x), y);\n\nget_loss() = @show sum(mse(model(X), YT));\n\n## Training\nfor epoch in 1:20\n  Flux.train!(loss, parameters, data_loader, optimizer, cb = Flux.throttle(get_loss, 5))\nend\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsum(mse(model(X), YT)) = 29.07346144222025\nsum(mse(model(X), YT)) = 2.2416517572050343\nsum(mse(model(X), YT)) = 1.5487028757261814\nsum(mse(model(X), YT)) = 1.0567789347316945\nsum(mse(model(X), YT)) = 0.7146277767677577\nsum(mse(model(X), YT)) = 0.4873170089597854\nsum(mse(model(X), YT)) = 0.2943587558821003\nsum(mse(model(X), YT)) = 0.23717687312953964\nsum(mse(model(X), YT)) = 0.15060292095730848\nsum(mse(model(X), YT)) = 0.12921184500980595\nsum(mse(model(X), YT)) = 0.11438955839064809\nsum(mse(model(X), YT)) = 0.1209811019396246\nsum(mse(model(X), YT)) = 0.1095651084591268\nsum(mse(model(X), YT)) = 0.10894502288937695\nsum(mse(model(X), YT)) = 0.09853287235725164\nsum(mse(model(X), YT)) = 0.1296184768039172\nsum(mse(model(X), YT)) = 0.11522763156480774\nsum(mse(model(X), YT)) = 0.09469441130795656\nsum(mse(model(X), YT)) = 0.09941779923178777\nsum(mse(model(X), YT)) = 0.10689945604907174\n```\n:::\n:::\n\n\nPredictions:\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-19_a07babe2f09ac0307ad54d32a0329909'}\n\n```{.julia .cell-code}\ntranspose(model(X))[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5-element Vector{Float64}:\n 5.044081758527378\n 4.578648187686671\n 4.7570435189899944\n 4.713898034456124\n 5.1239384911577\n```\n:::\n:::\n\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}