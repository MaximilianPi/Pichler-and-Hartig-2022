{
  "hash": "219809d794913267e992b79efecd801a",
  "result": {
    "markdown": "\n\n\n\n# Deep neural networks\n\nDeep neural networks, or more precisely here fully connected neural networks, can be flexibly built which makes their application more challenging than other ML algorithms.\n\nIn the following, we use the 'keras' (@kerasR; @chollet2015keras) package which is a higher level API on the python 'tensorflow' framework (@abadi2016tensorflow).\n\n## Classification\n\n::: panel-tabset\n### R\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-1_4f0a1449c6984889e6f2242782ee0809'}\n\n```{.r .cell-code}\nlibrary(keras)\nX = scale(as.matrix(iris[,1:4]))\nY = as.integer(iris$Species)\n# We need to one hot encode our response classes\nYT = k_one_hot(Y-1L, num_classes = 3)\n\nDNN = keras_model_sequential() %>% \n  # first hidden layer\n  layer_dense(input_shape = ncol(X), \n              units = 10, \n              activation = \"relu\") %>% \n  # second hidden layer with regularization\n  layer_dense(units = 20, \n              activation = \"relu\",\n              kernel_regularizer = regularizer_l1()) %>% \n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\n  layer_dense(units = 3, \n              activation = \"softmax\")\n\n# print architecture\nsummary(DNN)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_2 (Dense)                    (None, 10)                      50          \n dense_1 (Dense)                    (None, 20)                      220         \n dense (Dense)                      (None, 3)                       63          \n================================================================================\nTotal params: 333\nTrainable params: 333\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# add loss function and optimizer\nDNN %>% \n  compile(loss = loss_categorical_crossentropy,\n          optimizer = optimizer_adamax(0.01))\n\n# train model\nDNN %>% \n  fit(X, YT, epochs = 50, verbose = 0)\n```\n:::\n\n\nMake predictions (class probabilities):\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-2_62f640d2a07ac3aba47c2d3fed4ee785'}\n\n```{.r .cell-code}\nhead(predict(DNN, X), n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]        [,2]         [,3]\n[1,] 0.9968244 0.003130971 4.465887e-05\n[2,] 0.9858423 0.013976931 1.807758e-04\n[3,] 0.9960486 0.003893664 5.765830e-05\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-3_a5d8e291c5dc6f2313bea1486666d394'}\n\n```{.python .cell-code}\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\n# We need to one hot encode our response classes\nYT = keras.utils.to_categorical(Y, num_classes = 3)\n\nDNN = keras.Sequential()\n  # first hidden layer\nDNN.add(Dense(\n  input_shape=[X.shape[1]], \n  units = 10, \n  activation = \"relu\")) \n  # second hidden layer with regularization\nDNN.add(Dense(\n  units = 20, \n  activation = \"relu\",\n  kernel_regularizer = keras.regularizers.l1()))\n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\nDNN.add(Dense(\n  units = 3, \n  activation = \"softmax\"))\n\n# print architecture\nDNN.summary()\n\n# add loss function and optimizer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_3 (Dense)             (None, 10)                50        \n                                                                 \n dense_4 (Dense)             (None, 20)                220       \n                                                                 \n dense_5 (Dense)             (None, 3)                 63        \n                                                                 \n=================================================================\nTotal params: 333\nTrainable params: 333\nNon-trainable params: 0\n_________________________________________________________________\n```\n:::\n\n```{.python .cell-code}\nDNN.compile(loss = keras.losses.categorical_crossentropy,\n            optimizer = keras.optimizers.Adamax(0.01))\n\n# train model\nDNN.fit(X, YT, epochs = 50, verbose = 0)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.callbacks.History object at 0x7ff7e648e860>\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-4_c14dfac5e1655a1478429ac83ddf8ccd'}\n\n```{.python .cell-code}\nDNN.predict(X)[0:10,:]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n1/5 [=====>........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n5/5 [==============================] - 0s 1ms/step\narray([[9.9604118e-01, 3.9030442e-03, 5.5764249e-05],\n       [9.8632067e-01, 1.3529156e-02, 1.5015916e-04],\n       [9.9591041e-01, 4.0376107e-03, 5.2057381e-05],\n       [9.9484915e-01, 5.0889952e-03, 6.1775587e-05],\n       [9.9729079e-01, 2.6730390e-03, 3.6100675e-05],\n       [9.9496955e-01, 4.9541364e-03, 7.6401542e-05],\n       [9.9712002e-01, 2.8428256e-03, 3.7198897e-05],\n       [9.9558902e-01, 4.3485621e-03, 6.2399165e-05],\n       [9.9313205e-01, 6.7970627e-03, 7.0966795e-05],\n       [9.9308324e-01, 6.8320944e-03, 8.4627231e-05]], dtype=float32)\n```\n:::\n:::\n\n:::\n\n## Regression\n\n::: panel.-tabset\n### R\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-5_ddda7b33f56558ead683cac19db4e2f1'}\n\n```{.r .cell-code}\nlibrary(keras)\nX = scale(as.matrix(iris[,2:4]))\nY = as.matrix(iris[,1,drop=FALSE])\n\nDNN = keras_model_sequential() %>% \n  # first hidden layer\n  layer_dense(input_shape = ncol(X), \n              units = 10, \n              activation = \"relu\") %>% \n  # second hidden layer with regularization\n  layer_dense(units = 20, \n              activation = \"relu\",\n              kernel_regularizer = regularizer_l1()) %>% \n  # output layer, one output neuron for one response\n  # and no activation function\n  layer_dense(units = 1)\n\n# print architecture\nsummary(DNN)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_2\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_8 (Dense)                    (None, 10)                      40          \n dense_7 (Dense)                    (None, 20)                      220         \n dense_6 (Dense)                    (None, 1)                       21          \n================================================================================\nTotal params: 281\nTrainable params: 281\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# add loss function and optimizer\nDNN %>% \n  compile(loss = loss_mean_squared_error,\n          optimizer = optimizer_adamax(0.01))\n\n# train model\nDNN %>% \n  fit(X, YT, epochs = 50, verbose = 0)\n```\n:::\n\n\nMake predictions:\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-6_05c6decb1a6c8ad8f2519bc0b33c9601'}\n\n```{.r .cell-code}\nhead(predict(DNN, X), n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]\n[1,] 0.3332931\n[2,] 0.3334758\n[3,] 0.3333959\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-7_be34cd7a231eaa6d5fde27776f886286'}\n\n```{.python .cell-code}\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\nDNN = keras.Sequential()\n  # first hidden layer\nDNN.add(Dense(\n  input_shape=[X.shape[1]], \n  units = 10, \n  activation = \"relu\")) \n  # second hidden layer with regularization\nDNN.add(Dense(\n  units = 20, \n  activation = \"relu\",\n  kernel_regularizer = keras.regularizers.l1()))\n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\nDNN.add(Dense(\n  units = 1, \n  activation = None))\n\n# print architecture\nDNN.summary()\n\n# add loss function and optimizer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_9 (Dense)             (None, 10)                40        \n                                                                 \n dense_10 (Dense)            (None, 20)                220       \n                                                                 \n dense_11 (Dense)            (None, 1)                 21        \n                                                                 \n=================================================================\nTotal params: 281\nTrainable params: 281\nNon-trainable params: 0\n_________________________________________________________________\n```\n:::\n\n```{.python .cell-code}\nDNN.compile(loss = keras.losses.mean_squared_error,\n            optimizer = keras.optimizers.Adamax(0.01))\n\n# train model\nDNN.fit(X, Y, epochs = 50, verbose = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.callbacks.History object at 0x7ff7e61e5ac8>\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell hash='dnn_cache/html/unnamed-chunk-8_58d99fa4db8b8b63e7305cad86886d02'}\n\n```{.python .cell-code}\nDNN.predict(X)[0:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n1/5 [=====>........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n5/5 [==============================] - 0s 1ms/step\narray([[5.0288253],\n       [4.8119473],\n       [4.9024477],\n       [4.8307614],\n       [5.158363 ],\n       [5.384384 ],\n       [4.8593917],\n       [4.946008 ],\n       [4.780525 ],\n       [4.9478073]], dtype=float32)\n```\n:::\n:::\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}