{
  "hash": "55561e3337554cf99b84187074aa3c41",
  "result": {
    "markdown": "# Deep neural networks\n\n\n\n\n\n\nDeep neural networks, or more precisely here fully connected neural networks, can be flexibly built which makes their application more challenging than other ML algorithms.\n\nIn the following, we use the 'keras' (@kerasR; @chollet2015keras) (Python: 'keras' (@chollet2015keras); Julia: 'Flux' (@Flux)) package which is a higher level API on the python 'tensorflow' framework (@abadi2016tensorflow).\n\n## Classification\n\n::: panel-tabset\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nX = scale(as.matrix(iris[,1:4]))\nY = as.integer(iris$Species)\n# We need to one hot encode our response classes\nYT = k_one_hot(Y-1L, num_classes = 3)\n\nDNN = keras_model_sequential() %>% \n  # first hidden layer\n  layer_dense(input_shape = ncol(X), \n              units = 10, \n              activation = \"relu\") %>% \n  # second hidden layer with regularization\n  layer_dense(units = 20, \n              activation = \"relu\",\n              kernel_regularizer = regularizer_l1()) %>% \n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\n  layer_dense(units = 3, \n              activation = \"softmax\")\n\n# print architecture\nsummary(DNN)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_2 (Dense)                    (None, 10)                      50          \n dense_1 (Dense)                    (None, 20)                      220         \n dense (Dense)                      (None, 3)                       63          \n================================================================================\nTotal params: 333\nTrainable params: 333\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# add loss function and optimizer\nDNN %>% \n  compile(loss = loss_categorical_crossentropy,\n          optimizer = optimizer_adamax(0.01))\n\n# train model\nDNN %>% \n  fit(X, YT, epochs = 50, verbose = 0)\n```\n:::\n\n\nMake predictions (class probabilities):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(predict(DNN, X), n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]        [,2]         [,3]\n[1,] 0.9962465 0.003589567 0.0001640228\n[2,] 0.9848880 0.014879720 0.0002324550\n[3,] 0.9970835 0.002787639 0.0001286939\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\n# We need to one hot encode our response classes\nYT = keras.utils.to_categorical(Y, num_classes = 3)\n\nDNN = keras.Sequential()\n  # first hidden layer\nDNN.add(Dense(\n  input_shape=[X.shape[1]], \n  units = 10, \n  activation = \"relu\")) \n  # second hidden layer with regularization\nDNN.add(Dense(\n  units = 20, \n  activation = \"relu\",\n  kernel_regularizer = keras.regularizers.l1()))\n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\nDNN.add(Dense(\n  units = 3, \n  activation = \"softmax\"))\n\n# print architecture\nDNN.summary()\n\n# add loss function and optimizer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_3 (Dense)             (None, 10)                50        \n                                                                 \n dense_4 (Dense)             (None, 20)                220       \n                                                                 \n dense_5 (Dense)             (None, 3)                 63        \n                                                                 \n=================================================================\nTotal params: 333\nTrainable params: 333\nNon-trainable params: 0\n_________________________________________________________________\n```\n:::\n\n```{.python .cell-code}\nDNN.compile(loss = keras.losses.categorical_crossentropy,\n            optimizer = keras.optimizers.Adamax(0.01))\n\n# train model\nDNN.fit(X, YT, epochs = 50, verbose = 0)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.callbacks.History object at 0x7f09a7452460>\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nDNN.predict(X)[0:10,:]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n1/5 [=====>........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n5/5 [==============================] - 0s 940us/step\narray([[9.94787872e-01, 4.17001359e-03, 1.04207790e-03],\n       [9.82984900e-01, 1.54780671e-02, 1.53702230e-03],\n       [9.96189654e-01, 3.01337033e-03, 7.96947046e-04],\n       [9.94444370e-01, 4.62495722e-03, 9.30633105e-04],\n       [9.97212470e-01, 2.00825068e-03, 7.79209717e-04],\n       [9.94305611e-01, 3.97154968e-03, 1.72281812e-03],\n       [9.98162746e-01, 1.16384670e-03, 6.73333008e-04],\n       [9.93549705e-01, 5.33071114e-03, 1.11946405e-03],\n       [9.94785070e-01, 4.37794533e-03, 8.37010215e-04],\n       [9.86207247e-01, 1.25755025e-02, 1.21731847e-03]], dtype=float32)\n```\n:::\n:::\n\n\n### Julia\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nimport StatsBase\nusing RDatasets\nusing StatsBase\nusing DataFrames\nimport MLJBase.int\nusing Flux, Statistics\nusing Flux.Data: DataLoader\nusing Flux: onehotbatch, onecold, @epochs\nusing Flux.Losses: logitcrossentropy\n```\n:::\n\n\nData preparation:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\niris = dataset(\"datasets\", \"iris\");\nX = transpose(Matrix(mapcols(StatsBase.zscore, iris[:, 1:4])));\nY = int(iris[:, 5], type = Int);\nclasses = sort(unique(Y));\nYT = onehotbatch(Y, classes);\ndata_loader = DataLoader((X, YT), batchsize=10, shuffle=true);\n```\n:::\n\n\nCreate model (similar to Keras):\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nmodel = Chain(\n  Dense(4, 20, relu),\n  Dense(20, 20, relu),\n  Dense(20, 3)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChain(\n  Dense(4 => 20, relu),                 # 100 parameters\n  Dense(20 => 20, relu),                # 420 parameters\n  Dense(20 => 3),                       # 63 parameters\n)                   # Total: 6 arrays, 583 parameters, 2.652 KiB.\n```\n:::\n:::\n\n\nTrain/optimize Model:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nparameters = Flux.params(model);\noptimizer = ADAM(0.01);\n\n# Help functions\nloss(x, y) = logitcrossentropy(model(x), y);\n\nget_loss() = @show sum(logitcrossentropy(model(X), YT));\n\n## Training\nfor epoch in 1:20\n  Flux.train!(loss, parameters, data_loader, optimizer, cb = Flux.throttle(get_loss, 5))\nend\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsum(logitcrossentropy(model(X), YT)) = 0.9968472881302792\nsum(logitcrossentropy(model(X), YT)) = 0.407915665968644\nsum(logitcrossentropy(model(X), YT)) = 0.25693295877600025\nsum(logitcrossentropy(model(X), YT)) = 0.1766998314541965\nsum(logitcrossentropy(model(X), YT)) = 0.11782307504167162\nsum(logitcrossentropy(model(X), YT)) = 0.08904269600509418\nsum(logitcrossentropy(model(X), YT)) = 0.07343147452012645\nsum(logitcrossentropy(model(X), YT)) = 0.06219033144462982\nsum(logitcrossentropy(model(X), YT)) = 0.07896333188631052\nsum(logitcrossentropy(model(X), YT)) = 0.061009181350311414\nsum(logitcrossentropy(model(X), YT)) = 0.05415327509114975\nsum(logitcrossentropy(model(X), YT)) = 0.06574636561214808\nsum(logitcrossentropy(model(X), YT)) = 0.04392088669909506\nsum(logitcrossentropy(model(X), YT)) = 0.04204886024035571\nsum(logitcrossentropy(model(X), YT)) = 0.050127564760964735\nsum(logitcrossentropy(model(X), YT)) = 0.05248851799584753\nsum(logitcrossentropy(model(X), YT)) = 0.03977422400493945\nsum(logitcrossentropy(model(X), YT)) = 0.04544570242797193\nsum(logitcrossentropy(model(X), YT)) = 0.060201900874104756\nsum(logitcrossentropy(model(X), YT)) = 0.0396585422150025\n```\n:::\n:::\n\n\nPredictions:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\ntranspose(softmax(model(X)))[1:5,:]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5Ã—3 Matrix{Float64}:\n 0.999989  1.09298e-5  1.52312e-11\n 0.999916  8.3711e-5   1.14199e-9\n 0.999995  5.31299e-6  7.13309e-11\n 0.999993  7.32351e-6  2.90119e-10\n 0.999997  2.72559e-6  4.39277e-12\n```\n:::\n:::\n\n\n\n:::\n\n## Regression\n\n::: panel-tabset\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nX = scale(as.matrix(iris[,2:4]))\nY = as.matrix(iris[,1,drop=FALSE])\n\nDNN = keras_model_sequential() %>% \n  # first hidden layer\n  layer_dense(input_shape = ncol(X), \n              units = 10, \n              activation = \"relu\") %>% \n  # second hidden layer with regularization\n  layer_dense(units = 20, \n              activation = \"relu\",\n              kernel_regularizer = regularizer_l1()) %>% \n  # output layer, one output neuron for one response\n  # and no activation function\n  layer_dense(units = 1)\n\n# print architecture\nsummary(DNN)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_2\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_8 (Dense)                    (None, 10)                      40          \n dense_7 (Dense)                    (None, 20)                      220         \n dense_6 (Dense)                    (None, 1)                       21          \n================================================================================\nTotal params: 281\nTrainable params: 281\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# add loss function and optimizer\nDNN %>% \n  compile(loss = loss_mean_squared_error,\n          optimizer = optimizer_adamax(0.01))\n\n# train model\nDNN %>% \n  fit(X, YT, epochs = 50, verbose = 0)\n```\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(predict(DNN, X), n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]\n[1,] 0.3252823\n[2,] 0.3261368\n[3,] 0.3257285\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\nDNN = keras.Sequential()\n  # first hidden layer\nDNN.add(Dense(\n  input_shape=[X.shape[1]], \n  units = 10, \n  activation = \"relu\")) \n  # second hidden layer with regularization\nDNN.add(Dense(\n  units = 20, \n  activation = \"relu\",\n  kernel_regularizer = keras.regularizers.l1()))\n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\nDNN.add(Dense(\n  units = 1, \n  activation = None))\n\n# print architecture\nDNN.summary()\n\n# add loss function and optimizer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_9 (Dense)             (None, 10)                40        \n                                                                 \n dense_10 (Dense)            (None, 20)                220       \n                                                                 \n dense_11 (Dense)            (None, 1)                 21        \n                                                                 \n=================================================================\nTotal params: 281\nTrainable params: 281\nNon-trainable params: 0\n_________________________________________________________________\n```\n:::\n\n```{.python .cell-code}\nDNN.compile(loss = keras.losses.mean_squared_error,\n            optimizer = keras.optimizers.Adamax(0.01))\n\n# train model\nDNN.fit(X, Y, epochs = 50, verbose = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.callbacks.History object at 0x7f0873635d30>\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nDNN.predict(X)[0:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n1/5 [=====>........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n5/5 [==============================] - 0s 639us/step\narray([[5.1530886],\n       [4.635344 ],\n       [4.844628 ],\n       [4.718408 ],\n       [5.2458243],\n       [5.3749986],\n       [5.0117435],\n       [5.046633 ],\n       [4.583274 ],\n       [4.761703 ]], dtype=float32)\n```\n:::\n:::\n\n\n\n### Julia\n\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nimport StatsBase\nusing RDatasets\nusing StatsBase\nusing DataFrames\nimport MLJBase.int\nusing Flux, Statistics\nusing Flux.Data: DataLoader\nusing Flux: onehotbatch, onecold, @epochs\nusing Flux.Losses: mse\n```\n:::\n\n\nData preparation:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\niris = dataset(\"datasets\", \"iris\");\nX = transpose(Matrix(mapcols(StatsBase.zscore, iris[:, 2:4])));\nYT = iris[:, 1];\nYT = reshape(YT, 1, length(YT));\n\ndata_loader = DataLoader((X, YT), batchsize=10, shuffle=true);\n```\n:::\n\n\nCreate model (similar to Keras):\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nmodel = Chain(\n  Dense(3, 20, relu),\n  Dense(20, 20, relu),\n  Dense(20, 1)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChain(\n  Dense(3 => 20, relu),                 # 80 parameters\n  Dense(20 => 20, relu),                # 420 parameters\n  Dense(20 => 1),                       # 21 parameters\n)                   # Total: 6 arrays, 521 parameters, 2.410 KiB.\n```\n:::\n:::\n\n\nTrain/optimize Model:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nparameters = Flux.params(model);\noptimizer = ADAM(0.01);\n\n# Help functions\nloss(x, y) = mse(model(x), y);\n\nget_loss() = @show sum(mse(model(X), YT));\n\n## Training\nfor epoch in 1:20\n  Flux.train!(loss, parameters, data_loader, optimizer, cb = Flux.throttle(get_loss, 5))\nend\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsum(mse(model(X), YT)) = 35.254011263534736\nsum(mse(model(X), YT)) = 12.46171269327014\nsum(mse(model(X), YT)) = 2.2635190110050605\nsum(mse(model(X), YT)) = 1.2401000963017133\nsum(mse(model(X), YT)) = 0.6912489741242238\nsum(mse(model(X), YT)) = 0.5464189172742467\nsum(mse(model(X), YT)) = 0.39399982040159687\nsum(mse(model(X), YT)) = 0.2799709648562455\nsum(mse(model(X), YT)) = 0.22626626969525546\nsum(mse(model(X), YT)) = 0.19594999439068672\nsum(mse(model(X), YT)) = 0.15475237595663777\nsum(mse(model(X), YT)) = 0.15207562095418492\nsum(mse(model(X), YT)) = 0.1366212980594906\nsum(mse(model(X), YT)) = 0.14285041851790095\nsum(mse(model(X), YT)) = 0.12735811524649807\nsum(mse(model(X), YT)) = 0.11084125588102431\nsum(mse(model(X), YT)) = 0.10984711694555327\nsum(mse(model(X), YT)) = 0.10183028533968545\nsum(mse(model(X), YT)) = 0.10055789367635456\nsum(mse(model(X), YT)) = 0.10006587279868089\n```\n:::\n:::\n\n\nPredictions:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\ntranspose(model(X))[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5-element Vector{Float64}:\n 5.078324422496932\n 4.708044600690088\n 4.817372548540471\n 4.800092365508\n 5.157734536850637\n```\n:::\n:::\n\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}