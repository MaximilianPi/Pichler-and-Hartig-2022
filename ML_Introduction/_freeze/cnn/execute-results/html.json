{
  "hash": "225d192a4c635c2974c054ede7e9fc24",
  "result": {
    "markdown": "# Convolutional neural networks\n\n\n\n\n\n\nConvolutional neural networks (CNN) are also deep neural networks but they are based on convolutional layers, which is a biologically inspired variation optimized to process image-based data (@lecun2015deep). CNNs consist of two stages, in the first, the images are passed through convolutional layers and the models learns to detect edges and shapes in the images. In the second stage, the dimensions are dropped and fully-connected layers are used to classify the previously identified shapes.\n\nIn the following, we will use again the 'keras' package (Python: 'keras' (@chollet2015keras); Julia: 'Flux' (@Flux)) but we will not differentiate between classification and regression because the only difference would be to change the last layer and the loss function (see section 'Deep neural networks').\n\nWe will demonstrate the application of CNNs with the MNIST dataset which consists of handwritten digits. The objective of the CNNs is to classify the images. The MNIST dataset is one of the most famous benchmark dataset for image-based tasks (@lecun_mnist_2010).\n\n::: panel-tabset\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\ndata = keras::dataset_mnist()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.10.0\n```\n:::\n\n```{.r .cell-code}\ntrain = data$train\nX = train$x/255\n# we have to add a dimension that \n# informs the network about the channels\n# of the images\nX = array(X, dim = c(dim(X), 1))\nYT = k_one_hot(train$y, num_classes = 10)\n\n\n\nCNN = \n  keras_model_sequential() %>% \n  # first hidden layer\n  layer_conv_2d(input_shape = list(28, 28, 1), \n                filters = 16,\n                kernel_size = c(2, 2),\n                activation = \"relu\") %>%\n  layer_average_pooling_2d() %>% \n  layer_conv_2d(filters = 8,\n                kernel_size = c(2, 2),\n                activation = \"relu\") %>%\n  # we use a normal DNN on top of the CNN:\n  # the layer flatten will remove the additional \n  # dimensions\n  layer_flatten() %>% \n  layer_dense(units = 20, \n              activation = \"relu\") %>%\n  # 10 output neurons for 10 classes\n  layer_dense(units = 10, \n              activation = \"softmax\")\n\n\n# print architecture\nsummary(CNN)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n conv2d_1 (Conv2D)                  (None, 27, 27, 16)              80          \n average_pooling2d (AveragePooling2  (None, 13, 13, 16)             0           \n D)                                                                             \n conv2d (Conv2D)                    (None, 12, 12, 8)               520         \n flatten (Flatten)                  (None, 1152)                    0           \n dense_1 (Dense)                    (None, 20)                      23060       \n dense (Dense)                      (None, 10)                      210         \n================================================================================\nTotal params: 23,870\nTrainable params: 23,870\nNon-trainable params: 0\n________________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\n# add loss function and optimizer\nCNN %>% \n  compile(loss = loss_categorical_crossentropy,\n          optimizer = optimizer_adamax(0.01))\n\nCNN %>% \n  fit(X, YT, epochs = 3, batch_size = 125, verbose = 0)\n```\n:::\n\n\nMake predictions (class probabilites):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(predict(CNN, X[1:100,,,,drop=FALSE]), n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             [,1]         [,2]         [,3]         [,4]         [,5]\n[1,] 1.965090e-08 4.356115e-06 1.122467e-04 3.643005e-01 1.273639e-10\n[2,] 9.999302e-01 2.228819e-09 6.124400e-05 3.962018e-08 2.638534e-07\n[3,] 2.012695e-12 1.291264e-07 3.332906e-08 3.386993e-06 9.999701e-01\n             [,6]         [,7]         [,8]         [,9]        [,10]\n[1,] 6.353825e-01 6.928099e-09 1.841055e-05 1.732095e-04 8.759488e-06\n[2,] 1.047530e-08 1.649719e-06 6.396242e-06 7.546749e-08 4.506550e-08\n[3,] 2.200649e-11 4.529752e-10 7.846268e-06 1.812190e-06 1.660780e-05\n```\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\ndata = keras.datasets.mnist.load_data()\ntrain = data[0][0]\nlabels = data[0][1]\n\n# We need to one hot encode our response classes\nYT = keras.utils.to_categorical(labels, num_classes = 10)\n\nCNN = keras.Sequential()\n  # first hidden layer\nCNN.add(Conv2D(input_shape = [28, 28, 1], \n                filters = 16,\n                kernel_size = (2, 2),\n                activation = \"relu\"))\nCNN.add(AveragePooling2D())\nCNN.add(Conv2D(filters = 8,\n                kernel_size = (2, 2),\n                activation = \"relu\"))\n  # we use a normal DNN on top of the CNN:\n  # the layer flatten will remove the additional \n  # dimensions\nCNN.add(Flatten())\n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\nCNN.add(Dense(\n  units = 10, \n  activation = \"softmax\"))\n\n# print architecture\nCNN.summary()\n\n# add loss function and optimizer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_2 (Conv2D)           (None, 27, 27, 16)        80        \n                                                                 \n average_pooling2d_1 (Averag  (None, 13, 13, 16)       0         \n ePooling2D)                                                     \n                                                                 \n conv2d_3 (Conv2D)           (None, 12, 12, 8)         520       \n                                                                 \n flatten_1 (Flatten)         (None, 1152)              0         \n                                                                 \n dense_2 (Dense)             (None, 10)                11530     \n                                                                 \n=================================================================\nTotal params: 12,130\nTrainable params: 12,130\nNon-trainable params: 0\n_________________________________________________________________\n```\n:::\n\n```{.python .cell-code}\nCNN.compile(loss = keras.losses.categorical_crossentropy,\n            optimizer = keras.optimizers.Adamax(0.01))\n\n# train model\nCNN.fit(train, YT, epochs = 5, verbose = 0)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.callbacks.History object at 0x7f41f81cabe0>\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nCNN.predict(train[0:10,:,:])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 40ms/step\narray([[2.73058399e-11, 5.12852705e-10, 6.08049167e-10, 2.43562553e-02,\n        4.99432336e-11, 9.75430012e-01, 1.32788358e-09, 1.10361043e-09,\n        2.33132232e-05, 1.90236795e-04],\n       [9.99909341e-01, 4.28194667e-11, 2.47524281e-06, 3.22915097e-11,\n        3.69686504e-10, 3.31575795e-10, 8.78980791e-05, 1.30885678e-12,\n        1.75022961e-07, 2.92207769e-08],\n       [1.86242036e-10, 3.31999718e-06, 3.40689681e-08, 2.78217680e-08,\n        9.99971926e-01, 2.22715540e-10, 4.35771081e-12, 2.72133593e-06,\n        8.05194304e-07, 2.11609149e-05],\n       [1.04308064e-07, 9.99899924e-01, 2.12908731e-06, 9.57279056e-09,\n        1.54249974e-05, 6.92280056e-09, 1.16389579e-07, 4.02882006e-06,\n        7.77568057e-05, 4.73492236e-07],\n       [6.86259725e-08, 3.67166734e-07, 1.25773326e-07, 6.08295522e-07,\n        1.08382816e-03, 1.26706354e-07, 3.40244521e-12, 1.83549821e-02,\n        4.74471017e-06, 9.80555177e-01],\n       [2.71790706e-12, 7.43156381e-09, 9.99988377e-01, 1.16067767e-07,\n        3.00915337e-09, 1.63208232e-11, 5.44445999e-12, 1.09075728e-07,\n        1.12213984e-05, 1.28492715e-07],\n       [7.45668160e-07, 9.98325646e-01, 8.54519240e-05, 8.99816951e-06,\n        5.18015120e-04, 7.52458902e-07, 1.64569755e-07, 4.39679582e-07,\n        1.05933694e-03, 5.20451124e-07],\n       [6.69499386e-11, 8.18687201e-11, 3.74586136e-07, 9.99990761e-01,\n        9.71549975e-14, 1.62581344e-07, 5.61215510e-17, 2.27084040e-09,\n        3.95518327e-06, 4.80054996e-06],\n       [8.22694020e-08, 9.99639750e-01, 1.82648796e-06, 4.38489496e-06,\n        2.37529792e-04, 7.21763058e-07, 2.96431665e-07, 8.08423101e-06,\n        7.77193782e-05, 2.94992296e-05],\n       [4.26150208e-08, 9.40309963e-09, 5.55570452e-08, 2.99561909e-09,\n        9.99997020e-01, 7.58271113e-10, 6.39962194e-10, 5.68171856e-08,\n        2.87104945e-06, 1.34309447e-11]], dtype=float32)\n```\n:::\n:::\n\n\n## Julia\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nimport StatsBase\nusing RDatasets\nusing StatsBase\nusing DataFrames\nimport MLJBase.int\nusing MLDatasets: MNIST\nusing Flux, Statistics\nusing Flux.Data: DataLoader\nusing Flux: onehotbatch, onecold, @epochs\nusing Flux.Losses: logitcrossentropy\n```\n:::\n\n\nData preparation:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nENV[\"DATADEPS_ALWAYS_ACCEPT\"] = \"true\"\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\"true\"\n```\n:::\n\n```{.julia .cell-code}\n\nxtrain, ytrain = MNIST(:train)[:];\nxtrain = reshape(xtrain/255., 28, 28, 1, 60000);\nytrain = onehotbatch(ytrain, 0:9);\n\ndata_loader = DataLoader((xtrain, ytrain), batchsize=100, shuffle=true);\n```\n:::\n\n\nCreate model (similar to Keras):\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nmodel = Chain(\n  Conv((2, 2), 1=>16, pad = (1, 1), relu),\n  MeanPool((2, 2)),\n  Conv((2, 2), 16=>8, pad = (1, 1), relu),\n  MeanPool((2, 2)),\n  Flux.flatten,\n  Dense(392, 20, relu),\n  Dense(20, 10)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChain(\n  Conv((2, 2), 1 => 16, relu, pad=1),   # 80 parameters\n  MeanPool((2, 2)),\n  Conv((2, 2), 16 => 8, relu, pad=1),   # 520 parameters\n  MeanPool((2, 2)),\n  Flux.flatten,\n  Dense(392 => 20, relu),               # 7_860 parameters\n  Dense(20 => 10),                      # 210 parameters\n)                   # Total: 8 arrays, 8_670 parameters, 34.977 KiB.\n```\n:::\n:::\n\n\nTrain/optimize Model:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nparameters = Flux.params(model);\noptimizer = ADAM(0.01);\n\n# Help functions\nloss(x, y) = logitcrossentropy(model(x), y);\n\nget_loss() = @show sum(logitcrossentropy(model(xtrain[:,:,:,1:100]), ytrain[:,1:100]));\n\n## Training\nfor epoch in 1:1\n  Flux.train!(loss, parameters, data_loader, optimizer, cb = Flux.throttle(get_loss, 6000))\nend\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsum(logitcrossentropy(model(xtrain[:, :, :, 1:100]), ytrain[:, 1:100])) = 2.3016892844159544\n```\n:::\n:::\n\n\nPredictions:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nsoftmax(model(xtrain[:,:,:,1:5]))[:,1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10-element Vector{Float64}:\n 0.0949596365926727\n 0.11319646009727474\n 0.09622221648932856\n 0.1105004159927897\n 0.09568476487396564\n 0.08534299720248913\n 0.10008107419393342\n 0.10874953336990927\n 0.10192039819733163\n 0.09334250299030501\n```\n:::\n:::\n\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}