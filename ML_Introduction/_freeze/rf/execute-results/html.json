{
  "hash": "49580337c6345904bbc54c2df3d948b4",
  "result": {
    "markdown": "# Random forest\n\n\n\n\n\n\nThe random forest (RF) algorithm is probably one of the most famous ML algorithms, and not without reason. Compared to other well performing algorithms, the RF algorithm has only a few hyper-parameters and because of the bagging and the random sampling of available variables in for the node splits, it has a well working internal complexity adaption.\n\nIn the following, we use the 'ranger' package (@ranger) (Python: 'scikit-learn' (@scikit-learn), Julia: 'MLJ' (@anthony_blaom_2019_3541506)).\n\n## Classification\n\n::: panel-tabset\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ranger)\nX = iris[,1:4]\nY = iris[,5,drop=FALSE]\ndata = cbind(Y, X)\n\nrf = ranger(Species~., data = data, probability = TRUE, importance = \"impurity\")\n```\n:::\n\n\nShow feature importances:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimportance(rf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    9.067816     1.358848    41.845718    43.340283 \n```\n:::\n:::\n\n\nMake predictions (class probabilities):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(predict(rf, data = data)$predictions, n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        setosa   versicolor    virginica\n[1,] 1.0000000 0.0000000000 0.0000000000\n[2,] 0.9995556 0.0002222222 0.0002222222\n[3,] 1.0000000 0.0000000000 0.0000000000\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\nmodel = RandomForestClassifier().fit(X, Y)\n```\n:::\n\n\nFeature importance\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(model.feature_importances_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.1047826  0.02722545 0.43783582 0.43015613]\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel.predict_proba(X)[0:10,:]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.]])\n```\n:::\n:::\n\n\n### Julia\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nimport StatsBase;\nusing MLJ;\nRF_classifier = @load RandomForestClassifier pkg=DecisionTree;\nusing RDatasets;\nusing StatsBase;\nusing DataFrames;\n```\n:::\n\n::: {.cell}\n\n```{.julia .cell-code}\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 1:4]);\nY = iris[:, 5];\n```\n:::\n\n\nModels:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nmodel = fit!(machine(RF_classifier(), X, Y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrained Machine; caches model-specific representations of data\n  model: RandomForestClassifier(max_depth = -1, …)\n  args: \n    1:\tSource @613 ⏎ Table{AbstractVector{Continuous}}\n    2:\tSource @784 ⏎ AbstractVector{Multiclass{3}}\n```\n:::\n:::\n\n\nFeature importance:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nfeature_importances(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4-element Vector{Pair{Symbol, Float64}}:\n :PetalLength => 0.51551371534272\n  :PetalWidth => 0.3981451261378913\n :SepalLength => 0.06998182233047624\n  :SepalWidth => 0.01635933618891258\n```\n:::\n:::\n\n\n\nPredictions:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nMLJ.predict(model, X)[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n```\n:::\n:::\n\n\n:::\n\n## Regression\n\n::: panel-tabset\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ranger)\nX = iris[,2:4]\ndata = cbind(iris[,1,drop=FALSE], X)\n\nrf = ranger(Sepal.Length~., data = data, importance = \"impurity\")\n```\n:::\n\n\nShow feature importances:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimportance(rf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Sepal.Width Petal.Length  Petal.Width \n    11.72733     46.86181     37.13289 \n```\n:::\n:::\n\n\nMake predictions (class probabilities):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(predict(rf, data = data)$predictions, n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.104768 4.774441 4.649346\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\nmodel = RandomForestRegressor().fit(X, Y)\n```\n:::\n\n\nFeature importance:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(model.feature_importances_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.07991512 0.85830207 0.06178281]\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel.predict(X)[0:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([5.106     , 4.8205    , 4.57298571, 4.76945   , 5.017     ,\n       5.429     , 4.80283333, 5.06201667, 4.5855    , 4.856     ])\n```\n:::\n:::\n\n\n### Julia\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nimport StatsBase;\nusing MLJ;\nRF_regressor = @load RandomForestRegressor pkg=DecisionTree;\nusing RDatasets;\nusing DataFrames;\n```\n:::\n\n::: {.cell}\n\n```{.julia .cell-code}\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 2:4]);\nY = iris[:, 1];\n```\n:::\n\n\nModel:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nmodel = fit!(machine(RF_regressor(), X, Y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrained Machine; caches model-specific representations of data\n  model: RandomForestRegressor(max_depth = -1, …)\n  args: \n    1:\tSource @316 ⏎ Table{AbstractVector{Continuous}}\n    2:\tSource @129 ⏎ AbstractVector{Continuous}\n```\n:::\n:::\n\n\nFeature importance:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nfeature_importances(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n3-element Vector{Pair{Symbol, Float64}}:\n :PetalLength => 0.6626304609310221\n  :PetalWidth => 0.23647943010293143\n  :SepalWidth => 0.10089010896604662\n```\n:::\n:::\n\n\n\nPredictions:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nMLJ.predict(model, X)[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5-element Vector{Float64}:\n 5.1000000000000005\n 4.659999999999999\n 4.62\n 4.720000000000001\n 5.0600000000000005\n```\n:::\n:::\n\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}