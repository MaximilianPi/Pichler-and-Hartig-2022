{
  "hash": "0cf8fcb9abca9dee0753ab8ec810c0a6",
  "result": {
    "markdown": "# Support Vector Machines\n\n\n\n\n\nThe support vector machine (SVM) algorithm estimates hyper-planes to separate our response species. In the following we use the 'e1071' package which supports a variety of different SVM algorithms (@e1071) (Python: 'scikit-learn' (@scikit-learn), Julia: 'MLJ' (@anthony_blaom_2019_3541506)).\n\n## Classification\n\n::: panel-tabset\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(e1071)\nX = scale(iris[,1:4])\nY = iris$Species\n\nsv = svm(X, Y, probability = TRUE) \nsummary(sv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsvm.default(x = X, y = Y, probability = TRUE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  51\n\n ( 8 22 21 )\n\n\nNumber of Classes:  3 \n\nLevels: \n setosa versicolor virginica\n```\n:::\n:::\n\n\nMake predictions (class probabilities):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(attr(predict(sv, newdata = X, probability = TRUE), \"probabilities\"), n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     setosa versicolor  virginica\n1 0.9791731 0.01135581 0.00947110\n2 0.9716762 0.01816135 0.01016248\n3 0.9777791 0.01198490 0.01023600\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn import svm\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\nmodel = svm.SVC(probability=True).fit(X, Y)\n\n# Make predictions (class probabilities):\n\nmodel.predict_proba(X)[0:10,:]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[0.97956765, 0.01168732, 0.00874504],\n       [0.97215052, 0.01844973, 0.00939975],\n       [0.9783134 , 0.01226308, 0.00942351],\n       [0.9742125 , 0.01567632, 0.01011118],\n       [0.97870322, 0.01206444, 0.00923234],\n       [0.97312428, 0.01729716, 0.00957855],\n       [0.97486896, 0.01395157, 0.01117947],\n       [0.97946381, 0.01179526, 0.00874092],\n       [0.96530784, 0.02294644, 0.01174573],\n       [0.97603545, 0.01443107, 0.00953347]])\n```\n:::\n:::\n\n\n### Julia\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nimport StatsBase;\nusing MLJ;\nSVM_classifier = @load NuSVC pkg=LIBSVM;\nusing RDatasets;\nusing StatsBase;\nusing DataFrames;\n```\n:::\n\n::: {.cell}\n\n```{.julia .cell-code}\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 1:4]);\nY = iris[:, 5];\n```\n:::\n\n\nModels:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nmodel = fit!(machine(SVM_classifier(), X, Y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrained Machine; caches model-specific representations of data\n  model: NuSVC(kernel = RadialBasis, …)\n  args: \n    1:\tSource @424 ⏎ Table{AbstractVector{Continuous}}\n    2:\tSource @132 ⏎ AbstractVector{Multiclass{3}}\n```\n:::\n:::\n\n\nPredictions:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nMLJ.predict(model, X)[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5-element CategoricalArrays.CategoricalArray{String,1,UInt8}:\n \"setosa\"\n \"setosa\"\n \"setosa\"\n \"setosa\"\n \"setosa\"\n```\n:::\n:::\n\n:::\n\n## Regression\n\n::: panel-tabset\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(e1071)\nX = scale(iris[,2:4])\nY = iris[,1]\n\nsv = svm(X, Y) \nsummary(sv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsvm.default(x = X, y = Y)\n\n\nParameters:\n   SVM-Type:  eps-regression \n SVM-Kernel:  radial \n       cost:  1 \n      gamma:  0.3333333 \n    epsilon:  0.1 \n\n\nNumber of Support Vectors:  124\n```\n:::\n:::\n\n\nMake predictions (class probabilities):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(predict(sv, newdata = X), n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1        2        3 \n5.042085 4.711768 4.836291 \n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn import svm\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\nmodel = svm.SVR().fit(X, Y)\n\n# Make predictions:\n\nmodel.predict(X)[0:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([5.03583855, 4.69496586, 4.81438855, 4.77951854, 5.10018373,\n       5.29981857, 4.97308737, 4.98199033, 4.63701656, 4.78431078])\n```\n:::\n:::\n\n\n### Julia\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nimport StatsBase;\nusing MLJ;\nSVM_regressor =  @load NuSVR pkg=LIBSVM;\nusing RDatasets;\nusing DataFrames;\n```\n:::\n\n::: {.cell}\n\n```{.julia .cell-code}\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 2:4]);\nY = iris[:, 1];\n```\n:::\n\n\nModel:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nmodel = fit!(machine(SVM_regressor(), X, Y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrained Machine; caches model-specific representations of data\n  model: NuSVR(kernel = RadialBasis, …)\n  args: \n    1:\tSource @758 ⏎ Table{AbstractVector{Continuous}}\n    2:\tSource @893 ⏎ AbstractVector{Continuous}\n```\n:::\n:::\n\n\nPredictions:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nMLJ.predict(model, X)[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5-element Vector{Float64}:\n 5.058471741834634\n 4.6717512552719604\n 4.799641470830148\n 4.75734816087994\n 5.133728219775252\n```\n:::\n:::\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}