{
  "hash": "692e56443573fcd3e09de0e9e68743f2",
  "result": {
    "markdown": "\n\n\n# Support Vector Machines\n\nThe support vector machine (SVM) algorithm estimates hyper-planes to separate our response species. In the following we use the 'e1071' package which supports a variety of different SVM algorithms (@e1071).\n\n## Classification\n\n::: panel-tabset\n### R\n\n\n::: {.cell hash='svm_cache/html/unnamed-chunk-1_af09d49e9719174f34f690613bb07eac'}\n\n```{.r .cell-code}\nlibrary(e1071)\nX = scale(iris[,1:4])\nY = iris$Species\n\nsv = svm(X, Y, probability = TRUE) \nsummary(sv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsvm.default(x = X, y = Y, probability = TRUE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  51\n\n ( 8 22 21 )\n\n\nNumber of Classes:  3 \n\nLevels: \n setosa versicolor virginica\n```\n:::\n:::\n\n\nMake predictions (class probabilities):\n\n\n::: {.cell hash='svm_cache/html/unnamed-chunk-2_7027ef3de5bacbf2004328a995731449'}\n\n```{.r .cell-code}\nhead(attr(predict(sv, newdata = X, probability = TRUE), \"probabilities\"), n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     setosa versicolor   virginica\n1 0.9805055 0.01119040 0.008304068\n2 0.9731565 0.01793966 0.008903881\n3 0.9792215 0.01181014 0.008968335\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell hash='svm_cache/html/unnamed-chunk-3_3949c98e37481edc3a77d6d8bc92512a'}\n\n```{.python .cell-code}\nfrom sklearn import svm\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\nmodel = svm.SVC(probability=True).fit(X, Y)\n\n# Make predictions (class probabilities):\n\nmodel.predict_proba(X)[0:10,:]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[0.9811499 , 0.01051815, 0.00833194],\n       [0.97431749, 0.01672267, 0.00895984],\n       [0.97995873, 0.01104259, 0.00899868],\n       [0.97616243, 0.01416908, 0.00966849],\n       [0.98032481, 0.01085089, 0.0088243 ],\n       [0.97519955, 0.01563513, 0.00916532],\n       [0.97668604, 0.0125787 , 0.01073526],\n       [0.98105781, 0.01061998, 0.00832221],\n       [0.96786931, 0.02083993, 0.01129076],\n       [0.97787346, 0.01303339, 0.00909315]])\n```\n:::\n:::\n\n:::\n\n## Regression\n\n::: panel-tabset\n### R\n\n\n::: {.cell hash='svm_cache/html/unnamed-chunk-4_47f11b8c3dfaa1615fff01301efb6255'}\n\n```{.r .cell-code}\nlibrary(e1071)\nX = scale(iris[,2:4])\nY = iris[,1]\n\nsv = svm(X, Y) \nsummary(sv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsvm.default(x = X, y = Y)\n\n\nParameters:\n   SVM-Type:  eps-regression \n SVM-Kernel:  radial \n       cost:  1 \n      gamma:  0.3333333 \n    epsilon:  0.1 \n\n\nNumber of Support Vectors:  124\n```\n:::\n:::\n\n\nMake predictions (class probabilities):\n\n\n::: {.cell hash='svm_cache/html/unnamed-chunk-5_7cc3146f6a8708c353d181043a75c257'}\n\n```{.r .cell-code}\nhead(predict(sv, newdata = X), n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1        2        3 \n5.042085 4.711768 4.836291 \n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell hash='svm_cache/html/unnamed-chunk-6_5056369b8e4cb80cc6671c1f674c4ab9'}\n\n```{.python .cell-code}\nfrom sklearn import svm\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\nmodel = svm.SVR().fit(X, Y)\n\n# Make predictions:\n\nmodel.predict(X)[0:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([5.03583855, 4.69496586, 4.81438855, 4.77951854, 5.10018373,\n       5.29981857, 4.97308737, 4.98199033, 4.63701656, 4.78431078])\n```\n:::\n:::\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}