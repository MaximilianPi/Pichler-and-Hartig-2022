{
  "hash": "54c3944c4e345e020a9b314d6c1489e1",
  "result": {
    "markdown": "\n# Recurrent neural networks\n\n\n\n\n\n\nRecurrent neural networks are also deep neural networks but use layers specialized to handle time-series. In the following, we will use again the 'keras' package (Python: 'keras' (@chollet2015keras); Julia: 'Flux' (@Flux)) but we will not differentiate between classification and regression because the only difference would be to change the last layer and the loss function (see section 'Deep neural networks').\n\nAbout the data, we simulated in the following one time series from a simple ARIMA process, using the 'arima.sim' function. Our goal is to train a net which is able to predict the next 10 time points based on the previous 10 time points.\n\n::: panel-tabset\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## RNNs\nlibrary(keras)\ndata = as.matrix(arima.sim(n = 1000, list(ar = c(0.3, -0.7)) ))\n# We use here a simplified way to create X and Y \n# since the focus is on creating the RNNs\ndata = matrix(data, ncol = 10L, byrow = TRUE)\nX = array(data[seq(1, 100, by = 2), ], dim = c(50, 10, 1))\nY = data[seq(2, 100, by = 2), ]\n\nRNN = \n  keras_model_sequential() %>% \n  # first hidden layer\n  layer_gru(input_shape = list(10L, 1L),\n            units = 50, \n            activation = \"relu\") %>%\n  # we want to predict the next 10 time steps\n  layer_dense(units = 10)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.10.0\n```\n:::\n\n```{.r .cell-code}\n# add loss function and optimizer\nRNN %>% \n  compile(loss = loss_mean_squared_error,\n          optimizer = optimizer_adamax(0.01))\n\nRNN %>% \n  fit(X, Y, epochs = 5, verbose = 0)\n```\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(predict(RNN, X), n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             [,1]       [,2]      [,3]         [,4]       [,5]       [,6]\n[1,]  0.182871714 0.05779056 0.2804810  0.095762551 -0.2876692 -0.7354215\n[2,]  0.242111340 0.21076134 0.3804666  0.008897863 -0.3699271 -0.8426715\n[3,] -0.004352083 0.33250374 0.3860463 -0.031897288 -0.3857037 -0.5717312\n            [,7]      [,8]        [,9]       [,10]\n[1,] -0.08431444 0.4072831 -0.06652965 -0.09972054\n[2,] -0.04815055 0.4886538 -0.09961714 -0.06673245\n[3,]  0.07023557 0.5174094  0.04489198 -0.01002921\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nX = r.X # get data from R\nY = r.Y \n\nRNN = keras.Sequential()\n  # first hidden layer\nRNN.add(GRU(input_shape = [10, 1],units = 50, activation = \"relu\"))\nRNN.add(Dense(units = 10))\n\nRNN.summary()\n\n# add loss function and optimizer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_1 (GRU)                 (None, 50)                7950      \n                                                                 \n dense_1 (Dense)             (None, 10)                510       \n                                                                 \n=================================================================\nTotal params: 8,460\nTrainable params: 8,460\nNon-trainable params: 0\n_________________________________________________________________\n```\n:::\n\n```{.python .cell-code}\nRNN.compile(loss = keras.losses.mean_squared_error,\n            optimizer = keras.optimizers.Adamax(0.01))\n\n# train model\nRNN.fit(X, Y, epochs = 5, verbose = 0)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.callbacks.History object at 0x7f0436fa8ee0>\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nRNN.predict(X)[0:10,:]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n1/2 [==============>...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n2/2 [==============================] - 0s 3ms/step\narray([[ 0.08350393, -0.00180464,  0.34325618,  0.21686247, -0.18946874,\n        -0.5937582 , -0.16486388,  0.33871526, -0.11158126, -0.15158123],\n       [ 0.2227844 ,  0.4065776 ,  0.38111433, -0.00899748, -0.41014364,\n        -0.6645749 , -0.13457376,  0.36858225, -0.20998326, -0.1766688 ],\n       [-0.17471497,  0.74995714,  0.7496149 , -0.37774378, -0.77188545,\n        -0.6525765 ,  0.24792194,  0.66503304, -0.12292746, -0.17186296],\n       [-0.28794998,  1.5990531 ,  1.306996  , -0.829319  , -1.5312688 ,\n        -1.1471694 ,  0.5062746 ,  1.2400869 , -0.250422  , -0.299824  ],\n       [-0.23757203,  0.9901169 ,  1.0352945 , -0.53825665, -1.0542827 ,\n        -0.8925454 ,  0.3523099 ,  0.9200909 , -0.17282516, -0.25122654],\n       [-0.08620348,  1.4480689 ,  0.8258031 , -0.821919  , -1.1419564 ,\n        -0.7132156 ,  0.4358264 ,  0.6400992 , -0.24556354, -0.15073477],\n       [-0.17174806,  0.6353523 ,  0.73129326, -0.30230048, -0.7101729 ,\n        -0.66088593,  0.20432712,  0.67419755, -0.10780463, -0.18124355],\n       [-0.31512833, -0.9728573 ,  0.3660514 ,  0.58512855,  0.24408501,\n        -0.4672906 , -0.29277703,  0.7419173 ,  0.24761096, -0.18694049],\n       [-0.18512186,  0.9227145 ,  0.83411753, -0.45813644, -0.9110877 ,\n        -0.73084646,  0.29302734,  0.76778096, -0.14175564, -0.18799058],\n       [-0.3057377 ,  0.2650427 ,  0.8319554 , -0.1744615 , -0.5854104 ,\n        -0.6983775 ,  0.16514157,  0.7936327 , -0.02395438, -0.21329436]],\n      dtype=float32)\n```\n:::\n:::\n\n\n### Julia\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nimport StatsBase;\nusing RDatasets;\nusing StatsBase;\nusing DataFrames;\nimport MLJBase.int;\nusing Flux, Statistics;\nusing Flux.Losses: mse;\nusing ARFIMA;\n```\n:::\n\n\nData preparation:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nX = transpose(reshape(convert(Vector{Float32}, arfima(1000,0.5, 0.3, SVector(-0.7))), 100, 10));\nxtrain = X[:, collect(1:2:100)];\nytrain = X[:, collect(2:2:100)];\n```\n:::\n\n\nCreate model (similar to Keras):\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nmodel = Chain(\n  GRU(10=>50),\n  Dense(50, 10)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChain(\n  Recur(\n    GRUCell(10 => 50),                  # 9_200 parameters\n  ),\n  Dense(50 => 10),                      # 510 parameters\n)         # Total: 6 trainable arrays, 9_710 parameters,\n          # plus 1 non-trainable, 50 parameters, summarysize 38.297 KiB.\n```\n:::\n:::\n\n\nTrain/optimize Model:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nparameters = Flux.params(model);\noptimizer = ADAM(0.01);\n\nfor epoch in 1:10\n    Flux.reset!(model); \n    grads = gradient(parameters) do \n        Pred = [model( xtrain[:,i]) for i in 1:50];\n        loss = mean([mse(Pred[i], ytrain[:,i]) for i in 1:50]);\n        println(loss);\n        loss\n    end\n    Flux.update!(optimizer, parameters, grads);\nend\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.3758767\n0.31391445\n0.27496925\n0.25230625\n0.23890918\n0.22788806\n0.21786831\n0.20987608\n0.20394556\n0.1992052\n```\n:::\n:::\n\n\nPredictions:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nPred = [model( xtrain[:,i]) for i in 1:50];\nPred[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10-element Vector{Float32}:\n  0.5586194\n  0.21556926\n -0.4545607\n  0.31776372\n -0.19870074\n -0.2925711\n -0.1116208\n  0.16067675\n  0.064349174\n -0.20158282\n```\n:::\n:::\n\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}