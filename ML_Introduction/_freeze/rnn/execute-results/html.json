{
  "hash": "54c3944c4e345e020a9b314d6c1489e1",
  "result": {
    "markdown": "\n# Recurrent neural networks\n\n\n\n\n\n\nRecurrent neural networks are also deep neural networks but use layers specialized to handle time-series. In the following, we will use again the 'keras' package (Python: 'keras' (@chollet2015keras); Julia: 'Flux' (@Flux)) but we will not differentiate between classification and regression because the only difference would be to change the last layer and the loss function (see section 'Deep neural networks').\n\nAbout the data, we simulated in the following one time series from a simple ARIMA process, using the 'arima.sim' function. Our goal is to train a net which is able to predict the next 10 time points based on the previous 10 time points.\n\n::: panel-tabset\n### R\n\n\n::: {.cell hash='rnn_cache/html/unnamed-chunk-2_6fa735d59a642407e2ff7125db39ad3c'}\n\n```{.r .cell-code}\n## RNNs\nlibrary(keras)\ndata = as.matrix(arima.sim(n = 1000, list(ar = c(0.3, -0.7)) ))\n# We use here a simplified way to create X and Y \n# since the focus is on creating the RNNs\ndata = matrix(data, ncol = 10L, byrow = TRUE)\nX = array(data[seq(1, 100, by = 2), ], dim = c(50, 10, 1))\nY = data[seq(2, 100, by = 2), ]\n\nRNN = \n  keras_model_sequential() %>% \n  # first hidden layer\n  layer_gru(input_shape = list(10L, 1L),\n            units = 50, \n            activation = \"relu\") %>%\n  # we want to predict the next 10 time steps\n  layer_dense(units = 10)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded Tensorflow version 2.10.0\n```\n:::\n\n```{.r .cell-code}\n# add loss function and optimizer\nRNN %>% \n  compile(loss = loss_mean_squared_error,\n          optimizer = optimizer_adamax(0.01))\n\nRNN %>% \n  fit(X, Y, epochs = 5, verbose = 0)\n```\n:::\n\n\nMake predictions:\n\n\n::: {.cell hash='rnn_cache/html/unnamed-chunk-3_1c771a78f2badfa852734b96b45a338c'}\n\n```{.r .cell-code}\nhead(predict(RNN, X), n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]       [,2]       [,3]       [,4]       [,5]       [,6]\n[1,] 0.4800752  0.2515826 -0.1871746 -0.5251560 0.02715888  0.2110344\n[2,] 0.2533702 -0.5832934 -0.3672795  0.2838775 0.53607899 -0.2003853\n[3,] 0.4985844  0.2491744 -0.2287293 -0.5281581 0.04576387  0.1937918\n            [,7]       [,8]       [,9]      [,10]\n[1,] -0.03789480 -0.5814861 -0.4529335 0.08699021\n[2,] -0.23894967 -0.1293640  0.1142473 0.01777250\n[3,] -0.05073898 -0.5958483 -0.4544795 0.06178066\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell hash='rnn_cache/html/unnamed-chunk-4_c7ab51d3b044aa8ec43a6ca131219697'}\n\n```{.python .cell-code}\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nX = r.X # get data from R\nY = r.Y \n\nRNN = keras.Sequential()\n  # first hidden layer\nRNN.add(GRU(input_shape = [10, 1],units = 50, activation = \"relu\"))\nRNN.add(Dense(units = 10))\n\nRNN.summary()\n\n# add loss function and optimizer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_1 (GRU)                 (None, 50)                7950      \n                                                                 \n dense_1 (Dense)             (None, 10)                510       \n                                                                 \n=================================================================\nTotal params: 8,460\nTrainable params: 8,460\nNon-trainable params: 0\n_________________________________________________________________\n```\n:::\n\n```{.python .cell-code}\nRNN.compile(loss = keras.losses.mean_squared_error,\n            optimizer = keras.optimizers.Adamax(0.01))\n\n# train model\nRNN.fit(X, Y, epochs = 5, verbose = 0)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<keras.callbacks.History object at 0x7f91380e4670>\n```\n:::\n:::\n\n\nMake predictions:\n\n\n::: {.cell hash='rnn_cache/html/unnamed-chunk-5_8ff8b39fdb3ef9f294015d7bec4c0929'}\n\n```{.python .cell-code}\nRNN.predict(X)[0:10,:]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n1/2 [==============>...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n2/2 [==============================] - 0s 2ms/step\narray([[ 0.42014763, -0.18803373, -0.49838978, -0.23084594,  0.16043243,\n         0.09836911,  0.05774919, -0.6397091 , -0.17642473,  0.23359524],\n       [ 0.24713348, -0.5150123 , -0.41040528,  0.35002753,  0.5223741 ,\n        -0.25475818, -0.2640479 , -0.3793788 ,  0.28173554,  0.06628139],\n       [ 0.42807055, -0.23126799, -0.5387223 , -0.21818314,  0.17583738,\n         0.09494507,  0.04807471, -0.66506076, -0.17101239,  0.22419289],\n       [ 0.19364469, -0.16822672, -0.35907426, -0.05538802,  0.09245294,\n         0.09220832,  0.03405145, -0.39754653, -0.08505453,  0.07975613],\n       [ 0.5740196 , -0.04543666, -0.7570797 , -0.8942825 , -0.19053774,\n         0.56847376,  0.420092  , -0.9900241 , -0.62275285,  0.3780856 ],\n       [ 0.2136533 , -0.3909836 , -0.39041203,  0.27481425,  0.34575748,\n        -0.11220238, -0.15177974, -0.36721054,  0.15575731,  0.03337399],\n       [ 0.30640042, -0.04427434, -0.4467045 , -0.35476726, -0.06164659,\n         0.24404266,  0.19626519, -0.5636717 , -0.29490072,  0.18287963],\n       [ 0.26979104, -0.5362462 , -0.39888373,  0.36888862,  0.58633494,\n        -0.32184407, -0.3478408 , -0.33489403,  0.36450946,  0.10723882],\n       [ 0.41119274, -0.26509812, -0.5204819 , -0.14221832,  0.2413957 ,\n         0.00485324, -0.00819756, -0.6338831 , -0.0883438 ,  0.22701262],\n       [ 0.2998161 ,  0.01500335, -0.41577554, -0.41534966, -0.12229332,\n         0.30252767,  0.2377799 , -0.54849315, -0.35903308,  0.1939047 ]],\n      dtype=float32)\n```\n:::\n:::\n\n\n### Julia\n\n\n::: {.cell hash='rnn_cache/html/unnamed-chunk-6_205446bab38e9884ffa297967f221cb8'}\n\n```{.julia .cell-code}\nimport StatsBase;\nusing RDatasets;\nusing StatsBase;\nusing DataFrames;\nimport MLJBase.int;\nusing Flux, Statistics;\nusing Flux.Losses: mse;\nusing ARFIMA;\n```\n:::\n\n\nData preparation:\n\n\n::: {.cell hash='rnn_cache/html/unnamed-chunk-7_5ffd98d9c42209adf879facbb866bcbb'}\n\n```{.julia .cell-code}\nX = transpose(reshape(convert(Vector{Float32}, arfima(1000,0.5, 0.3, SVector(-0.7))), 100, 10));\nxtrain = X[:, collect(1:2:100)];\nytrain = X[:, collect(2:2:100)];\n```\n:::\n\n\nCreate model (similar to Keras):\n\n\n::: {.cell hash='rnn_cache/html/unnamed-chunk-8_e547111b47fbcd4f2ac92eb63f870866'}\n\n```{.julia .cell-code}\nmodel = Chain(\n  GRU(10=>50),\n  Dense(50, 10)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChain(\n  Recur(\n    GRUCell(10 => 50),                  # 9_200 parameters\n  ),\n  Dense(50 => 10),                      # 510 parameters\n)         # Total: 6 trainable arrays, 9_710 parameters,\n          # plus 1 non-trainable, 50 parameters, summarysize 38.297 KiB.\n```\n:::\n:::\n\n\nTrain/optimize Model:\n\n\n::: {.cell hash='rnn_cache/html/unnamed-chunk-9_92ff3129b03974245bd29719b77cbcef'}\n\n```{.julia .cell-code}\nparameters = Flux.params(model);\noptimizer = ADAM(0.01);\n\nfor epoch in 1:10\n    Flux.reset!(model); \n    grads = gradient(parameters) do \n        Pred = [model( xtrain[:,i]) for i in 1:50];\n        loss = mean([mse(Pred[i], ytrain[:,i]) for i in 1:50]);\n        println(loss);\n        loss\n    end\n    Flux.update!(optimizer, parameters, grads);\nend\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.5030231\n0.40875202\n0.34945393\n0.31208286\n0.28855073\n0.27399114\n0.2656516\n0.2610054\n0.25678155\n0.25014922\n```\n:::\n:::\n\n\nPredictions:\n\n\n::: {.cell hash='rnn_cache/html/unnamed-chunk-10_fe5af368978b17ddb9d1d1e517625fa5'}\n\n```{.julia .cell-code}\nPred = [model( xtrain[:,i]) for i in 1:50];\nPred[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10-element Vector{Float32}:\n -0.15317366\n  0.6826891\n  0.10764982\n  1.342341\n  0.4174205\n -0.35652244\n -0.5429363\n -0.72320706\n  0.1646748\n  0.08904388\n```\n:::\n:::\n\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}