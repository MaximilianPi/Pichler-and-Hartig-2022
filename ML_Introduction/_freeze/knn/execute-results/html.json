{
  "hash": "b93cc420182441d86f61891bed73ad3a",
  "result": {
    "markdown": "# k-nearest-neighbor\n\n\n\n\n\n\nThe k-nearest-neighbor algorithm doesn't really learn from the data, predictions for new observations are made based on the class affiliation (or response value) of the nearest neighbors, e.g. by majority voting or averaging. The nearest neighbors are found by calculating the distance of the new observation to all observations in the train dataset.\n\nIn the following we use the 'kknn' package (@kknn) (Python: 'scikit-learn' (@scikit-learn), Julia: 'MLJ' (@anthony_blaom_2019_3541506)). Different to other ML packages we can provide here already the test dataset in the fit function.\n\n## Classification\n\n::: panel-tabset\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(kknn)\nX = scale(iris[,1:4])\nY = iris[,5,drop=FALSE]\ndata = cbind(Y, X)\n\nknn = kknn(Species~., train = data, test = data) \n```\n:::\n\n\nMake predictions (class probabilities):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(knn$prob, n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     setosa versicolor virginica\n[1,]      1          0         0\n[2,]      1          0         0\n[3,]      1          0         0\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\nmodel = KNeighborsClassifier().fit(X, Y)\n\n# Make predictions:\n\nmodel.predict_proba(X)[0:10,:]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.]])\n```\n:::\n:::\n\n\n\n### Julia\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nimport StatsBase;\nusing MLJ;\nkNN_classifier = @load KNNClassifier pkg=NearestNeighborModels;\nusing RDatasets;\nusing StatsBase;\nusing DataFrames;\n```\n:::\n\n::: {.cell}\n\n```{.julia .cell-code}\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 1:4]);\nY = iris[:, 5];\n```\n:::\n\n\nModels:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nmodel = fit!(machine(kNN_classifier(), X, Y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrained Machine; caches model-specific representations of data\n  model: KNNClassifier(K = 5, …)\n  args: \n    1:\tSource @749 ⏎ Table{AbstractVector{Continuous}}\n    2:\tSource @020 ⏎ AbstractVector{Multiclass{3}}\n```\n:::\n:::\n\n\nPredictions:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nMLJ.predict(model, X)[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n```\n:::\n:::\n\n\n\n:::\n\n## Regression\n\n::: panel-tabset\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(e1071)\nX = scale(iris[,2:4])\ndata = cbind(iris[,1,drop=FALSE], X)\n\nknn = kknn(Sepal.Length~., train = data, test = data) \n```\n:::\n\n\nMake predictions (class probabilities):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(predict(knn), n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.188492 4.739986 4.685332\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\nmodel = KNeighborsRegressor().fit(X, Y)\n\n# Make predictions:\n\nmodel.predict(X)[0:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([5.18, 4.78, 4.68, 4.76, 4.98, 5.34, 5.06, 5.1 , 4.7 , 4.8 ])\n```\n:::\n:::\n\n\n### Julia\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nimport StatsBase;\nusing MLJ;\nkNN_regressor =  @load KNNRegressor pkg=NearestNeighborModels;\nusing RDatasets;\nusing DataFrames;\n```\n:::\n\n::: {.cell}\n\n```{.julia .cell-code}\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 2:4]);\nY = iris[:, 1];\n```\n:::\n\n\nModel:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nmodel = fit!(machine(kNN_regressor(), X, Y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrained Machine; caches model-specific representations of data\n  model: KNNRegressor(K = 5, …)\n  args: \n    1:\tSource @797 ⏎ Table{AbstractVector{Continuous}}\n    2:\tSource @974 ⏎ AbstractVector{Continuous}\n```\n:::\n:::\n\n\nPredictions:\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nMLJ.predict(model, X)[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5-element Vector{Float64}:\n 5.18\n 4.779999999999999\n 4.68\n 4.82\n 5.0200000000000005\n```\n:::\n:::\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}