
# Graph (convolutional) neural networks

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
reticulate::use_condaenv("r-sjsdm", required = TRUE)
#reticulate::use_python("/home/maxpichler/miniconda3/envs/r-sjsdm/bin/python", required = TRUE)
```

Graph neural networks (GNN) is a young representative of the deep neural network family but is receiving more and more attention in the last years because of their ability to process non-Euclidean data such as graphs.

Currently there is no R package for GNNs available. However, we can use the 'reticulate' package (@reticulate) to use the python packages 'torch' and 'torch_geometric' (@NEURIPS2019_9015, 2019; @Fey_Lenssen_2019).

The following example was mostly adapted from the 'Node Classification with Graph Neural Networks' example from the torch_geometric documentation (<https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html>).

The dataset is also provided by the 'torch_geometric' package and consists of molecules presented as graphs and the task is to predict whether HIV virus replication is inhibited by the molecule or not (classification, binary classification).

We have not implemented this example in Julia because there is not yet a well-established library for GNNs.


::: panel-tabset
### R

```{r, eval=TRUE}
library(reticulate)
# Load python packages torch and torch_geometric via the reticulate R package
torch = import("torch") 
torch_geometric = import("torch_geometric")

# helper functions from the torch_geometric modules
GCNConv = torch_geometric$nn$GCNConv
global_mean_pool = torch_geometric$nn$global_mean_pool


# Download the MUTAG TUDataset
dataset = torch_geometric$datasets$TUDataset(root='data/TUDataset', 
                                             name='MUTAG')
dataloader = torch_geometric$loader$DataLoader(dataset, 
                                               batch_size=64L,
                                               shuffle=TRUE)

# Create the model with a python class
# There are two classes in the response variable
GCN = PyClass(
  "GCN", 
   inherit = torch$nn$Module, 
   defs = list(
       `__init__` = function(self, hidden_channels) {
         super()$`__init__`()
         torch$manual_seed(42L)
         self$conv = GCNConv(dataset$num_node_features, hidden_channels)
         self$linear = torch$nn$Linear(hidden_channels, dataset$num_classes)
         NULL
       },
       forward = function(self, x, edge_index, batch) {
         x = self$conv(x, edge_index)
         x = x$relu()
         x = global_mean_pool(x, batch)
         
         x = torch$nn$functional$dropout(x, p = 0.5, training=self$training)
         x = self$linear(x)
         return(x)
       }
   ))
```

Training loop:

```{r,eval=TRUE}
# create model object
model = GCN(hidden_channels = 64L)

# get optimizer and loss function
optimizer = torch$optim$Adamax(model$parameters(), lr = 0.01)
loss_func = torch$nn$CrossEntropyLoss()

# set model into training mode (because of the dropout layer)
model$train()

# train model
for(e in 1:50) {
  iterator = reticulate::as_iterator(dataloader)
  coro::loop(for (b in iterator) { 
     pred = model(b$x, b$edge_index, b$batch)
     loss = loss_func(pred, b$y)
     loss$backward()
     optimizer$step()
     optimizer$zero_grad()
  })
  if(e %% 10 ==0) cat(paste0("Epoch: ",e," Loss: ", round(loss$item()[1], 4), "\n"))
}
```

Make predictions:

```{r,eval=TRUE}
preds = list()
test = torch_geometric$loader$DataLoader(dataset, batch_size=64L,shuffle=FALSE)
iterator = reticulate::as_iterator(test)
model$eval()
counter = 1
coro::loop(for (b in iterator) {
  preds[[counter]] = model(b$x, b$edge_index, b$batch)
  counter <<- counter + 1
  })
head(torch$concat(preds)$sigmoid()$data$cpu()$numpy(), n = 3)
```

### Python

```{python, eval=TRUE}
# Load python packages torch and torch_geometric via the reticulate R package
import torch
import torch_geometric

# helper functions from the torch_geometric modules
GCNConv = torch_geometric.nn.GCNConv
global_mean_pool = torch_geometric.nn.global_mean_pool


# Download the MUTAG TUDataset
dataset = torch_geometric.datasets.TUDataset(root='data/TUDataset', 
                                             name='MUTAG')
dataloader = torch_geometric.loader.DataLoader(dataset, 
                                               batch_size=64,
                                               shuffle=True)

# Create the model with a python class
# There are two classes in the response variable
class GCN(torch.nn.Module):
    def __init__(self, hidden_channels):
         super().__init__()
         torch.manual_seed(42)
         self.conv = GCNConv(dataset.num_node_features, hidden_channels)
         self.linear = torch.nn.Linear(hidden_channels, dataset.num_classes)
         
    def forward(self, x, edge_index, batch):
        x = self.conv(x, edge_index)
        x = x.relu()
        x = global_mean_pool(x, batch)
        x = torch.nn.functional.dropout(x, p = 0.5, training=self.training)
        x = self.linear(x)
        return x
```

Training loop:

```{python,eval=TRUE}
# create model object
model = GCN(hidden_channels = 64)

# get optimizer and loss function
optimizer = torch.optim.Adamax(model.parameters(), lr = 0.01)
loss_func = torch.nn.CrossEntropyLoss()

# set model into training mode (because of the dropout layer)
model.train()

# train model
for e in range(50):
  for b in dataloader:
  
    pred = model(b.x, b.edge_index, b.batch)
    loss = loss_func(pred, b.y)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
     
  if e % 10 ==0:
    print("Epoch: ", e ," Loss: ", loss.item(), "\n")
```

Make predictions:

```{python,eval=TRUE}
preds = []
test = torch_geometric.loader.DataLoader(dataset, batch_size=64,shuffle=False)
model.eval()
counter = 1
for b in test:
  preds.append( model(b.x, b.edge_index, b.batch) )
  
  
torch.concat(preds).sigmoid().data.cpu().numpy()[0:10]
```
:::
