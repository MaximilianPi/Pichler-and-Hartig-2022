[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Machine Learning and Deep Learning",
    "section": "",
    "text": "The goal of this book is to provide short code examples for all common supervised ML algorithms. All examples are shown in the R programming language and are demonstrated at the example of the iris dataset (4 continuous and 1 nominal variables). We demonstrate a) how to apply the algorithms on classification tasks (response = nominal, three species) and how to generate class specific (quasi) probability predictions and b) how to apply the algorithms to regression tasks (response = continuous) and make continuous predictions."
  },
  {
    "objectID": "elastic_net.html",
    "href": "elastic_net.html",
    "title": "1  Ridge, LASSO, and elastic-net regression",
    "section": "",
    "text": "We can use the ‘glmnet’ R package (Simon et al. (2011)) for Ridge, LASSO, or elastic-net regularization. The ‘glmnet’ package supports different response families including ‘gaussian’, ‘binomial’ and ‘Poisson’. The strength of the regularization is set by the ‘lambda’ argument (\\(\\lambda\\)) and the weighting between Ridge and LASSO regularization by the ‘alpha’ parameter (\\(\\alpha\\)):\n\\[\n\\lambda*[(1 - \\alpha)\\|\\beta\\|_1 + \\alpha\\|\\beta||^2]\n\\] Setting alpha = 0 turns off the LASSO and alpha = 1 the Ridge. Alphas between (0,1) will use both regularization types, turning the model into an elastic-net regularization.\nWhen using regularization, it is important to scale all features otherwise effects for features that are on a larger scale are stronger regularized.\nIn python, the ‘scikit-learn’ package provides an interface for many different ML algorithms, including elastic-net regression models (Pedregosa et al. (2011)).\nThe ‘MLJ’ package provides a generic interface for different ML algorithms (Blaom et al. (2019)). Elastic-net regression models can be accessed via the ‘MLJLinearModels’ package within MLJ."
  },
  {
    "objectID": "elastic_net.html#classification",
    "href": "elastic_net.html#classification",
    "title": "1  Ridge, LASSO, and elastic-net regression",
    "section": "1.1 Classification",
    "text": "1.1 Classification\nBuild models (for regularization it is important to scale the features):\n\nRPythonJulia\n\n\n\nlibrary(glmnet)\nX = scale(iris[,1:4])\nY = iris$Species\n\n# Ridge:\nridge = glmnet(X, Y, family = \"multinomial\", alpha = 0, lambda = 0.01)\n\n# LASSO:\nlasso = glmnet(X, Y, family = \"multinomial\", alpha = 1, lambda = 0.01)\n\n# Elastic-net:\nelastic = glmnet(X, Y, family = \"multinomial\", alpha = 0.5, lambda = 0.01)\n\nMake predictions (class probabilities):\n\nhead(predict(lasso, newx = X, type = \"response\")[,,1], n = 3)\n\n        setosa versicolor    virginica\n[1,] 0.9858987 0.01410131 3.438452e-09\n[2,] 0.9668897 0.03311031 1.397684e-08\n[3,] 0.9815369 0.01846312 5.279315e-09\n\n\n\n\nIn the sklearn implementation the regularization strength parameter ‘C’ corresponds to the lambda parameter from glmnet:\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\nModels:\n\n\n# Ridge:\nridge = LogisticRegression(multi_class='multinomial', \n                           penalty = \"l2\", \n                           C = 0.01, \n                           solver=\"saga\")\nridge.fit(X, Y)\n\n# LASSO:\n\nLogisticRegression(C=0.01, multi_class='multinomial', solver='saga')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(C=0.01, multi_class='multinomial', solver='saga')\n\nlasso = LogisticRegression(multi_class='multinomial', \n                           penalty = \"l1\", \n                           C = 0.01, \n                           solver=\"saga\")\nlasso.fit(X, Y)\n\n# Elastic-net:\n\nLogisticRegression(C=0.01, multi_class='multinomial', penalty='l1',\n                   solver='saga')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(C=0.01, multi_class='multinomial', penalty='l1',\n                   solver='saga')\n\nelastic = LogisticRegression(multi_class='multinomial', \n                             penalty = \"elasticnet\", \n                             C = 0.01, \n                             l1_ratio=0.5, \n                             solver=\"saga\")\nelastic.fit(X, Y)\n\nLogisticRegression(C=0.01, l1_ratio=0.5, multi_class='multinomial',\n                   penalty='elasticnet', solver='saga')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(C=0.01, l1_ratio=0.5, multi_class='multinomial',\n                   penalty='elasticnet', solver='saga')\n\n\nPredictions (probabilities):\n\n\nlasso.predict_proba(X)[0:5,:]\n\narray([[0.34946216, 0.33816677, 0.31237106],\n       [0.34946216, 0.33816677, 0.31237106],\n       [0.34946216, 0.33816677, 0.31237106],\n       [0.34946216, 0.33816677, 0.31237106],\n       [0.34946216, 0.33816677, 0.31237106]])\n\n\n\n\n\nimport StatsBase;\nusing MLJ;\nusing MLJLinearModels;\n@load MultinomialClassifier pkg=MLJLinearModels;\nusing RDatasets;\nusing StatsBase;\nusing DataFrames;\n\n\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 1:4]);\nY = iris[:, 5];\n\nModels:\n\n\n# Ridge\nridge = fit!(machine(MultinomialClassifier(lambda = 0.01, penalty = \"l2\"), X, Y));\n\n# Lasso\nlasso = fit!(machine(MultinomialClassifier(lambda = 0.01, penalty = \"l1\"), X, Y));\n\n\n# Elastic-net\nelastic = fit!(machine(MultinomialClassifier(lambda = 0.01, gamma = 0.01, penalty = \"en\"), X, Y));\n\nPredictions:\n\nMLJ.predict(lasso, X)[1:5]\n\n5-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>0.992, versicolor=>0.00829, virginica=>2.2e-9)\n UnivariateFinite{Multiclass{3}}(setosa=>0.969, versicolor=>0.031, virginica=>1.4e-8)\n UnivariateFinite{Multiclass{3}}(setosa=>0.985, versicolor=>0.0149, virginica=>4.6e-9)\n UnivariateFinite{Multiclass{3}}(setosa=>0.971, versicolor=>0.0293, virginica=>1.41e-8)\n UnivariateFinite{Multiclass{3}}(setosa=>0.994, versicolor=>0.00635, virginica=>1.51e-9)"
  },
  {
    "objectID": "elastic_net.html#regression",
    "href": "elastic_net.html#regression",
    "title": "1  Ridge, LASSO, and elastic-net regression",
    "section": "1.2 Regression",
    "text": "1.2 Regression\n\nRPythonJulia\n\n\n\nX = scale(iris[,2:4])\nY = iris[,1]\n\n# Ridge:\nridge = glmnet(X, Y, family = gaussian(), alpha = 0, lambda = 0.01)\n\n# LASSO:\nlasso = glmnet(X, Y, family = gaussian(), alpha = 1, lambda = 0.01)\n\n# Elastic-net:\nelastic = glmnet(X, Y, family = gaussian(), alpha = 0.5, lambda = 0.01)\n\nMake predictions (class probabilities):\n\nhead(predict(lasso, newx = X), n = 3)\n\n           s0\n[1,] 5.006484\n[2,] 4.720600\n[3,] 4.781548\n\n\n\n\nFor regressions we can use the ElasticNet model class, here, however, lambda corresponds to alpha and l1_ratio to the alpha parameter.\n\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\n\n# Ridge:\nridge = ElasticNet(alpha = 0.01,\n                   l1_ratio = 0.011)\nridge.fit(X, Y)\n\n# LASSO:\n\nElasticNet(alpha=0.01, l1_ratio=0.011)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ElasticNetElasticNet(alpha=0.01, l1_ratio=0.011)\n\nlasso = ElasticNet(alpha = 0.01,\n                   l1_ratio = 1.0)\nlasso.fit(X, Y)\n\n# Elastic-net:\n\nElasticNet(alpha=0.01, l1_ratio=1.0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ElasticNetElasticNet(alpha=0.01, l1_ratio=1.0)\n\nelastic = ElasticNet(alpha = 0.01,\n                     l1_ratio = 0.5)\nelastic.fit(X, Y)\n\n# Make predictions:\n\nElasticNet(alpha=0.01)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ElasticNetElasticNet(alpha=0.01)\n\nlasso.predict(X)[0:10]\n\narray([5.0064384 , 4.72032938, 4.78125162, 4.83107256, 5.06366021,\n       5.36149937, 4.93202142, 5.00273797, 4.66310758, 4.84826774])\n\n\n\n\n\nimport StatsBase;\nusing MLJ;\nusing MLJLinearModels;\n@load LassoRegressor pkg=MLJLinearModels;\n@load RidgeRegressor pkg=MLJLinearModels;\n@load ElasticNetRegressor pkg=MLJLinearModels;\nusing RDatasets;\nusing DataFrames;\n\n\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 2:4]);\nY = iris[:, 1];\n\nModels:\n\n# Ridge\nridge = fit!(machine(RidgeRegressor(lambda = 0.01), X, Y));\n\n# Lasso\nlasso = fit!(machine(LassoRegressor(lambda = 0.01), X, Y));\n\n\n# Elastic-net\nelastic = fit!(machine(ElasticNetRegressor(lambda = 0.01, gamma = 0.01), X, Y));\n\nPredictions (probabilities):\n\nMLJ.predict(lasso, X)[1:5]\n\n5-element Vector{Float64}:\n 5.007709152258313\n 4.711530001523257\n 4.770100849643125\n 4.830666643844422\n 5.0669449824053245\n\n\n\n\n\n\n\n\n\nBlaom, Anthony, Franz Kiraly, Thibaut Lienart, and Sebastian Vollmer. 2019. Alan-Turing-Institute/MLJ.jl: V0.5.3 (version v0.5.3). Zenodo. https://doi.org/10.5281/zenodo.3541506.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nSimon, Noah, Jerome Friedman, Trevor Hastie, and Rob Tibshirani. 2011. “Regularization Paths for Cox’s Proportional Hazards Model via Coordinate Descent.” Journal of Statistical Software 39 (5): 1–13. https://doi.org/10.18637/jss.v039.i05."
  },
  {
    "objectID": "svm.html",
    "href": "svm.html",
    "title": "2  Support Vector Machines",
    "section": "",
    "text": "The support vector machine (SVM) algorithm estimates hyper-planes to separate our response species. In the following we use the ‘e1071’ package which supports a variety of different SVM algorithms (Meyer et al. (2022)) (Python: ‘scikit-learn’ (Pedregosa et al. (2011)), Julia: ‘MLJ’ (Blaom et al. (2019)))."
  },
  {
    "objectID": "svm.html#classification",
    "href": "svm.html#classification",
    "title": "2  Support Vector Machines",
    "section": "2.1 Classification",
    "text": "2.1 Classification\n\nRPythonJulia\n\n\n\nlibrary(e1071)\nX = scale(iris[,1:4])\nY = iris$Species\n\nsv = svm(X, Y, probability = TRUE) \nsummary(sv)\n\n\nCall:\nsvm.default(x = X, y = Y, probability = TRUE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  51\n\n ( 8 22 21 )\n\n\nNumber of Classes:  3 \n\nLevels: \n setosa versicolor virginica\n\n\nMake predictions (class probabilities):\n\nhead(attr(predict(sv, newdata = X, probability = TRUE), \"probabilities\"), n = 3)\n\n     setosa versicolor   virginica\n1 0.9805061 0.01078548 0.008708392\n2 0.9733815 0.01726277 0.009355782\n3 0.9792178 0.01138534 0.009396853\n\n\n\n\n\nfrom sklearn import svm\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\nmodel = svm.SVC(probability=True).fit(X, Y)\n\n# Make predictions (class probabilities):\n\nmodel.predict_proba(X)[0:10,:]\n\narray([[0.98010734, 0.01119254, 0.00870012],\n       [0.97290852, 0.01775589, 0.00933559],\n       [0.97887363, 0.01174829, 0.00937808],\n       [0.97488338, 0.01505638, 0.01006024],\n       [0.97925052, 0.01154893, 0.00920055],\n       [0.97383204, 0.01661416, 0.00955379],\n       [0.97548544, 0.0133769 , 0.01113766],\n       [0.98001151, 0.01129966, 0.00868883],\n       [0.96617856, 0.02211025, 0.01171119],\n       [0.97667451, 0.01385397, 0.00947152]])\n\n\n\n\n\nimport StatsBase;\nusing MLJ;\nSVM_classifier = @load NuSVC pkg=LIBSVM;\nusing RDatasets;\nusing StatsBase;\nusing DataFrames;\n\n\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 1:4]);\nY = iris[:, 5];\n\nModels:\n\nmodel = fit!(machine(SVM_classifier(), X, Y))\n\ntrained Machine; caches model-specific representations of data\n  model: NuSVC(kernel = RadialBasis, …)\n  args: \n    1:  Source @942 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @621 ⏎ AbstractVector{Multiclass{3}}\n\n\nPredictions:\n\nMLJ.predict(model, X)[1:5]\n\n5-element CategoricalArrays.CategoricalArray{String,1,UInt8}:\n \"setosa\"\n \"setosa\"\n \"setosa\"\n \"setosa\"\n \"setosa\""
  },
  {
    "objectID": "svm.html#regression",
    "href": "svm.html#regression",
    "title": "2  Support Vector Machines",
    "section": "2.2 Regression",
    "text": "2.2 Regression\n\nRPythonJulia\n\n\n\nlibrary(e1071)\nX = scale(iris[,2:4])\nY = iris[,1]\n\nsv = svm(X, Y) \nsummary(sv)\n\n\nCall:\nsvm.default(x = X, y = Y)\n\n\nParameters:\n   SVM-Type:  eps-regression \n SVM-Kernel:  radial \n       cost:  1 \n      gamma:  0.3333333 \n    epsilon:  0.1 \n\n\nNumber of Support Vectors:  124\n\n\nMake predictions (class probabilities):\n\nhead(predict(sv, newdata = X), n = 3)\n\n       1        2        3 \n5.042085 4.711768 4.836291 \n\n\n\n\n\nfrom sklearn import svm\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\nmodel = svm.SVR().fit(X, Y)\n\n# Make predictions:\n\nmodel.predict(X)[0:10]\n\narray([5.03583855, 4.69496586, 4.81438855, 4.77951854, 5.10018373,\n       5.29981857, 4.97308737, 4.98199033, 4.63701656, 4.78431078])\n\n\n\n\n\nimport StatsBase;\nusing MLJ;\nSVM_regressor =  @load NuSVR pkg=LIBSVM;\nusing RDatasets;\nusing DataFrames;\n\n\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 2:4]);\nY = iris[:, 1];\n\nModel:\n\nmodel = fit!(machine(SVM_regressor(), X, Y))\n\ntrained Machine; caches model-specific representations of data\n  model: NuSVR(kernel = RadialBasis, …)\n  args: \n    1:  Source @875 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @099 ⏎ AbstractVector{Continuous}\n\n\nPredictions:\n\nMLJ.predict(model, X)[1:5]\n\n5-element Vector{Float64}:\n 5.058471741834634\n 4.6717512552719604\n 4.799641470830148\n 4.75734816087994\n 5.133728219775252\n\n\n\n\n\n\n\n\n\nBlaom, Anthony, Franz Kiraly, Thibaut Lienart, and Sebastian Vollmer. 2019. Alan-Turing-Institute/MLJ.jl: V0.5.3 (version v0.5.3). Zenodo. https://doi.org/10.5281/zenodo.3541506.\n\n\nMeyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and Friedrich Leisch. 2022. E1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien. https://CRAN.R-project.org/package=e1071.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30."
  },
  {
    "objectID": "knn.html",
    "href": "knn.html",
    "title": "3  k-nearest-neighbor",
    "section": "",
    "text": "The k-nearest-neighbor algorithm doesn’t really learn from the data, predictions for new observations are made based on the class affiliation (or response value) of the nearest neighbors, e.g. by majority voting or averaging. The nearest neighbors are found by calculating the distance of the new observation to all observations in the train dataset.\nIn the following we use the ‘kknn’ package (Schliep and Hechenbichler (2016)) (Python: ‘scikit-learn’ (Pedregosa et al. (2011)), Julia: ‘MLJ’ (Blaom et al. (2019))). Different to other ML packages we can provide here already the test dataset in the fit function."
  },
  {
    "objectID": "knn.html#classification",
    "href": "knn.html#classification",
    "title": "3  k-nearest-neighbor",
    "section": "3.1 Classification",
    "text": "3.1 Classification\n\nRPythonJulia\n\n\n\nlibrary(kknn)\nX = scale(iris[,1:4])\nY = iris[,5,drop=FALSE]\ndata = cbind(Y, X)\n\nknn = kknn(Species~., train = data, test = data) \n\nMake predictions (class probabilities):\n\nhead(knn$prob, n = 3)\n\n     setosa versicolor virginica\n[1,]      1          0         0\n[2,]      1          0         0\n[3,]      1          0         0\n\n\n\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\nmodel = KNeighborsClassifier().fit(X, Y)\n\n# Make predictions:\n\nmodel.predict_proba(X)[0:10,:]\n\narray([[1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.]])\n\n\n\n\n\nimport StatsBase;\nusing MLJ;\nkNN_classifier = @load KNNClassifier pkg=NearestNeighborModels;\nusing RDatasets;\nusing StatsBase;\nusing DataFrames;\n\n\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 1:4]);\nY = iris[:, 5];\n\nModels:\n\nmodel = fit!(machine(kNN_classifier(), X, Y))\n\ntrained Machine; caches model-specific representations of data\n  model: KNNClassifier(K = 5, …)\n  args: \n    1:  Source @082 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @399 ⏎ AbstractVector{Multiclass{3}}\n\n\nPredictions:\n\nMLJ.predict(model, X)[1:5]\n\n5-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)"
  },
  {
    "objectID": "knn.html#regression",
    "href": "knn.html#regression",
    "title": "3  k-nearest-neighbor",
    "section": "3.2 Regression",
    "text": "3.2 Regression\n\nRPythonJulia\n\n\n\nlibrary(e1071)\nX = scale(iris[,2:4])\ndata = cbind(iris[,1,drop=FALSE], X)\n\nknn = kknn(Sepal.Length~., train = data, test = data) \n\nMake predictions (class probabilities):\n\nhead(predict(knn), n = 3)\n\n[1] 5.188492 4.739986 4.685332\n\n\n\n\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\nmodel = KNeighborsRegressor().fit(X, Y)\n\n# Make predictions:\n\nmodel.predict(X)[0:10]\n\narray([5.18, 4.78, 4.68, 4.76, 4.98, 5.34, 5.06, 5.1 , 4.7 , 4.8 ])\n\n\n\n\n\nimport StatsBase;\nusing MLJ;\nkNN_regressor =  @load KNNRegressor pkg=NearestNeighborModels;\nusing RDatasets;\nusing DataFrames;\n\n\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 2:4]);\nY = iris[:, 1];\n\nModel:\n\nmodel = fit!(machine(kNN_regressor(), X, Y))\n\ntrained Machine; caches model-specific representations of data\n  model: KNNRegressor(K = 5, …)\n  args: \n    1:  Source @112 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @138 ⏎ AbstractVector{Continuous}\n\n\nPredictions:\n\nMLJ.predict(model, X)[1:5]\n\n5-element Vector{Float64}:\n 5.18\n 4.779999999999999\n 4.68\n 4.82\n 5.0200000000000005\n\n\n\n\n\n\n\n\n\nBlaom, Anthony, Franz Kiraly, Thibaut Lienart, and Sebastian Vollmer. 2019. Alan-Turing-Institute/MLJ.jl: V0.5.3 (version v0.5.3). Zenodo. https://doi.org/10.5281/zenodo.3541506.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nSchliep, Klaus, and Klaus Hechenbichler. 2016. Kknn: Weighted k-Nearest Neighbors. https://CRAN.R-project.org/package=kknn."
  },
  {
    "objectID": "rf.html",
    "href": "rf.html",
    "title": "4  Random forest",
    "section": "",
    "text": "The random forest (RF) algorithm is probably one of the most famous ML algorithms, and not without reason. Compared to other well performing algorithms, the RF algorithm has only a few hyper-parameters and because of the bagging and the random sampling of available variables in for the node splits, it has a well working internal complexity adaption.\nIn the following, we use the ‘ranger’ package (Wright and Ziegler (2017)) (Python: ‘scikit-learn’ (Pedregosa et al. (2011)), Julia: ‘MLJ’ (Blaom et al. (2019)))."
  },
  {
    "objectID": "rf.html#classification",
    "href": "rf.html#classification",
    "title": "4  Random forest",
    "section": "4.1 Classification",
    "text": "4.1 Classification\n\nRPythonJulia\n\n\n\nlibrary(ranger)\nX = iris[,1:4]\nY = iris[,5,drop=FALSE]\ndata = cbind(Y, X)\n\nrf = ranger(Species~., data = data, probability = TRUE, importance = \"impurity\")\n\nShow feature importances:\n\nimportance(rf)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    8.175576     1.178026    43.096449    43.261933 \n\n\nMake predictions (class probabilities):\n\nhead(predict(rf, data = data)$predictions, n = 3)\n\n        setosa   versicolor    virginica\n[1,] 1.0000000 0.0000000000 0.0000000000\n[2,] 0.9986389 0.0009166667 0.0004444444\n[3,] 1.0000000 0.0000000000 0.0000000000\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\nmodel = RandomForestClassifier().fit(X, Y)\n\nFeature importance\n\nprint(model.feature_importances_)\n\n[0.10290567 0.02364397 0.38585687 0.4875935 ]\n\n\nMake predictions:\n\nmodel.predict_proba(X)[0:10,:]\n\narray([[1.  , 0.  , 0.  ],\n       [1.  , 0.  , 0.  ],\n       [1.  , 0.  , 0.  ],\n       [1.  , 0.  , 0.  ],\n       [1.  , 0.  , 0.  ],\n       [1.  , 0.  , 0.  ],\n       [1.  , 0.  , 0.  ],\n       [1.  , 0.  , 0.  ],\n       [0.99, 0.  , 0.01],\n       [1.  , 0.  , 0.  ]])\n\n\n\n\n\nimport StatsBase;\nusing MLJ;\nRF_classifier = @load RandomForestClassifier pkg=DecisionTree;\nusing RDatasets;\nusing StatsBase;\nusing DataFrames;\n\n\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 1:4]);\nY = iris[:, 5];\n\nModels:\n\nmodel = fit!(machine(RF_classifier(), X, Y))\n\ntrained Machine; caches model-specific representations of data\n  model: RandomForestClassifier(max_depth = -1, …)\n  args: \n    1:  Source @390 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @805 ⏎ AbstractVector{Multiclass{3}}\n\n\nFeature importance:\n\nfeature_importances(model)\n\n4-element Vector{Pair{Symbol, Float64}}:\n :PetalLength => 0.6973071180941611\n  :PetalWidth => 0.25359114793092297\n :SepalLength => 0.04389338912322112\n  :SepalWidth => 0.005208344851694811\n\n\nPredictions:\n\nMLJ.predict(model, X)[1:5]\n\n5-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)"
  },
  {
    "objectID": "rf.html#regression",
    "href": "rf.html#regression",
    "title": "4  Random forest",
    "section": "4.2 Regression",
    "text": "4.2 Regression\n\nRPythonJulia\n\n\n\nlibrary(ranger)\nX = iris[,2:4]\ndata = cbind(iris[,1,drop=FALSE], X)\n\nrf = ranger(Sepal.Length~., data = data, importance = \"impurity\")\n\nShow feature importances:\n\nimportance(rf)\n\n Sepal.Width Petal.Length  Petal.Width \n    12.17374     45.58927     37.51995 \n\n\nMake predictions (class probabilities):\n\nhead(predict(rf, data = data)$predictions, n = 3)\n\n[1] 5.108278 4.776606 4.653740\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\nmodel = RandomForestRegressor().fit(X, Y)\n\nFeature importance:\n\nprint(model.feature_importances_)\n\n[0.08088907 0.85602898 0.06308195]\n\n\nMake predictions:\n\nmodel.predict(X)[0:10]\n\narray([5.118     , 4.7942    , 4.58115   , 4.75405   , 5.031     ,\n       5.416     , 4.80333333, 5.05493333, 4.6002    , 4.8445    ])\n\n\n\n\n\nimport StatsBase;\nusing MLJ;\nRF_regressor = @load RandomForestRegressor pkg=DecisionTree;\nusing RDatasets;\nusing DataFrames;\n\n\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 2:4]);\nY = iris[:, 1];\n\nModel:\n\nmodel = fit!(machine(RF_regressor(), X, Y))\n\ntrained Machine; caches model-specific representations of data\n  model: RandomForestRegressor(max_depth = -1, …)\n  args: \n    1:  Source @645 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @682 ⏎ AbstractVector{Continuous}\n\n\nFeature importance:\n\nfeature_importances(model)\n\n3-element Vector{Pair{Symbol, Float64}}:\n :PetalLength => 0.7388544773497039\n  :PetalWidth => 0.1792108665840036\n  :SepalWidth => 0.08193465606629259\n\n\nPredictions:\n\nMLJ.predict(model, X)[1:5]\n\n5-element Vector{Float64}:\n 5.17\n 4.640000000000001\n 4.535000000000001\n 4.71\n 5.0\n\n\n\n\n\n\n\n\n\nBlaom, Anthony, Franz Kiraly, Thibaut Lienart, and Sebastian Vollmer. 2019. Alan-Turing-Institute/MLJ.jl: V0.5.3 (version v0.5.3). Zenodo. https://doi.org/10.5281/zenodo.3541506.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nWright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” Journal of Statistical Software 77 (1): 1–17. https://doi.org/10.18637/jss.v077.i01."
  },
  {
    "objectID": "brt.html",
    "href": "brt.html",
    "title": "5  Boosted gradient trees",
    "section": "",
    "text": "Boosted gradient machines achieve currently state-of-the-art performance for structured (tabular) data which makes them probably one of the most important algorithms for E&E where structured data dominates the field.\nIn the following, we use the ‘xgboost’ package (Chen et al. (2022)) (Python: ‘xgboost’ (Chen et al. (2022)), Julia: ‘MLJ’ (Blaom et al. (2019)))"
  },
  {
    "objectID": "brt.html#classification",
    "href": "brt.html#classification",
    "title": "5  Boosted gradient trees",
    "section": "5.1 Classification",
    "text": "5.1 Classification\n\nRPythonJulia\n\n\n\nlibrary(xgboost)\nX = as.matrix(iris[,1:4])\nY = as.integer(iris[,5]) - 1 # classes must be integers starting from 0\n\nxgdata = xgb.DMatrix(X, label = Y)\n\n# nrounds = number of trees in the ensemble\nbrt = xgboost(data = xgdata, \n              objective=\"multi:softprob\", \n              nrounds = 50, \n              num_class = 3,\n              verbose = 0)\n\nShow feature importances:\n\nxgb.importance(model = brt)\n\n        Feature        Gain      Cover Frequency\n1: Petal.Length 0.671879438 0.57441039 0.3792049\n2:  Petal.Width 0.311535837 0.29261084 0.3088685\n3:  Sepal.Width 0.010177107 0.04910115 0.1162080\n4: Sepal.Length 0.006407618 0.08387763 0.1957187\n\n\nMake predictions (class probabilities):\n\nhead(matrix(predict(brt, newdata = xgb.DMatrix(X)), ncol =3), n = 3)\n\n            [,1]        [,2]        [,3]\n[1,] 0.995287061 0.002195822 0.001027058\n[2,] 0.003323558 0.995396435 0.001592265\n[3,] 0.001389398 0.002407764 0.997380674\n\n\n\n\n\nimport xgboost as xgb\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\n# Parameters:\nparam = {\n  'max_depth':2, \n  'eta':1, \n  'objective':'multi:softmax' }\nnum_round = 50\n\nmodel = xgb.XGBClassifier(param, num_round, verbosity = 0).fit(X, Y)\n\n/home/max/miniconda3/envs/r-sjsdm/lib/python3.9/site-packages/xgboost/core.py:568: FutureWarning: Pass `objective, use_label_encoder` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n  warnings.warn(\n\n\nFeature importance\n\nmodel.feature_importances_\n\narray([0.00959796, 0.01645038, 0.6765859 , 0.29736578], dtype=float32)\n\n\nMake predictions:\n\nmodel.predict_proba(X)[0:10,:]\n\narray([[9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9636227e-01, 2.3820533e-03, 1.2557388e-03],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9679452e-01, 2.3830866e-03, 8.2237815e-04],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9636227e-01, 2.3820533e-03, 1.2557388e-03],\n       [9.9679452e-01, 2.3830866e-03, 8.2237815e-04]], dtype=float32)\n\n\n\n\n\nimport StatsBase;\nusing MLJ;\nBRT_classifier = @load XGBoostClassifier pkg=XGBoost;\nusing RDatasets;\nusing StatsBase;\nusing DataFrames;\n\n\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 1:4]);\nY = iris[:, 5];\n\nModels:\n\nmodel = fit!(machine(BRT_classifier(), X, Y))\n\ntrained Machine; caches model-specific representations of data\n  model: XGBoostClassifier(num_round = 100, …)\n  args: \n    1:  Source @141 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @384 ⏎ AbstractVector{Multiclass{3}}\n\n\nPredictions:\n\nMLJ.predict(model, X)[1:5]\n\n5-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float32}:\n UnivariateFinite{Multiclass{3}}(setosa=>0.997, versicolor=>0.00238, virginica=>0.000814)\n UnivariateFinite{Multiclass{3}}(setosa=>0.996, versicolor=>0.00238, virginica=>0.00126)\n UnivariateFinite{Multiclass{3}}(setosa=>0.997, versicolor=>0.00238, virginica=>0.000814)\n UnivariateFinite{Multiclass{3}}(setosa=>0.997, versicolor=>0.00238, virginica=>0.000822)\n UnivariateFinite{Multiclass{3}}(setosa=>0.997, versicolor=>0.00238, virginica=>0.000814)"
  },
  {
    "objectID": "brt.html#regression",
    "href": "brt.html#regression",
    "title": "5  Boosted gradient trees",
    "section": "5.2 Regression",
    "text": "5.2 Regression\n\nRPythonJulia\n\n\n\nlibrary(xgboost)\nX = as.matrix(iris[,2:4])\nY = iris[,1]\n\nxgdata = xgb.DMatrix(X, label = Y)\n\n# nrounds = number of trees in the ensemble\nbrt = xgboost(data = xgdata, \n              objective=\"reg:squarederror\", \n              nrounds = 50, \n              verbose = 0)\n\nShow feature importances:\n\nxgb.importance(model = brt)\n\n        Feature       Gain     Cover Frequency\n1: Petal.Length 0.86781219 0.4789538 0.3789062\n2:  Petal.Width 0.06987880 0.2128402 0.2626953\n3:  Sepal.Width 0.06230901 0.3082060 0.3583984\n\n\nMake predictions:\n\nhead(predict(brt, newdata = xgb.DMatrix(X), n = 3))\n\n[14:02:44] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n\n[1] 3.506606 3.506606 3.506606 3.506606 3.506606 3.506606\n\n\n\n\n\nimport xgboost as xgb\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\n# Parameters:\nmodel = xgb.XGBRegressor(\n  objective = 'reg:squarederror',\n  max_depth = 2,  \n  n_estimators = 50, \n  verbosity = 0).fit(X, Y)\n\nFeature importance:\n\nprint(model.feature_importances_)\n\n[0.08471056 0.835755   0.07953447]\n\n\nMake predictions:\n\nmodel.predict(X)[0:10]\n\narray([5.0407157, 4.6844926, 4.711238 , 4.917956 , 5.0407157, 5.450946 ,\n       4.928966 , 4.986462 , 4.6750975, 4.917956 ], dtype=float32)\n\n\n\n\n\nimport StatsBase;\nusing MLJ;\nBRT_regressor =  @load XGBoostRegressor pkg=XGBoost;\nusing RDatasets;\nusing DataFrames;\n\n\niris = dataset(\"datasets\", \"iris\");\nX = mapcols(StatsBase.zscore, iris[:, 2:4]);\nY = iris[:, 1];\n\nModel:\n\nmodel = fit!(machine(BRT_regressor(), X, Y))\n\ntrained Machine; caches model-specific representations of data\n  model: XGBoostRegressor(num_round = 100, …)\n  args: \n    1:  Source @935 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @070 ⏎ AbstractVector{Continuous}\n\n\nPredictions:\n\nMLJ.predict(model, X)[1:5]\n\n5-element Vector{Float32}:\n 5.1509466\n 4.8569074\n 4.551141\n 4.7587333\n 4.999504\n\n\n\n\n\n\n\n\n\nBlaom, Anthony, Franz Kiraly, Thibaut Lienart, and Sebastian Vollmer. 2019. Alan-Turing-Institute/MLJ.jl: V0.5.3 (version v0.5.3). Zenodo. https://doi.org/10.5281/zenodo.3541506.\n\n\nChen, Tianqi, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, et al. 2022. Xgboost: Extreme Gradient Boosting. https://CRAN.R-project.org/package=xgboost."
  },
  {
    "objectID": "dnn.html",
    "href": "dnn.html",
    "title": "6  Deep neural networks",
    "section": "",
    "text": "Deep neural networks, or more precisely here fully connected neural networks, can be flexibly built which makes their application more challenging than other ML algorithms.\nIn the following, we use the ‘keras’ (Allaire and Chollet (2022); Chollet et al. (2015)) (Python: ‘keras’ (Chollet et al. (2015)); Julia: ‘Flux’ (Innes et al. (2018))) package which is a higher level API on the python ‘tensorflow’ framework (Abadi et al. (2016))."
  },
  {
    "objectID": "dnn.html#classification",
    "href": "dnn.html#classification",
    "title": "6  Deep neural networks",
    "section": "6.1 Classification",
    "text": "6.1 Classification\n\nRPythonJulia\n\n\n\nlibrary(keras)\nX = scale(as.matrix(iris[,1:4]))\nY = as.integer(iris$Species)\n# We need to one hot encode our response classes\nYT = k_one_hot(Y-1L, num_classes = 3)\n\nDNN = keras_model_sequential() %>% \n  # first hidden layer\n  layer_dense(input_shape = ncol(X), \n              units = 10, \n              activation = \"relu\") %>% \n  # second hidden layer with regularization\n  layer_dense(units = 20, \n              activation = \"relu\",\n              kernel_regularizer = regularizer_l1()) %>% \n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\n  layer_dense(units = 3, \n              activation = \"softmax\")\n\n# print architecture\nsummary(DNN)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_2 (Dense)                    (None, 10)                      50          \n dense_1 (Dense)                    (None, 20)                      220         \n dense (Dense)                      (None, 3)                       63          \n================================================================================\nTotal params: 333\nTrainable params: 333\nNon-trainable params: 0\n________________________________________________________________________________\n\n# add loss function and optimizer\nDNN %>% \n  compile(loss = loss_categorical_crossentropy,\n          optimizer = optimizer_adamax(0.01))\n\n# train model\nDNN %>% \n  fit(X, YT, epochs = 50, verbose = 0)\n\nMake predictions (class probabilities):\n\nhead(predict(DNN, X), n = 3)\n\n          [,1]        [,2]         [,3]\n[1,] 0.9943717 0.004811807 0.0008164378\n[2,] 0.9866450 0.011190814 0.0021641329\n[3,] 0.9952909 0.003913700 0.0007954266\n\n\n\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\n# We need to one hot encode our response classes\nYT = keras.utils.to_categorical(Y, num_classes = 3)\n\nDNN = keras.Sequential()\n  # first hidden layer\nDNN.add(Dense(\n  input_shape=[X.shape[1]], \n  units = 10, \n  activation = \"relu\")) \n  # second hidden layer with regularization\nDNN.add(Dense(\n  units = 20, \n  activation = \"relu\",\n  kernel_regularizer = keras.regularizers.l1()))\n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\nDNN.add(Dense(\n  units = 3, \n  activation = \"softmax\"))\n\n# print architecture\nDNN.summary()\n\n# add loss function and optimizer\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_3 (Dense)             (None, 10)                50        \n                                                                 \n dense_4 (Dense)             (None, 20)                220       \n                                                                 \n dense_5 (Dense)             (None, 3)                 63        \n                                                                 \n=================================================================\nTotal params: 333\nTrainable params: 333\nNon-trainable params: 0\n_________________________________________________________________\n\nDNN.compile(loss = keras.losses.categorical_crossentropy,\n            optimizer = keras.optimizers.Adamax(0.01))\n\n# train model\nDNN.fit(X, YT, epochs = 50, verbose = 0)\n\n<keras.callbacks.History object at 0x7fb0896b0850>\n\n\nMake predictions:\n\nDNN.predict(X)[0:10,:]\n\n\n1/5 [=====>........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n5/5 [==============================] - 0s 680us/step\narray([[9.9583131e-01, 4.1106688e-03, 5.8080172e-05],\n       [9.8362643e-01, 1.6319780e-02, 5.3694650e-05],\n       [9.9434531e-01, 5.6184102e-03, 3.6299647e-05],\n       [9.9109250e-01, 8.8622421e-03, 4.5296307e-05],\n       [9.9725479e-01, 2.6933698e-03, 5.1819748e-05],\n       [9.9600965e-01, 3.8014781e-03, 1.8886165e-04],\n       [9.9641293e-01, 3.5384791e-03, 4.8511523e-05],\n       [9.9425328e-01, 5.6840330e-03, 6.2594314e-05],\n       [9.8860162e-01, 1.1364626e-02, 3.3743974e-05],\n       [9.8900944e-01, 1.0945212e-02, 4.5381825e-05]], dtype=float32)\n\n\n\n\n\nimport StatsBase\nusing RDatasets\nusing StatsBase\nusing DataFrames\nimport MLJBase.int\nusing Flux, Statistics\nusing Flux.Data: DataLoader\nusing Flux: onehotbatch, onecold, @epochs\nusing Flux.Losses: logitcrossentropy\n\nData preparation:\n\niris = dataset(\"datasets\", \"iris\");\nX = transpose(Matrix(mapcols(StatsBase.zscore, iris[:, 1:4])));\nY = int(iris[:, 5], type = Int);\nclasses = sort(unique(Y));\nYT = onehotbatch(Y, classes);\ndata_loader = DataLoader((X, YT), batchsize=10, shuffle=true);\n\nCreate model (similar to Keras):\n\nmodel = Chain(\n  Dense(4, 20, relu),\n  Dense(20, 20, relu),\n  Dense(20, 3)\n)\n\nChain(\n  Dense(4 => 20, relu),                 # 100 parameters\n  Dense(20 => 20, relu),                # 420 parameters\n  Dense(20 => 3),                       # 63 parameters\n)                   # Total: 6 arrays, 583 parameters, 2.652 KiB.\n\n\nTrain/optimize Model:\n\nparameters = Flux.params(model);\noptimizer = ADAM(0.01);\n\n# Help functions\nloss(x, y) = logitcrossentropy(model(x), y);\n\nget_loss() = @show sum(logitcrossentropy(model(X), YT));\n\n## Training\nfor epoch in 1:20\n  Flux.train!(loss, parameters, data_loader, optimizer, cb = Flux.throttle(get_loss, 5))\nend\n\nsum(logitcrossentropy(model(X), YT)) = 1.0847621827469989\nsum(logitcrossentropy(model(X), YT)) = 0.41861586893929087\nsum(logitcrossentropy(model(X), YT)) = 0.28409296777747656\nsum(logitcrossentropy(model(X), YT)) = 0.19468215242911596\nsum(logitcrossentropy(model(X), YT)) = 0.13160069597338106\nsum(logitcrossentropy(model(X), YT)) = 0.09292801366692326\nsum(logitcrossentropy(model(X), YT)) = 0.08476726510840268\nsum(logitcrossentropy(model(X), YT)) = 0.07357130215647495\nsum(logitcrossentropy(model(X), YT)) = 0.05955705216439545\nsum(logitcrossentropy(model(X), YT)) = 0.06489229857135734\nsum(logitcrossentropy(model(X), YT)) = 0.04552515538160692\nsum(logitcrossentropy(model(X), YT)) = 0.04941019442329357\nsum(logitcrossentropy(model(X), YT)) = 0.04051517193299854\nsum(logitcrossentropy(model(X), YT)) = 0.05666814742618713\nsum(logitcrossentropy(model(X), YT)) = 0.05791907364710918\nsum(logitcrossentropy(model(X), YT)) = 0.04238850341678316\nsum(logitcrossentropy(model(X), YT)) = 0.08791328847980895\nsum(logitcrossentropy(model(X), YT)) = 0.047533954738316425\nsum(logitcrossentropy(model(X), YT)) = 0.044352655498664274\nsum(logitcrossentropy(model(X), YT)) = 0.0397672009806662\n\n\nPredictions:\n\ntranspose(softmax(model(X)))[1:5,:]\n\n5×3 Matrix{Float64}:\n 0.999997  2.61079e-6  1.15129e-11\n 0.999971  2.85092e-5  2.56738e-10\n 0.999999  1.29144e-6  1.78573e-11\n 0.999998  2.14865e-6  5.09192e-11\n 0.999999  9.7007e-7   3.61251e-12"
  },
  {
    "objectID": "dnn.html#regression",
    "href": "dnn.html#regression",
    "title": "6  Deep neural networks",
    "section": "6.2 Regression",
    "text": "6.2 Regression\n\nRPythonJulia\n\n\n\nlibrary(keras)\nX = scale(as.matrix(iris[,2:4]))\nY = as.matrix(iris[,1,drop=FALSE])\n\nDNN = keras_model_sequential() %>% \n  # first hidden layer\n  layer_dense(input_shape = ncol(X), \n              units = 10, \n              activation = \"relu\") %>% \n  # second hidden layer with regularization\n  layer_dense(units = 20, \n              activation = \"relu\",\n              kernel_regularizer = regularizer_l1()) %>% \n  # output layer, one output neuron for one response\n  # and no activation function\n  layer_dense(units = 1)\n\n# print architecture\nsummary(DNN)\n\nModel: \"sequential_2\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_8 (Dense)                    (None, 10)                      40          \n dense_7 (Dense)                    (None, 20)                      220         \n dense_6 (Dense)                    (None, 1)                       21          \n================================================================================\nTotal params: 281\nTrainable params: 281\nNon-trainable params: 0\n________________________________________________________________________________\n\n# add loss function and optimizer\nDNN %>% \n  compile(loss = loss_mean_squared_error,\n          optimizer = optimizer_adamax(0.01))\n\n# train model\nDNN %>% \n  fit(X, YT, epochs = 50, verbose = 0)\n\nMake predictions:\n\nhead(predict(DNN, X), n = 3)\n\n          [,1]\n[1,] 0.3332271\n[2,] 0.3336180\n[3,] 0.3334625\n\n\n\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\nDNN = keras.Sequential()\n  # first hidden layer\nDNN.add(Dense(\n  input_shape=[X.shape[1]], \n  units = 10, \n  activation = \"relu\")) \n  # second hidden layer with regularization\nDNN.add(Dense(\n  units = 20, \n  activation = \"relu\",\n  kernel_regularizer = keras.regularizers.l1()))\n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\nDNN.add(Dense(\n  units = 1, \n  activation = None))\n\n# print architecture\nDNN.summary()\n\n# add loss function and optimizer\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_9 (Dense)             (None, 10)                40        \n                                                                 \n dense_10 (Dense)            (None, 20)                220       \n                                                                 \n dense_11 (Dense)            (None, 1)                 21        \n                                                                 \n=================================================================\nTotal params: 281\nTrainable params: 281\nNon-trainable params: 0\n_________________________________________________________________\n\nDNN.compile(loss = keras.losses.mean_squared_error,\n            optimizer = keras.optimizers.Adamax(0.01))\n\n# train model\nDNN.fit(X, Y, epochs = 50, verbose = 0)\n\n<keras.callbacks.History object at 0x7faf45b6fac0>\n\n\nMake predictions:\n\nDNN.predict(X)[0:10]\n\n\n1/5 [=====>........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n5/5 [==============================] - 0s 626us/step\narray([[5.153783 ],\n       [4.8413796],\n       [4.947369 ],\n       [4.906414 ],\n       [5.2335167],\n       [5.212913 ],\n       [4.928632 ],\n       [5.086681 ],\n       [4.784372 ],\n       [5.047194 ]], dtype=float32)\n\n\n\n\n\nimport StatsBase\nusing RDatasets\nusing StatsBase\nusing DataFrames\nimport MLJBase.int\nusing Flux, Statistics\nusing Flux.Data: DataLoader\nusing Flux: onehotbatch, onecold, @epochs\nusing Flux.Losses: mse\n\nData preparation:\n\niris = dataset(\"datasets\", \"iris\");\nX = transpose(Matrix(mapcols(StatsBase.zscore, iris[:, 2:4])));\nYT = iris[:, 1];\nYT = reshape(YT, 1, length(YT));\n\ndata_loader = DataLoader((X, YT), batchsize=10, shuffle=true);\n\nCreate model (similar to Keras):\n\nmodel = Chain(\n  Dense(3, 20, relu),\n  Dense(20, 20, relu),\n  Dense(20, 1)\n)\n\nChain(\n  Dense(3 => 20, relu),                 # 80 parameters\n  Dense(20 => 20, relu),                # 420 parameters\n  Dense(20 => 1),                       # 21 parameters\n)                   # Total: 6 arrays, 521 parameters, 2.410 KiB.\n\n\nTrain/optimize Model:\n\nparameters = Flux.params(model);\noptimizer = ADAM(0.01);\n\n# Help functions\nloss(x, y) = mse(model(x), y);\n\nget_loss() = @show sum(mse(model(X), YT));\n\n## Training\nfor epoch in 1:20\n  Flux.train!(loss, parameters, data_loader, optimizer, cb = Flux.throttle(get_loss, 5))\nend\n\nsum(mse(model(X), YT)) = 29.07346144222025\nsum(mse(model(X), YT)) = 2.2416517572050343\nsum(mse(model(X), YT)) = 1.5487028757261814\nsum(mse(model(X), YT)) = 1.0567789347316945\nsum(mse(model(X), YT)) = 0.7146277767677577\nsum(mse(model(X), YT)) = 0.4873170089597854\nsum(mse(model(X), YT)) = 0.2943587558821003\nsum(mse(model(X), YT)) = 0.23717687312953964\nsum(mse(model(X), YT)) = 0.15060292095730848\nsum(mse(model(X), YT)) = 0.12921184500980595\nsum(mse(model(X), YT)) = 0.11438955839064809\nsum(mse(model(X), YT)) = 0.1209811019396246\nsum(mse(model(X), YT)) = 0.1095651084591268\nsum(mse(model(X), YT)) = 0.10894502288937695\nsum(mse(model(X), YT)) = 0.09853287235725164\nsum(mse(model(X), YT)) = 0.1296184768039172\nsum(mse(model(X), YT)) = 0.11522763156480774\nsum(mse(model(X), YT)) = 0.09469441130795656\nsum(mse(model(X), YT)) = 0.09941779923178777\nsum(mse(model(X), YT)) = 0.10689945604907174\n\n\nPredictions:\n\ntranspose(model(X))[1:5]\n\n5-element Vector{Float64}:\n 5.044081758527378\n 4.578648187686671\n 4.7570435189899944\n 4.713898034456124\n 5.1239384911577\n\n\n\n\n\n\n\n\n\nAbadi, Martı́n, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. 2016. “Tensorflow: A System for Large-Scale Machine Learning.” In 12th \\(\\{\\)USENIX\\(\\}\\) Symposium on Operating Systems Design and Implementation (\\(\\{\\)OSDI\\(\\}\\) 16), 265–83.\n\n\nAllaire, JJ, and François Chollet. 2022. Keras: R Interface to ’Keras’. https://CRAN.R-project.org/package=keras.\n\n\nChollet, Francois et al. 2015. “Keras.” GitHub. 2015. https://github.com/fchollet/keras.\n\n\nInnes, Michael, Elliot Saba, Keno Fischer, Dhairya Gandhi, Marco Concetto Rudilosso, Neethu Mariya Joy, Tejan Karmali, Avik Pal, and Viral Shah. 2018. “Fashionable Modelling with Flux.” CoRR abs/1811.01457. https://arxiv.org/abs/1811.01457."
  },
  {
    "objectID": "cnn.html",
    "href": "cnn.html",
    "title": "7  Convolutional neural networks",
    "section": "",
    "text": "In the following, we will use again the ‘keras’ package (Python: ‘keras’ (Chollet et al. (2015)); Julia: ‘Flux’ (Innes et al. (2018))) but we will not differentiate between classification and regression because the only difference would be to change the last layer and the loss function (see section ‘Deep neural networks’).\nWe will demonstrate the application of CNNs with the MNIST dataset which consists of handwritten digits. The objective of the CNNs is to classify the images. The MNIST dataset is one of the most famous benchmark dataset for image-based tasks (LeCun, Cortes, and Burges (2010)).\n\nRPythonJulia\n\n\n\nlibrary(keras)\ndata = keras::dataset_mnist()\n\nLoaded Tensorflow version 2.10.0\n\ntrain = data$train\nX = train$x/255\n# we have to add a dimension that \n# informs the network about the channels\n# of the images\nX = array(X, dim = c(dim(X), 1))\nYT = k_one_hot(train$y, num_classes = 10)\n\n\n\nCNN = \n  keras_model_sequential() %>% \n  # first hidden layer\n  layer_conv_2d(input_shape = list(28, 28, 1), \n                filters = 16,\n                kernel_size = c(2, 2),\n                activation = \"relu\") %>%\n  layer_average_pooling_2d() %>% \n  layer_conv_2d(filters = 8,\n                kernel_size = c(2, 2),\n                activation = \"relu\") %>%\n  # we use a normal DNN on top of the CNN:\n  # the layer flatten will remove the additional \n  # dimensions\n  layer_flatten() %>% \n  layer_dense(units = 20, \n              activation = \"relu\") %>%\n  # 10 output neurons for 10 classes\n  layer_dense(units = 10, \n              activation = \"softmax\")\n\n\n# print architecture\nsummary(CNN)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n conv2d_1 (Conv2D)                  (None, 27, 27, 16)              80          \n average_pooling2d (AveragePooling2  (None, 13, 13, 16)             0           \n D)                                                                             \n conv2d (Conv2D)                    (None, 12, 12, 8)               520         \n flatten (Flatten)                  (None, 1152)                    0           \n dense_1 (Dense)                    (None, 20)                      23060       \n dense (Dense)                      (None, 10)                      210         \n================================================================================\nTotal params: 23,870\nTrainable params: 23,870\nNon-trainable params: 0\n________________________________________________________________________________\n\n# add loss function and optimizer\nCNN %>% \n  compile(loss = loss_categorical_crossentropy,\n          optimizer = optimizer_adamax(0.01))\n\nCNN %>% \n  fit(X, YT, epochs = 3, batch_size = 125, verbose = 0)\n\nMake predictions (class probabilites):\n\nhead(predict(CNN, X[1:100,,,,drop=FALSE]), n = 3)\n\n             [,1]         [,2]         [,3]         [,4]         [,5]\n[1,] 2.425084e-08 2.880918e-07 4.143541e-08 9.217440e-02 5.395139e-10\n[2,] 9.999260e-01 5.911451e-09 5.907062e-05 2.765627e-08 1.134630e-08\n[3,] 6.202117e-08 1.781751e-06 7.806528e-07 6.182400e-06 9.959503e-01\n             [,6]         [,7]         [,8]         [,9]        [,10]\n[1,] 9.077224e-01 5.556984e-08 5.069603e-08 7.870004e-05 2.408227e-05\n[2,] 7.944796e-09 2.350163e-06 1.962552e-07 4.941584e-06 7.313301e-06\n[3,] 1.674008e-07 1.992673e-08 1.871501e-05 1.647718e-03 2.374151e-03\n\n\n\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\ndata = keras.datasets.mnist.load_data()\ntrain = data[0][0]\nlabels = data[0][1]\n\n# We need to one hot encode our response classes\nYT = keras.utils.to_categorical(labels, num_classes = 10)\n\nCNN = keras.Sequential()\n  # first hidden layer\nCNN.add(Conv2D(input_shape = [28, 28, 1], \n                filters = 16,\n                kernel_size = (2, 2),\n                activation = \"relu\"))\nCNN.add(AveragePooling2D())\nCNN.add(Conv2D(filters = 8,\n                kernel_size = (2, 2),\n                activation = \"relu\"))\n  # we use a normal DNN on top of the CNN:\n  # the layer flatten will remove the additional \n  # dimensions\nCNN.add(Flatten())\n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\nCNN.add(Dense(\n  units = 10, \n  activation = \"softmax\"))\n\n# print architecture\nCNN.summary()\n\n# add loss function and optimizer\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_2 (Conv2D)           (None, 27, 27, 16)        80        \n                                                                 \n average_pooling2d_1 (Averag  (None, 13, 13, 16)       0         \n ePooling2D)                                                     \n                                                                 \n conv2d_3 (Conv2D)           (None, 12, 12, 8)         520       \n                                                                 \n flatten_1 (Flatten)         (None, 1152)              0         \n                                                                 \n dense_2 (Dense)             (None, 10)                11530     \n                                                                 \n=================================================================\nTotal params: 12,130\nTrainable params: 12,130\nNon-trainable params: 0\n_________________________________________________________________\n\nCNN.compile(loss = keras.losses.categorical_crossentropy,\n            optimizer = keras.optimizers.Adamax(0.01))\n\n# train model\nCNN.fit(train, YT, epochs = 5, verbose = 0)\n\n<keras.callbacks.History object at 0x7f0b44785c70>\n\n\nMake predictions:\n\nCNN.predict(train[0:10,:,:])\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n1/1 [==============================] - 0s 39ms/step\narray([[1.1181537e-09, 2.2871836e-10, 8.1618259e-09, 1.0941692e-02,\n        1.2186857e-12, 9.8905164e-01, 7.4941531e-10, 3.2881280e-06,\n        2.9509465e-06, 4.6295472e-07],\n       [9.9913085e-01, 5.6235383e-08, 7.3662569e-04, 3.6811846e-07,\n        3.6626862e-08, 6.4597992e-07, 7.8866004e-05, 1.0132234e-10,\n        3.3926879e-06, 4.9185153e-05],\n       [7.7702627e-12, 5.6763956e-06, 4.4048205e-09, 1.5371381e-07,\n        9.9968910e-01, 4.1317495e-08, 5.5084448e-10, 7.7227251e-06,\n        8.8550350e-06, 2.8850837e-04],\n       [6.7505084e-08, 9.9908459e-01, 6.0488185e-04, 3.7413037e-07,\n        5.9135229e-05, 1.0721207e-08, 2.4258597e-08, 1.6805495e-04,\n        8.2748382e-05, 1.2584145e-07],\n       [1.6376575e-10, 1.8257330e-07, 1.0599615e-07, 2.1357984e-04,\n        2.0651119e-04, 4.6220787e-08, 3.0120306e-11, 6.0984075e-05,\n        5.0008623e-08, 9.9951857e-01],\n       [8.1270095e-08, 1.7270331e-08, 9.9999934e-01, 3.6143469e-08,\n        3.9866994e-09, 3.2378559e-14, 8.1235237e-15, 2.5397111e-07,\n        9.8850762e-08, 3.6428407e-08],\n       [8.8377601e-07, 9.9956709e-01, 6.9225271e-06, 1.0397793e-05,\n        4.9456748e-06, 3.5048351e-06, 2.2041748e-05, 1.1251929e-07,\n        3.7966701e-04, 4.5567990e-06],\n       [2.7390620e-12, 6.8532867e-11, 2.4177119e-08, 9.9999791e-01,\n        9.8699826e-17, 8.3391809e-09, 6.0832223e-18, 1.5824456e-10,\n        3.4807883e-08, 1.9520967e-06],\n       [6.6492216e-06, 9.9919182e-01, 2.7197627e-06, 2.3677121e-06,\n        5.3769606e-04, 2.3768218e-05, 2.7554268e-05, 1.1666738e-04,\n        8.4651707e-05, 6.0710104e-06],\n       [1.6910246e-08, 2.0487221e-07, 1.0832826e-07, 1.6800383e-09,\n        9.9999189e-01, 1.1831921e-08, 1.7606248e-09, 7.0846800e-06,\n        7.0496088e-07, 5.9994800e-12]], dtype=float32)\n\n\n\n\n\nimport StatsBase\nusing RDatasets\nusing StatsBase\nusing DataFrames\nimport MLJBase.int\nusing MLDatasets: MNIST\nusing Flux, Statistics\nusing Flux.Data: DataLoader\nusing Flux: onehotbatch, onecold, @epochs\nusing Flux.Losses: logitcrossentropy\n\nData preparation:\n\nENV[\"DATADEPS_ALWAYS_ACCEPT\"] = \"true\"\n\n\"true\"\n\n\nxtrain, ytrain = MNIST(:train)[:];\nxtrain = reshape(xtrain/255., 28, 28, 1, 60000);\nytrain = onehotbatch(ytrain, 0:9);\n\ndata_loader = DataLoader((xtrain, ytrain), batchsize=100, shuffle=true);\n\nCreate model (similar to Keras):\n\nmodel = Chain(\n  Conv((2, 2), 1=>16, pad = (1, 1), relu),\n  MeanPool((2, 2)),\n  Conv((2, 2), 16=>8, pad = (1, 1), relu),\n  MeanPool((2, 2)),\n  Flux.flatten,\n  Dense(392, 20, relu),\n  Dense(20, 10)\n)\n\nChain(\n  Conv((2, 2), 1 => 16, relu, pad=1),   # 80 parameters\n  MeanPool((2, 2)),\n  Conv((2, 2), 16 => 8, relu, pad=1),   # 520 parameters\n  MeanPool((2, 2)),\n  Flux.flatten,\n  Dense(392 => 20, relu),               # 7_860 parameters\n  Dense(20 => 10),                      # 210 parameters\n)                   # Total: 8 arrays, 8_670 parameters, 34.977 KiB.\n\n\nTrain/optimize Model:\n\nparameters = Flux.params(model);\noptimizer = ADAM(0.01);\n\n# Help functions\nloss(x, y) = logitcrossentropy(model(x), y);\n\nget_loss() = @show sum(logitcrossentropy(model(xtrain[:,:,:,1:100]), ytrain[:,1:100]));\n\n## Training\nfor epoch in 1:1\n  Flux.train!(loss, parameters, data_loader, optimizer, cb = Flux.throttle(get_loss, 6000))\nend\n\nsum(logitcrossentropy(model(xtrain[:, :, :, 1:100]), ytrain[:, 1:100])) = 2.301290467968534\n\n\nPredictions:\n\nsoftmax(model(xtrain[:,:,:,1:5]))[:,1]\n\n10-element Vector{Float64}:\n 0.10213041258365503\n 0.11425505318257256\n 0.09493333518971231\n 0.10597267878754779\n 0.09368559004328189\n 0.0916453414163753\n 0.10353605754913195\n 0.09766811239812598\n 0.09657222751232929\n 0.09960119133726789\n\n\n\n\n\n\n\n\n\nChollet, Francois et al. 2015. “Keras.” GitHub. 2015. https://github.com/fchollet/keras.\n\n\nInnes, Michael, Elliot Saba, Keno Fischer, Dhairya Gandhi, Marco Concetto Rudilosso, Neethu Mariya Joy, Tejan Karmali, Avik Pal, and Viral Shah. 2018. “Fashionable Modelling with Flux.” CoRR abs/1811.01457. https://arxiv.org/abs/1811.01457.\n\n\nLeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. “Deep Learning.” Nature 521 (7553): 436–44.\n\n\nLeCun, Yann, Corinna Cortes, and CJ Burges. 2010. “MNIST Handwritten Digit Database.” ATT Labs [Online]. Available: Http://Yann.lecun.com/Exdb/Mnist 2."
  },
  {
    "objectID": "rnn.html",
    "href": "rnn.html",
    "title": "8  Recurrent neural networks",
    "section": "",
    "text": "About the data, we simulated in the following one time series from a simple ARIMA process, using the ‘arima.sim’ function. Our goal is to train a net which is able to predict the next 10 time points based on the previous 10 time points.\n\nRPythonJulia\n\n\n\n## RNNs\nlibrary(keras)\ndata = as.matrix(arima.sim(n = 1000, list(ar = c(0.3, -0.7)) ))\n# We use here a simplified way to create X and Y \n# since the focus is on creating the RNNs\ndata = matrix(data, ncol = 10L, byrow = TRUE)\nX = array(data[seq(1, 100, by = 2), ], dim = c(50, 10, 1))\nY = data[seq(2, 100, by = 2), ]\n\nRNN = \n  keras_model_sequential() %>% \n  # first hidden layer\n  layer_gru(input_shape = list(10L, 1L),\n            units = 50, \n            activation = \"relu\") %>%\n  # we want to predict the next 10 time steps\n  layer_dense(units = 10)\n\nLoaded Tensorflow version 2.10.0\n\n# add loss function and optimizer\nRNN %>% \n  compile(loss = loss_mean_squared_error,\n          optimizer = optimizer_adamax(0.01))\n\nRNN %>% \n  fit(X, Y, epochs = 5, verbose = 0)\n\nMake predictions:\n\nhead(predict(RNN, X), n = 3)\n\n          [,1]       [,2]       [,3]       [,4]       [,5]       [,6]\n[1,] 0.4800752  0.2515826 -0.1871746 -0.5251560 0.02715888  0.2110344\n[2,] 0.2533702 -0.5832934 -0.3672795  0.2838775 0.53607899 -0.2003853\n[3,] 0.4985844  0.2491744 -0.2287293 -0.5281581 0.04576387  0.1937918\n            [,7]       [,8]       [,9]      [,10]\n[1,] -0.03789480 -0.5814861 -0.4529335 0.08699021\n[2,] -0.23894967 -0.1293640  0.1142473 0.01777250\n[3,] -0.05073898 -0.5958483 -0.4544795 0.06178066\n\n\n\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nX = r.X # get data from R\nY = r.Y \n\nRNN = keras.Sequential()\n  # first hidden layer\nRNN.add(GRU(input_shape = [10, 1],units = 50, activation = \"relu\"))\nRNN.add(Dense(units = 10))\n\nRNN.summary()\n\n# add loss function and optimizer\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_1 (GRU)                 (None, 50)                7950      \n                                                                 \n dense_1 (Dense)             (None, 10)                510       \n                                                                 \n=================================================================\nTotal params: 8,460\nTrainable params: 8,460\nNon-trainable params: 0\n_________________________________________________________________\n\nRNN.compile(loss = keras.losses.mean_squared_error,\n            optimizer = keras.optimizers.Adamax(0.01))\n\n# train model\nRNN.fit(X, Y, epochs = 5, verbose = 0)\n\n<keras.callbacks.History object at 0x7f91380e4670>\n\n\nMake predictions:\n\nRNN.predict(X)[0:10,:]\n\n\n1/2 [==============>...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n2/2 [==============================] - 0s 2ms/step\narray([[ 0.42014763, -0.18803373, -0.49838978, -0.23084594,  0.16043243,\n         0.09836911,  0.05774919, -0.6397091 , -0.17642473,  0.23359524],\n       [ 0.24713348, -0.5150123 , -0.41040528,  0.35002753,  0.5223741 ,\n        -0.25475818, -0.2640479 , -0.3793788 ,  0.28173554,  0.06628139],\n       [ 0.42807055, -0.23126799, -0.5387223 , -0.21818314,  0.17583738,\n         0.09494507,  0.04807471, -0.66506076, -0.17101239,  0.22419289],\n       [ 0.19364469, -0.16822672, -0.35907426, -0.05538802,  0.09245294,\n         0.09220832,  0.03405145, -0.39754653, -0.08505453,  0.07975613],\n       [ 0.5740196 , -0.04543666, -0.7570797 , -0.8942825 , -0.19053774,\n         0.56847376,  0.420092  , -0.9900241 , -0.62275285,  0.3780856 ],\n       [ 0.2136533 , -0.3909836 , -0.39041203,  0.27481425,  0.34575748,\n        -0.11220238, -0.15177974, -0.36721054,  0.15575731,  0.03337399],\n       [ 0.30640042, -0.04427434, -0.4467045 , -0.35476726, -0.06164659,\n         0.24404266,  0.19626519, -0.5636717 , -0.29490072,  0.18287963],\n       [ 0.26979104, -0.5362462 , -0.39888373,  0.36888862,  0.58633494,\n        -0.32184407, -0.3478408 , -0.33489403,  0.36450946,  0.10723882],\n       [ 0.41119274, -0.26509812, -0.5204819 , -0.14221832,  0.2413957 ,\n         0.00485324, -0.00819756, -0.6338831 , -0.0883438 ,  0.22701262],\n       [ 0.2998161 ,  0.01500335, -0.41577554, -0.41534966, -0.12229332,\n         0.30252767,  0.2377799 , -0.54849315, -0.35903308,  0.1939047 ]],\n      dtype=float32)\n\n\n\n\n\nimport StatsBase;\nusing RDatasets;\nusing StatsBase;\nusing DataFrames;\nimport MLJBase.int;\nusing Flux, Statistics;\nusing Flux.Losses: mse;\nusing ARFIMA;\n\nData preparation:\n\nX = transpose(reshape(convert(Vector{Float32}, arfima(1000,0.5, 0.3, SVector(-0.7))), 100, 10));\nxtrain = X[:, collect(1:2:100)];\nytrain = X[:, collect(2:2:100)];\n\nCreate model (similar to Keras):\n\nmodel = Chain(\n  GRU(10=>50),\n  Dense(50, 10)\n)\n\nChain(\n  Recur(\n    GRUCell(10 => 50),                  # 9_200 parameters\n  ),\n  Dense(50 => 10),                      # 510 parameters\n)         # Total: 6 trainable arrays, 9_710 parameters,\n          # plus 1 non-trainable, 50 parameters, summarysize 38.297 KiB.\n\n\nTrain/optimize Model:\n\nparameters = Flux.params(model);\noptimizer = ADAM(0.01);\n\nfor epoch in 1:10\n    Flux.reset!(model); \n    grads = gradient(parameters) do \n        Pred = [model( xtrain[:,i]) for i in 1:50];\n        loss = mean([mse(Pred[i], ytrain[:,i]) for i in 1:50]);\n        println(loss);\n        loss\n    end\n    Flux.update!(optimizer, parameters, grads);\nend\n\n0.5030231\n0.40875202\n0.34945393\n0.31208286\n0.28855073\n0.27399114\n0.2656516\n0.2610054\n0.25678155\n0.25014922\n\n\nPredictions:\n\nPred = [model( xtrain[:,i]) for i in 1:50];\nPred[1]\n\n10-element Vector{Float32}:\n -0.15317366\n  0.6826891\n  0.10764982\n  1.342341\n  0.4174205\n -0.35652244\n -0.5429363\n -0.72320706\n  0.1646748\n  0.08904388\n\n\n\n\n\n\n\n\n\nChollet, Francois et al. 2015. “Keras.” GitHub. 2015. https://github.com/fchollet/keras.\n\n\nInnes, Michael, Elliot Saba, Keno Fischer, Dhairya Gandhi, Marco Concetto Rudilosso, Neethu Mariya Joy, Tejan Karmali, Avik Pal, and Viral Shah. 2018. “Fashionable Modelling with Flux.” CoRR abs/1811.01457. https://arxiv.org/abs/1811.01457."
  },
  {
    "objectID": "gnn.html",
    "href": "gnn.html",
    "title": "9  Graph (convolutional) neural networks",
    "section": "",
    "text": "Currently there is no R package for GNNs available. However, we can use the ‘reticulate’ package (Ushey, Allaire, and Tang (2022)) to use the python packages ‘torch’ and ‘torch_geometric’ (Paszke et al. (2019), 2019; Fey and Lenssen (2019)).\nThe following example was mostly adapted from the ‘Node Classification with Graph Neural Networks’ example from the torch_geometric documentation (https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html).\nThe dataset is also provided by the ‘torch_geometric’ package and consists of molecules presented as graphs and the task is to predict whether HIV virus replication is inhibited by the molecule or not (classification, binary classification).\nWe have not implemented this example in Julia because there is not yet a well-established library for GNNs.\n\nRPython\n\n\n\nlibrary(reticulate)\n# Load python packages torch and torch_geometric via the reticulate R package\ntorch = import(\"torch\") \ntorch_geometric = import(\"torch_geometric\")\n\n# helper functions from the torch_geometric modules\nGCNConv = torch_geometric$nn$GCNConv\nglobal_mean_pool = torch_geometric$nn$global_mean_pool\n\n\n# Download the MUTAG TUDataset\ndataset = torch_geometric$datasets$TUDataset(root='data/TUDataset', \n                                             name='MUTAG')\ndataloader = torch_geometric$loader$DataLoader(dataset, \n                                               batch_size=64L,\n                                               shuffle=TRUE)\n\n# Create the model with a python class\n# There are two classes in the response variable\nGCN = PyClass(\n  \"GCN\", \n   inherit = torch$nn$Module, \n   defs = list(\n       `__init__` = function(self, hidden_channels) {\n         super()$`__init__`()\n         torch$manual_seed(42L)\n         self$conv = GCNConv(dataset$num_node_features, hidden_channels)\n         self$linear = torch$nn$Linear(hidden_channels, dataset$num_classes)\n         NULL\n       },\n       forward = function(self, x, edge_index, batch) {\n         x = self$conv(x, edge_index)\n         x = x$relu()\n         x = global_mean_pool(x, batch)\n         \n         x = torch$nn$functional$dropout(x, p = 0.5, training=self$training)\n         x = self$linear(x)\n         return(x)\n       }\n   ))\n\nTraining loop:\n\n# create model object\nmodel = GCN(hidden_channels = 64L)\n\n# get optimizer and loss function\noptimizer = torch$optim$Adamax(model$parameters(), lr = 0.01)\nloss_func = torch$nn$CrossEntropyLoss()\n\n# set model into training mode (because of the dropout layer)\nmodel$train()\n\nGCN(\n  (conv): GCNConv(7, 64)\n  (linear): Linear(in_features=64, out_features=2, bias=True)\n)\n\n# train model\nfor(e in 1:50) {\n  iterator = reticulate::as_iterator(dataloader)\n  coro::loop(for (b in iterator) { \n     pred = model(b$x, b$edge_index, b$batch)\n     loss = loss_func(pred, b$y)\n     loss$backward()\n     optimizer$step()\n     optimizer$zero_grad()\n  })\n  if(e %% 10 ==0) cat(paste0(\"Epoch: \",e,\" Loss: \", round(loss$item()[1], 4), \"\\n\"))\n}\n\nEpoch: 10 Loss: 0.6151\nEpoch: 20 Loss: 0.6163\nEpoch: 30 Loss: 0.5745\nEpoch: 40 Loss: 0.5362\nEpoch: 50 Loss: 0.5829\n\n\nMake predictions:\n\npreds = list()\ntest = torch_geometric$loader$DataLoader(dataset, batch_size=64L,shuffle=FALSE)\niterator = reticulate::as_iterator(test)\nmodel$eval()\n\nGCN(\n  (conv): GCNConv(7, 64)\n  (linear): Linear(in_features=64, out_features=2, bias=True)\n)\n\ncounter = 1\ncoro::loop(for (b in iterator) {\n  preds[[counter]] = model(b$x, b$edge_index, b$batch)\n  counter <<- counter + 1\n  })\nhead(torch$concat(preds)$sigmoid()$data$cpu()$numpy(), n = 3)\n\n          [,1]      [,2]\n[1,] 0.3076028 0.6427078\n[2,] 0.4121239 0.5515330\n[3,] 0.4119514 0.5516798\n\n\n\n\n\n# Load python packages torch and torch_geometric via the reticulate R package\nimport torch\nimport torch_geometric\n\n# helper functions from the torch_geometric modules\nGCNConv = torch_geometric.nn.GCNConv\nglobal_mean_pool = torch_geometric.nn.global_mean_pool\n\n\n# Download the MUTAG TUDataset\ndataset = torch_geometric.datasets.TUDataset(root='data/TUDataset', \n                                             name='MUTAG')\ndataloader = torch_geometric.loader.DataLoader(dataset, \n                                               batch_size=64,\n                                               shuffle=True)\n\n# Create the model with a python class\n# There are two classes in the response variable\nclass GCN(torch.nn.Module):\n    def __init__(self, hidden_channels):\n         super().__init__()\n         torch.manual_seed(42)\n         self.conv = GCNConv(dataset.num_node_features, hidden_channels)\n         self.linear = torch.nn.Linear(hidden_channels, dataset.num_classes)\n         \n    def forward(self, x, edge_index, batch):\n        x = self.conv(x, edge_index)\n        x = x.relu()\n        x = global_mean_pool(x, batch)\n        x = torch.nn.functional.dropout(x, p = 0.5, training=self.training)\n        x = self.linear(x)\n        return x\n\nTraining loop:\n\n# create model object\nmodel = GCN(hidden_channels = 64)\n\n# get optimizer and loss function\noptimizer = torch.optim.Adamax(model.parameters(), lr = 0.01)\nloss_func = torch.nn.CrossEntropyLoss()\n\n# set model into training mode (because of the dropout layer)\nmodel.train()\n\n# train model\n\nGCN(\n  (conv): GCNConv(7, 64)\n  (linear): Linear(in_features=64, out_features=2, bias=True)\n)\n\nfor e in range(50):\n  for b in dataloader:\n  \n    pred = model(b.x, b.edge_index, b.batch)\n    loss = loss_func(pred, b.y)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n     \n  if e % 10 ==0:\n    print(\"Epoch: \", e ,\" Loss: \", loss.item(), \"\\n\")\n\nEpoch:  0  Loss:  0.6617004871368408 \n\nEpoch:  10  Loss:  0.614981472492218 \n\nEpoch:  20  Loss:  0.6161867380142212 \n\nEpoch:  30  Loss:  0.5802667737007141 \n\nEpoch:  40  Loss:  0.5124867558479309 \n\n\nMake predictions:\n\npreds = []\ntest = torch_geometric.loader.DataLoader(dataset, batch_size=64,shuffle=False)\nmodel.eval()\n\nGCN(\n  (conv): GCNConv(7, 64)\n  (linear): Linear(in_features=64, out_features=2, bias=True)\n)\n\ncounter = 1\nfor b in test:\n  preds.append( model(b.x, b.edge_index, b.batch) )\n  \n  \ntorch.concat(preds).sigmoid().data.cpu().numpy()[0:10]\n\narray([[0.30760282, 0.64270777],\n       [0.41212386, 0.551533  ],\n       [0.4119514 , 0.5516798 ],\n       [0.29887193, 0.650517  ],\n       [0.48894534, 0.48584774],\n       [0.4310807 , 0.5360305 ],\n       [0.31375578, 0.63721913],\n       [0.34597102, 0.6093393 ],\n       [0.50279325, 0.4740774 ],\n       [0.30924183, 0.6412629 ]], dtype=float32)\n\n\n\n\n\n\n\n\n\nFey, Matthias, and Jan E. Lenssen. 2019. “Fast Graph Representation Learning with PyTorch Geometric.” In ICLR Workshop on Representation Learning on Graphs and Manifolds.\n\n\nPaszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, et al. 2019. “PyTorch: An Imperative Style, High-Performance Deep Learning Library.” In Advances in Neural Information Processing Systems 32, 8024–35. Curran Associates, Inc. http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.\n\n\nUshey, Kevin, JJ Allaire, and Yuan Tang. 2022. Reticulate: Interface to ’Python’. https://CRAN.R-project.org/package=reticulate."
  },
  {
    "objectID": "refs.html",
    "href": "refs.html",
    "title": "References",
    "section": "",
    "text": "Abadi, Martı́n, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis,\nJeffrey Dean, Matthieu Devin, et al. 2016. “Tensorflow: A System\nfor Large-Scale Machine Learning.” In 12th {USENIX}\nSymposium on Operating Systems Design and Implementation ({OSDI}\n16), 265–83.\n\n\nAllaire, JJ, and François Chollet. 2022. Keras: R Interface to\n’Keras’. https://CRAN.R-project.org/package=keras.\n\n\nBlaom, Anthony, Franz Kiraly, Thibaut Lienart, and Sebastian Vollmer.\n2019. Alan-Turing-Institute/MLJ.jl: V0.5.3 (version v0.5.3).\nZenodo. https://doi.org/10.5281/zenodo.3541506.\n\n\nChen, Tianqi, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang,\nHyunsu Cho, Kailong Chen, et al. 2022. Xgboost: Extreme Gradient\nBoosting. https://CRAN.R-project.org/package=xgboost.\n\n\nChollet, Francois et al. 2015. “Keras.” GitHub. 2015. https://github.com/fchollet/keras.\n\n\nFey, Matthias, and Jan E. Lenssen. 2019. “Fast Graph\nRepresentation Learning with PyTorch Geometric.” In\nICLR Workshop on Representation Learning on Graphs and\nManifolds.\n\n\nInnes, Michael, Elliot Saba, Keno Fischer, Dhairya Gandhi, Marco\nConcetto Rudilosso, Neethu Mariya Joy, Tejan Karmali, Avik Pal, and\nViral Shah. 2018. “Fashionable Modelling with Flux.”\nCoRR abs/1811.01457. https://arxiv.org/abs/1811.01457.\n\n\nLeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. “Deep\nLearning.” Nature 521 (7553): 436–44.\n\n\nLeCun, Yann, Corinna Cortes, and CJ Burges. 2010.\n“MNIST Handwritten Digit Database.” ATT\nLabs [Online]. Available: Http://Yann.lecun.com/Exdb/Mnist 2.\n\n\nMeyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and\nFriedrich Leisch. 2022. E1071: Misc Functions of the Department of\nStatistics, Probability Theory Group (Formerly: E1071), TU Wien. https://CRAN.R-project.org/package=e1071.\n\n\nPaszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,\nGregory Chanan, Trevor Killeen, et al. 2019. “PyTorch: An\nImperative Style, High-Performance Deep Learning Library.” In\nAdvances in Neural Information Processing Systems 32, 8024–35.\nCurran Associates, Inc. http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O.\nGrisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning\nin Python.” Journal of Machine Learning\nResearch 12: 2825–30.\n\n\nSchliep, Klaus, and Klaus Hechenbichler. 2016. Kknn: Weighted\nk-Nearest Neighbors. https://CRAN.R-project.org/package=kknn.\n\n\nSimon, Noah, Jerome Friedman, Trevor Hastie, and Rob Tibshirani. 2011.\n“Regularization Paths for Cox’s Proportional Hazards Model via\nCoordinate Descent.” Journal of Statistical Software 39\n(5): 1–13. https://doi.org/10.18637/jss.v039.i05.\n\n\nUshey, Kevin, JJ Allaire, and Yuan Tang. 2022. Reticulate: Interface\nto ’Python’. https://CRAN.R-project.org/package=reticulate.\n\n\nWright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests\nfor High Dimensional Data in C++ and\nR.” Journal of Statistical Software 77 (1):\n1–17. https://doi.org/10.18637/jss.v077.i01."
  }
]