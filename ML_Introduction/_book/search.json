[
  {
    "objectID": "elastic_net.html",
    "href": "elastic_net.html",
    "title": "1  Ridge, LASSO, and elastic-net regression",
    "section": "",
    "text": "We can use the ‘glmnet’ R package (Simon et al. (2011)) for Ridge, LASSO, or elastic-net regularization. The ‘glmnet’ package supports different response families including ‘gaussian’, ‘binomial’ and ‘Poisson’. The strength of the regularization is set by the ‘lambda’ argument (\\(\\lambda\\)) and the weighting between Ridge and LASSO regularization by the ‘alpha’ parameter (\\(\\alpha\\)):\n\\[\n\\lambda*[(1 - \\alpha)\\|\\beta\\|_1 + \\alpha\\|\\beta||^2]\n\\] Setting alpha = 0 turns off the LASSO and alpha = 1 the Ridge. Alphas between (0,1) will use both regularization types, turning the model into an elastic-net regularization.\nWhen using regularization, it is important to scale all features otherwise effects for features that are on a larger scale are stronger regularized."
  },
  {
    "objectID": "elastic_net.html#classification",
    "href": "elastic_net.html#classification",
    "title": "1  Ridge, LASSO, and elastic-net regression",
    "section": "1.1 Classification",
    "text": "1.1 Classification\nBuild models (for regularization it is important to scale the features):\n\nRPython\n\n\n\nlibrary(glmnet)\nX = scale(iris[,1:4])\nY = iris$Species\n\n# Ridge:\nridge = glmnet(X, Y, family = \"multinomial\", alpha = 0, lambda = 0.01)\n\n# LASSO:\nlasso = glmnet(X, Y, family = \"multinomial\", alpha = 1, lambda = 0.01)\n\n# Elastic-net:\nelastic = glmnet(X, Y, family = \"multinomial\", alpha = 0.5, lambda = 0.01)\n\nMake predictions (class probabilities):\n\nhead(predict(lasso, newx = X, type = \"response\")[,,1], n = 3)\n\n        setosa versicolor    virginica\n[1,] 0.9858987 0.01410131 3.438452e-09\n[2,] 0.9668897 0.03311031 1.397684e-08\n[3,] 0.9815369 0.01846312 5.279315e-09\n\n\n\n\nIn the sklearn implementation the regularization strength parameter ‘C’ corresponds to the lambda parameter from glmnet:\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\n\n# Ridge:\nridge = LogisticRegression(multi_class='multinomial', \n                           penalty = \"l2\", \n                           C = 0.01, \n                           solver=\"saga\")\nridge.fit(X, Y)\n\n# LASSO:\n\nLogisticRegression(C=0.01, multi_class='multinomial', solver='saga')\n\nlasso = LogisticRegression(multi_class='multinomial', \n                           penalty = \"l1\", \n                           C = 0.01, \n                           solver=\"saga\")\nlasso.fit(X, Y)\n\n# Elastic-net:\n\nLogisticRegression(C=0.01, multi_class='multinomial', penalty='l1',\n                   solver='saga')\n\nelastic = LogisticRegression(multi_class='multinomial', \n                             penalty = \"elasticnet\", \n                             C = 0.01, \n                             l1_ratio=0.5, \n                             solver=\"saga\")\nelastic.fit(X, Y)\n\n# Make predictions (class probabilities):\n\nLogisticRegression(C=0.01, l1_ratio=0.5, multi_class='multinomial',\n                   penalty='elasticnet', solver='saga')\n\nlasso.predict_proba(X)[0:10,:]\n\narray([[0.32460693, 0.34261077, 0.3327823 ],\n       [0.32460693, 0.34261077, 0.3327823 ],\n       [0.32460693, 0.34261077, 0.3327823 ],\n       [0.32460693, 0.34261077, 0.3327823 ],\n       [0.32460693, 0.34261077, 0.3327823 ],\n       [0.32460693, 0.34261077, 0.3327823 ],\n       [0.32460693, 0.34261077, 0.3327823 ],\n       [0.32460693, 0.34261077, 0.3327823 ],\n       [0.32460693, 0.34261077, 0.3327823 ],\n       [0.32460693, 0.34261077, 0.3327823 ]])"
  },
  {
    "objectID": "elastic_net.html#regression",
    "href": "elastic_net.html#regression",
    "title": "1  Ridge, LASSO, and elastic-net regression",
    "section": "1.2 Regression",
    "text": "1.2 Regression\n\nRPython\n\n\n\nX = scale(iris[,2:4])\nY = iris[,1]\n\n# Ridge:\nridge = glmnet(X, Y, family = gaussian(), alpha = 0, lambda = 0.01)\n\n# LASSO:\nlasso = glmnet(X, Y, family = gaussian(), alpha = 1, lambda = 0.01)\n\n# Elastic-net:\nelastic = glmnet(X, Y, family = gaussian(), alpha = 0.5, lambda = 0.01)\n\nMake predictions (class probabilities):\n\nhead(predict(lasso, newx = X), n = 3)\n\n           s0\n[1,] 5.006484\n[2,] 4.720600\n[3,] 4.781548\n\n\n\n\nFor regressions we can use the ElasticNet model class, here, however, lambda corresponds to alpha and l1_ratio to the alpha parameter.\n\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\n\n# Ridge:\nridge = ElasticNet(alpha = 0.01,\n                   l1_ratio = 0.011)\nridge.fit(X, Y)\n\n# LASSO:\n\nElasticNet(alpha=0.01, l1_ratio=0.011)\n\nlasso = ElasticNet(alpha = 0.01,\n                   l1_ratio = 1.0)\nlasso.fit(X, Y)\n\n# Elastic-net:\n\nElasticNet(alpha=0.01, l1_ratio=1.0)\n\nelastic = ElasticNet(alpha = 0.01,\n                     l1_ratio = 0.5)\nelastic.fit(X, Y)\n\n# Make predictions:\n\nElasticNet(alpha=0.01)\n\nlasso.predict(X)[0:10]\n\narray([5.0064384 , 4.72032938, 4.78125162, 4.83107256, 5.06366021,\n       5.36149937, 4.93202142, 5.00273797, 4.66310758, 4.84826774])\n\n\n\n\n\n\n\n\n\nSimon, Noah, Jerome Friedman, Trevor Hastie, and Rob Tibshirani. 2011. “Regularization Paths for Cox’s Proportional Hazards Model via Coordinate Descent.” Journal of Statistical Software 39 (5): 1–13. https://doi.org/10.18637/jss.v039.i05."
  },
  {
    "objectID": "svm.html",
    "href": "svm.html",
    "title": "2  Support Vector Machines",
    "section": "",
    "text": "The support vector machine (SVM) algorithm estimates hyper-planes to separate our response species. In the following we use the ‘e1071’ package which supports a variety of different SVM algorithms (Meyer et al. (2022))."
  },
  {
    "objectID": "svm.html#classification",
    "href": "svm.html#classification",
    "title": "2  Support Vector Machines",
    "section": "2.1 Classification",
    "text": "2.1 Classification\n\nRPython\n\n\n\nlibrary(e1071)\nX = scale(iris[,1:4])\nY = iris$Species\n\nsv = svm(X, Y, probability = TRUE) \nsummary(sv)\n\n\nCall:\nsvm.default(x = X, y = Y, probability = TRUE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  51\n\n ( 8 22 21 )\n\n\nNumber of Classes:  3 \n\nLevels: \n setosa versicolor virginica\n\n\nMake predictions (class probabilities):\n\nhead(attr(predict(sv, newdata = X, probability = TRUE), \"probabilities\"), n = 3)\n\n     setosa versicolor   virginica\n1 0.9805055 0.01119040 0.008304068\n2 0.9731565 0.01793966 0.008903881\n3 0.9792215 0.01181014 0.008968335\n\n\n\n\n\nfrom sklearn import svm\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\nmodel = svm.SVC(probability=True).fit(X, Y)\n\n# Make predictions (class probabilities):\n\nmodel.predict_proba(X)[0:10,:]\n\narray([[0.9811499 , 0.01051815, 0.00833194],\n       [0.97431749, 0.01672267, 0.00895984],\n       [0.97995873, 0.01104259, 0.00899868],\n       [0.97616243, 0.01416908, 0.00966849],\n       [0.98032481, 0.01085089, 0.0088243 ],\n       [0.97519955, 0.01563513, 0.00916532],\n       [0.97668604, 0.0125787 , 0.01073526],\n       [0.98105781, 0.01061998, 0.00832221],\n       [0.96786931, 0.02083993, 0.01129076],\n       [0.97787346, 0.01303339, 0.00909315]])"
  },
  {
    "objectID": "svm.html#regression",
    "href": "svm.html#regression",
    "title": "2  Support Vector Machines",
    "section": "2.2 Regression",
    "text": "2.2 Regression\n\nRPython\n\n\n\nlibrary(e1071)\nX = scale(iris[,2:4])\nY = iris[,1]\n\nsv = svm(X, Y) \nsummary(sv)\n\n\nCall:\nsvm.default(x = X, y = Y)\n\n\nParameters:\n   SVM-Type:  eps-regression \n SVM-Kernel:  radial \n       cost:  1 \n      gamma:  0.3333333 \n    epsilon:  0.1 \n\n\nNumber of Support Vectors:  124\n\n\nMake predictions (class probabilities):\n\nhead(predict(sv, newdata = X), n = 3)\n\n       1        2        3 \n5.042085 4.711768 4.836291 \n\n\n\n\n\nfrom sklearn import svm\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\nmodel = svm.SVR().fit(X, Y)\n\n# Make predictions:\n\nmodel.predict(X)[0:10]\n\narray([5.03583855, 4.69496586, 4.81438855, 4.77951854, 5.10018373,\n       5.29981857, 4.97308737, 4.98199033, 4.63701656, 4.78431078])\n\n\n\n\n\n\n\n\n\nMeyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and Friedrich Leisch. 2022. E1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien. https://CRAN.R-project.org/package=e1071."
  },
  {
    "objectID": "knn.html",
    "href": "knn.html",
    "title": "3  k-nearest-neighbor",
    "section": "",
    "text": "The k-nearest-neighbor algorithm doesn’t really learn from the data, predictions for new observations are made based on the class affiliation (or response value) of the nearest neighbors, e.g. by majority voting or averaging. The nearest neighbors are found by calculating the distance of the new observation to all observations in the train dataset.\nIn the following we use the ‘kknn’ package (Schliep and Hechenbichler (2016)). Different to other ML packages we can provide here already the test dataset in the fit function."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Machine Learning and Deep Learning",
    "section": "",
    "text": "The goal of this book is to provide short code examples for all common supervised ML algorithms. All examples are shown in the R programming language and are demonstrated at the example of the iris dataset (4 continuous and 1 nominal variables). We demonstrate a) how to apply the algorithms on classification tasks (response = nominal, three species) and how to generate class specific (quasi) probability predictions and b) how to apply the algorithms to regression tasks (response = continuous) and make continuous predictions."
  },
  {
    "objectID": "knn.html#classification",
    "href": "knn.html#classification",
    "title": "3  k-nearest-neighbor",
    "section": "3.1 Classification",
    "text": "3.1 Classification\n\nRPython\n\n\n\nlibrary(kknn)\nX = scale(iris[,1:4])\nY = iris[,5,drop=FALSE]\ndata = cbind(Y, X)\n\nknn = kknn(Species~., train = data, test = data) \n\nMake predictions (class probabilities):\n\nhead(knn$prob, n = 3)\n\n     setosa versicolor virginica\n[1,]      1          0         0\n[2,]      1          0         0\n[3,]      1          0         0\n\n\n\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\nmodel = KNeighborsClassifier().fit(X, Y)\n\n# Make predictions:\n\nmodel.predict_proba(X)[0:10,:]\n\narray([[1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.]])"
  },
  {
    "objectID": "knn.html#regression",
    "href": "knn.html#regression",
    "title": "3  k-nearest-neighbor",
    "section": "3.2 Regression",
    "text": "3.2 Regression\n\nRPython\n\n\n\nlibrary(e1071)\nX = scale(iris[,2:4])\ndata = cbind(iris[,1,drop=FALSE], X)\n\nknn = kknn(Sepal.Length~., train = data, test = data) \n\nMake predictions (class probabilities):\n\nhead(predict(knn), n = 3)\n\n[1] 5.188492 4.739986 4.685332\n\n\n\n\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\nmodel = KNeighborsRegressor().fit(X, Y)\n\n# Make predictions:\n\nmodel.predict(X)[0:10]\n\narray([5.18, 4.78, 4.68, 4.76, 4.98, 5.34, 5.06, 5.1 , 4.7 , 4.8 ])\n\n\n\n\n\n\n\n\n\nSchliep, Klaus, and Klaus Hechenbichler. 2016. Kknn: Weighted k-Nearest Neighbors. https://CRAN.R-project.org/package=kknn."
  },
  {
    "objectID": "rf.html",
    "href": "rf.html",
    "title": "4  Random forest",
    "section": "",
    "text": "The random forest (RF) algorithm is probably one of the most famous ML algorithms, and not without reason. Compared to other well performing algorithms, the RF algorithm has only a few hyper-parameters and because of the bagging and the random sampling of available variables in for the node splits, it has a well working internal complexity adaption.\nIn the following, we use the ‘ranger’ package (Wright and Ziegler (2017))."
  },
  {
    "objectID": "rf.html#classification",
    "href": "rf.html#classification",
    "title": "4  Random forest",
    "section": "4.1 Classification",
    "text": "4.1 Classification\n\nRPython\n\n\n\nlibrary(ranger)\nX = iris[,1:4]\nY = iris[,5,drop=FALSE]\ndata = cbind(Y, X)\n\nrf = ranger(Species~., data = data, probability = TRUE, importance = \"impurity\")\n\nShow feature importances:\n\nimportance(rf)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    9.249145     1.219569    43.129082    42.020543 \n\n\nMake predictions (class probabilities):\n\nhead(predict(rf, data = data)$predictions, n = 3)\n\n     setosa versicolor virginica\n[1,]  1.000      0.000         0\n[2,]  0.998      0.002         0\n[3,]  1.000      0.000         0\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\nmodel = RandomForestClassifier().fit(X, Y)\n\nFeature importance\n\nprint(model.feature_importances_)\n\n[0.10941572 0.0180583  0.42478094 0.44774504]\n\n\nMake predictions:\n\nmodel.predict_proba(X)[0:10,:]\n\narray([[1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.]])"
  },
  {
    "objectID": "rf.html#regression",
    "href": "rf.html#regression",
    "title": "4  Random forest",
    "section": "4.2 Regression",
    "text": "4.2 Regression\n\nRPython\n\n\n\nlibrary(ranger)\nX = iris[,2:4]\ndata = cbind(iris[,1,drop=FALSE], X)\n\nrf = ranger(Sepal.Length~., data = data, importance = \"impurity\")\n\nShow feature importances:\n\nimportance(rf)\n\n Sepal.Width Petal.Length  Petal.Width \n    12.23700     45.33879     37.60782 \n\n\nMake predictions (class probabilities):\n\nhead(predict(rf, data = data)$predictions, n = 3)\n\n[1] 5.107437 4.769263 4.659477\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\nmodel = RandomForestRegressor().fit(X, Y)\n\nFeature importance:\n\nprint(model.feature_importances_)\n\n[0.08116014 0.85334203 0.06549783]\n\n\nMake predictions:\n\nmodel.predict(X)[0:10]\n\narray([5.105     , 4.825     , 4.61075   , 4.75075   , 5.014     ,\n       5.438     , 4.81933333, 5.05696667, 4.52      , 4.8435    ])\n\n\n\n\n\n\n\n\n\nWright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” Journal of Statistical Software 77 (1): 1–17. https://doi.org/10.18637/jss.v077.i01."
  },
  {
    "objectID": "gnn.html",
    "href": "gnn.html",
    "title": "9  Graph (convolutional) neural networks",
    "section": "",
    "text": "Currently there is no R package for GNNs available. However, we can use the ‘reticulate’ package (Ushey, Allaire, and Tang (2022)) to use the python packages ‘torch’ and ‘torch_geometric’ (Paszke et al. (2019), 2019; Fey and Lenssen (2019)).\nThe following example was mostly adapted from the ‘Node Classification with Graph Neural Networks’ example from the torch_geometric documentation (https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html).\nThe dataset is also provided by the ‘torch_geometric’ package and consists of molecules presented as graphs and the task is to predict whether HIV virus replication is inhibited by the molecule or not (classification, binary classification).\n\nRPython\n\n\n\nlibrary(reticulate)\n# Load python packages torch and torch_geometric via the reticulate R package\ntorch = import(\"torch\") \ntorch_geometric = import(\"torch_geometric\")\n\n# helper functions from the torch_geometric modules\nGCNConv = torch_geometric$nn$GCNConv\nglobal_mean_pool = torch_geometric$nn$global_mean_pool\n\n\n# Download the MUTAG TUDataset\ndataset = torch_geometric$datasets$TUDataset(root='data/TUDataset', \n                                             name='MUTAG')\ndataloader = torch_geometric$loader$DataLoader(dataset, \n                                               batch_size=64L,\n                                               shuffle=TRUE)\n\n# Create the model with a python class\n# There are two classes in the response variable\nGCN = PyClass(\n  \"GCN\", \n   inherit = torch$nn$Module, \n   defs = list(\n       `__init__` = function(self, hidden_channels) {\n         super()$`__init__`()\n         torch$manual_seed(42L)\n         self$conv = GCNConv(dataset$num_node_features, hidden_channels)\n         self$linear = torch$nn$Linear(hidden_channels, dataset$num_classes)\n         NULL\n       },\n       forward = function(self, x, edge_index, batch) {\n         x = self$conv(x, edge_index)\n         x = x$relu()\n         x = global_mean_pool(x, batch)\n         \n         x = torch$nn$functional$dropout(x, p = 0.5, training=self$training)\n         x = self$linear(x)\n         return(x)\n       }\n   ))\n\nTraining loop:\n\n# create model object\nmodel = GCN(hidden_channels = 64L)\n\n# get optimizer and loss function\noptimizer = torch$optim$Adamax(model$parameters(), lr = 0.01)\nloss_func = torch$nn$CrossEntropyLoss()\n\n# set model into training mode (because of the dropout layer)\nmodel$train()\n\nGCN(\n  (conv): GCNConv(7, 64)\n  (linear): Linear(in_features=64, out_features=2, bias=True)\n)\n\n# train model\nfor(e in 1:50) {\n  iterator = reticulate::as_iterator(dataloader)\n  coro::loop(for (b in iterator) { \n     pred = model(b$x, b$edge_index, b$batch)\n     loss = loss_func(pred, b$y)\n     loss$backward()\n     optimizer$step()\n     optimizer$zero_grad()\n  })\n  if(e %% 10 ==0) cat(paste0(\"Epoch: \",e,\" Loss: \", round(loss$item()[1], 4), \"\\n\"))\n}\n\nEpoch: 10 Loss: 0.6151\nEpoch: 20 Loss: 0.6163\nEpoch: 30 Loss: 0.5745\nEpoch: 40 Loss: 0.5362\nEpoch: 50 Loss: 0.5829\n\n\nMake predictions:\n\npreds = list()\ntest = torch_geometric$loader$DataLoader(dataset, batch_size=64L,shuffle=FALSE)\niterator = reticulate::as_iterator(test)\nmodel$eval()\n\nGCN(\n  (conv): GCNConv(7, 64)\n  (linear): Linear(in_features=64, out_features=2, bias=True)\n)\n\ncounter = 1\ncoro::loop(for (b in iterator) {\n  preds[[counter]] = model(b$x, b$edge_index, b$batch)\n  counter <<- counter + 1\n  })\nhead(torch$concat(preds)$sigmoid()$data$cpu()$numpy(), n = 3)\n\n          [,1]      [,2]\n[1,] 0.3076028 0.6427078\n[2,] 0.4121239 0.5515330\n[3,] 0.4119514 0.5516798\n\n\n\n\n\n# Load python packages torch and torch_geometric via the reticulate R package\nimport torch\nimport torch_geometric\n\n# helper functions from the torch_geometric modules\nGCNConv = torch_geometric.nn.GCNConv\nglobal_mean_pool = torch_geometric.nn.global_mean_pool\n\n\n# Download the MUTAG TUDataset\ndataset = torch_geometric.datasets.TUDataset(root='data/TUDataset', \n                                             name='MUTAG')\ndataloader = torch_geometric.loader.DataLoader(dataset, \n                                               batch_size=64,\n                                               shuffle=True)\n\n# Create the model with a python class\n# There are two classes in the response variable\nclass GCN(torch.nn.Module):\n    def __init__(self, hidden_channels):\n         super().__init__()\n         torch.manual_seed(42)\n         self.conv = GCNConv(dataset.num_node_features, hidden_channels)\n         self.linear = torch.nn.Linear(hidden_channels, dataset.num_classes)\n         \n    def forward(self, x, edge_index, batch):\n        x = self.conv(x, edge_index)\n        x = x.relu()\n        x = global_mean_pool(x, batch)\n        x = torch.nn.functional.dropout(x, p = 0.5, training=self.training)\n        x = self.linear(x)\n        return x\n\nTraining loop:\n\n# create model object\nmodel = GCN(hidden_channels = 64)\n\n# get optimizer and loss function\noptimizer = torch.optim.Adamax(model.parameters(), lr = 0.01)\nloss_func = torch.nn.CrossEntropyLoss()\n\n# set model into training mode (because of the dropout layer)\nmodel.train()\n\n# train model\n\nGCN(\n  (conv): GCNConv(7, 64)\n  (linear): Linear(in_features=64, out_features=2, bias=True)\n)\n\nfor e in range(50):\n  for b in dataloader:\n  \n    pred = model(b.x, b.edge_index, b.batch)\n    loss = loss_func(pred, b.y)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n     \n  if e % 10 ==0:\n    print(\"Epoch: \", e ,\" Loss: \", loss.item(), \"\\n\")\n\nEpoch:  0  Loss:  0.6617005467414856 \n\nEpoch:  10  Loss:  0.614981472492218 \n\nEpoch:  20  Loss:  0.6161867380142212 \n\nEpoch:  30  Loss:  0.5802667737007141 \n\nEpoch:  40  Loss:  0.5124867558479309 \n\n\nMake predictions:\n\npreds = []\ntest = torch_geometric.loader.DataLoader(dataset, batch_size=64,shuffle=False)\nmodel.eval()\n\nGCN(\n  (conv): GCNConv(7, 64)\n  (linear): Linear(in_features=64, out_features=2, bias=True)\n)\n\ncounter = 1\nfor b in test:\n  preds.append( model(b.x, b.edge_index, b.batch) )\n  \n  \ntorch.concat(preds).sigmoid().data.cpu().numpy()[0:10]\n\narray([[0.30760282, 0.64270777],\n       [0.41212386, 0.551533  ],\n       [0.4119514 , 0.5516798 ],\n       [0.29887193, 0.650517  ],\n       [0.48894534, 0.48584774],\n       [0.4310807 , 0.5360305 ],\n       [0.31375578, 0.63721913],\n       [0.34597102, 0.6093393 ],\n       [0.50279325, 0.4740774 ],\n       [0.30924183, 0.6412629 ]], dtype=float32)\n\n\n\n\n\n\n\n\n\nFey, Matthias, and Jan E. Lenssen. 2019. “Fast Graph Representation Learning with PyTorch Geometric.” In ICLR Workshop on Representation Learning on Graphs and Manifolds.\n\n\nPaszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, et al. 2019. “PyTorch: An Imperative Style, High-Performance Deep Learning Library.” In Advances in Neural Information Processing Systems 32, 8024–35. Curran Associates, Inc. http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.\n\n\nUshey, Kevin, JJ Allaire, and Yuan Tang. 2022. Reticulate: Interface to ’Python’. https://CRAN.R-project.org/package=reticulate."
  },
  {
    "objectID": "refs.html",
    "href": "refs.html",
    "title": "References",
    "section": "",
    "text": "Fey, Matthias, and Jan E. Lenssen. 2019. “Fast Graph\nRepresentation Learning with PyTorch Geometric.” In\nICLR Workshop on Representation Learning on Graphs and\nManifolds.\n\n\nMeyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and\nFriedrich Leisch. 2022. E1071: Misc Functions of the Department of\nStatistics, Probability Theory Group (Formerly: E1071), TU Wien. https://CRAN.R-project.org/package=e1071.\n\n\nPaszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,\nGregory Chanan, Trevor Killeen, et al. 2019. “PyTorch: An\nImperative Style, High-Performance Deep Learning Library.” In\nAdvances in Neural Information Processing Systems 32, 8024–35.\nCurran Associates, Inc. http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.\n\n\nSchliep, Klaus, and Klaus Hechenbichler. 2016. Kknn: Weighted\nk-Nearest Neighbors. https://CRAN.R-project.org/package=kknn.\n\n\nSimon, Noah, Jerome Friedman, Trevor Hastie, and Rob Tibshirani. 2011.\n“Regularization Paths for Cox’s Proportional Hazards Model via\nCoordinate Descent.” Journal of Statistical Software 39\n(5): 1–13. https://doi.org/10.18637/jss.v039.i05.\n\n\nUshey, Kevin, JJ Allaire, and Yuan Tang. 2022. Reticulate: Interface\nto ’Python’. https://CRAN.R-project.org/package=reticulate.\n\n\nWright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests\nfor High Dimensional Data in C++ and\nR.” Journal of Statistical Software 77 (1):\n1–17. https://doi.org/10.18637/jss.v077.i01."
  },
  {
    "objectID": "brt.html",
    "href": "brt.html",
    "title": "5  Boosted gradient trees",
    "section": "",
    "text": "Boosted gradient machines achieve currently state-of-the-art performance for structured (tabular) data which makes them probably one of the most important algorithms for E&E where structured data dominates the field.\nIn the following, we use the ‘xgboost’ package (Chen et al. (2022))."
  },
  {
    "objectID": "brt.html#classification",
    "href": "brt.html#classification",
    "title": "5  Boosted gradient trees",
    "section": "5.1 Classification",
    "text": "5.1 Classification\n\nRPython\n\n\n\nlibrary(xgboost)\nX = as.matrix(iris[,1:4])\nY = as.integer(iris[,5]) - 1 # classes must be integers starting from 0\n\nxgdata = xgb.DMatrix(X, label = Y)\n\n# nrounds = number of trees in the ensemble\nbrt = xgboost(data = xgdata, \n              objective=\"multi:softprob\", \n              nrounds = 50, \n              num_class = 3,\n              verbose = 0)\n\nShow feature importances:\n\nxgb.importance(model = brt)\n\n        Feature        Gain      Cover Frequency\n1: Petal.Length 0.671879438 0.57441039 0.3792049\n2:  Petal.Width 0.311535837 0.29261084 0.3088685\n3:  Sepal.Width 0.010177107 0.04910115 0.1162080\n4: Sepal.Length 0.006407618 0.08387763 0.1957187\n\n\nMake predictions (class probabilities):\n\nhead(matrix(predict(brt, newdata = xgb.DMatrix(X)), ncol =3), n = 3)\n\n            [,1]        [,2]        [,3]\n[1,] 0.995287061 0.002195822 0.001027058\n[2,] 0.003323558 0.995396435 0.001592265\n[3,] 0.001389398 0.002407764 0.997380674\n\n\n\n\n\nimport xgboost as xgb\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\n# Parameters:\nparam = {\n  'max_depth':2, \n  'eta':1, \n  'objective':'multi:softmax' }\nnum_round = 50\n\nmodel = xgb.XGBClassifier(param, num_round, verbosity = 0).fit(X, Y)\n\n/home/maxpichler/miniconda3/envs/r-sjsdm/lib/python3.7/site-packages/xgboost/core.py:502: FutureWarning: Pass `objective, use_label_encoder` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n  format(\", \".join(args_msg)), FutureWarning\n/home/maxpichler/miniconda3/envs/r-sjsdm/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n\n\nFeature importance\n\nmodel.feature_importances_\n\narray([0.00959796, 0.01645038, 0.6765859 , 0.29736578], dtype=float32)\n\n\nMake predictions:\n\nmodel.predict_proba(X)[0:10,:]\n\narray([[9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9636227e-01, 2.3820533e-03, 1.2557388e-03],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9679452e-01, 2.3830866e-03, 8.2237815e-04],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9680281e-01, 2.3831066e-03, 8.1413286e-04],\n       [9.9636227e-01, 2.3820533e-03, 1.2557388e-03],\n       [9.9679452e-01, 2.3830866e-03, 8.2237815e-04]], dtype=float32)"
  },
  {
    "objectID": "brt.html#regression",
    "href": "brt.html#regression",
    "title": "5  Boosted gradient trees",
    "section": "5.2 Regression",
    "text": "5.2 Regression\n\nRPython\n\n\n\nlibrary(xgboost)\nX = as.matrix(iris[,2:4])\nY = iris[,1]\n\nxgdata = xgb.DMatrix(X, label = Y)\n\n# nrounds = number of trees in the ensemble\nbrt = xgboost(data = xgdata, \n              objective=\"reg:squarederror\", \n              nrounds = 50, \n              verbose = 0)\n\nShow feature importances:\n\nxgb.importance(model = brt)\n\n        Feature       Gain     Cover Frequency\n1: Petal.Length 0.86781219 0.4789538 0.3789062\n2:  Petal.Width 0.06987880 0.2128402 0.2626953\n3:  Sepal.Width 0.06230901 0.3082060 0.3583984\n\n\nMake predictions:\n\nhead(predict(brt, newdata = xgb.DMatrix(X), n = 3))\n\n[10:50:56] WARNING: amalgamation/../src/c_api/c_api.cc:785: `ntree_limit` is deprecated, use `iteration_range` instead.\n\n\n[1] 3.506606 3.506606 3.506606 3.506606 3.506606 3.506606\n\n\n\n\n\nimport xgboost as xgb\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\n# Parameters:\nmodel = xgb.XGBRegressor(\n  objective = 'reg:squarederror',\n  max_depth = 2,  \n  n_estimators = 50, \n  verbosity = 0).fit(X, Y)\n\nFeature importance:\n\nprint(model.feature_importances_)\n\n[0.08471056 0.835755   0.07953447]\n\n\nMake predictions:\n\nmodel.predict(X)[0:10]\n\narray([5.0407157, 4.6844926, 4.711238 , 4.917956 , 5.0407157, 5.450946 ,\n       4.928966 , 4.986462 , 4.6750975, 4.917956 ], dtype=float32)\n\n\n\n\n\n\n\n\n\nChen, Tianqi, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, et al. 2022. Xgboost: Extreme Gradient Boosting. https://CRAN.R-project.org/package=xgboost."
  },
  {
    "objectID": "dnn.html",
    "href": "dnn.html",
    "title": "6  Deep neural networks",
    "section": "",
    "text": "Deep neural networks, or more precisely here fully connected neural networks, can be flexibly built which makes their application more challenging than other ML algorithms.\nIn the following, we use the ‘keras’ (Allaire and Chollet (2022); Chollet et al. (2015)) package which is a higher level API on the python ‘tensorflow’ framework (Abadi et al. (2016))."
  },
  {
    "objectID": "dnn.html#classification",
    "href": "dnn.html#classification",
    "title": "6  Deep neural networks",
    "section": "6.1 Classification",
    "text": "6.1 Classification\n\nRPython\n\n\n\nlibrary(keras)\nX = scale(as.matrix(iris[,1:4]))\nY = as.integer(iris$Species)\n# We need to one hot encode our response classes\nYT = k_one_hot(Y-1L, num_classes = 3)\n\nDNN = keras_model_sequential() %>% \n  # first hidden layer\n  layer_dense(input_shape = ncol(X), \n              units = 10, \n              activation = \"relu\") %>% \n  # second hidden layer with regularization\n  layer_dense(units = 20, \n              activation = \"relu\",\n              kernel_regularizer = regularizer_l1()) %>% \n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\n  layer_dense(units = 3, \n              activation = \"softmax\")\n\n# print architecture\nsummary(DNN)\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_2 (Dense)                    (None, 10)                      50          \n dense_1 (Dense)                    (None, 20)                      220         \n dense (Dense)                      (None, 3)                       63          \n================================================================================\nTotal params: 333\nTrainable params: 333\nNon-trainable params: 0\n________________________________________________________________________________\n\n# add loss function and optimizer\nDNN %>% \n  compile(loss = loss_categorical_crossentropy,\n          optimizer = optimizer_adamax(0.01))\n\n# train model\nDNN %>% \n  fit(X, YT, epochs = 50, verbose = 0)\n\nMake predictions (class probabilities):\n\nhead(predict(DNN, X), n = 3)\n\n          [,1]        [,2]         [,3]\n[1,] 0.9968244 0.003130971 4.465887e-05\n[2,] 0.9858423 0.013976931 1.807758e-04\n[3,] 0.9960486 0.003893664 5.765830e-05\n\n\n\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\nX = scale(iris.data)\nY = iris.target\n\n# We need to one hot encode our response classes\nYT = keras.utils.to_categorical(Y, num_classes = 3)\n\nDNN = keras.Sequential()\n  # first hidden layer\nDNN.add(Dense(\n  input_shape=[X.shape[1]], \n  units = 10, \n  activation = \"relu\")) \n  # second hidden layer with regularization\nDNN.add(Dense(\n  units = 20, \n  activation = \"relu\",\n  kernel_regularizer = keras.regularizers.l1()))\n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\nDNN.add(Dense(\n  units = 3, \n  activation = \"softmax\"))\n\n# print architecture\nDNN.summary()\n\n# add loss function and optimizer\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_3 (Dense)             (None, 10)                50        \n                                                                 \n dense_4 (Dense)             (None, 20)                220       \n                                                                 \n dense_5 (Dense)             (None, 3)                 63        \n                                                                 \n=================================================================\nTotal params: 333\nTrainable params: 333\nNon-trainable params: 0\n_________________________________________________________________\n\nDNN.compile(loss = keras.losses.categorical_crossentropy,\n            optimizer = keras.optimizers.Adamax(0.01))\n\n# train model\nDNN.fit(X, YT, epochs = 50, verbose = 0)\n\n<keras.callbacks.History object at 0x7ff7e648e860>\n\n\nMake predictions:\n\nDNN.predict(X)[0:10,:]\n\n\n1/5 [=====>........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n5/5 [==============================] - 0s 1ms/step\narray([[9.9604118e-01, 3.9030442e-03, 5.5764249e-05],\n       [9.8632067e-01, 1.3529156e-02, 1.5015916e-04],\n       [9.9591041e-01, 4.0376107e-03, 5.2057381e-05],\n       [9.9484915e-01, 5.0889952e-03, 6.1775587e-05],\n       [9.9729079e-01, 2.6730390e-03, 3.6100675e-05],\n       [9.9496955e-01, 4.9541364e-03, 7.6401542e-05],\n       [9.9712002e-01, 2.8428256e-03, 3.7198897e-05],\n       [9.9558902e-01, 4.3485621e-03, 6.2399165e-05],\n       [9.9313205e-01, 6.7970627e-03, 7.0966795e-05],\n       [9.9308324e-01, 6.8320944e-03, 8.4627231e-05]], dtype=float32)"
  },
  {
    "objectID": "dnn.html#regression",
    "href": "dnn.html#regression",
    "title": "6  Deep neural networks",
    "section": "6.2 Regression",
    "text": "6.2 Regression\n\n\n6.2.1 R\n\nlibrary(keras)\nX = scale(as.matrix(iris[,2:4]))\nY = as.matrix(iris[,1,drop=FALSE])\n\nDNN = keras_model_sequential() %>% \n  # first hidden layer\n  layer_dense(input_shape = ncol(X), \n              units = 10, \n              activation = \"relu\") %>% \n  # second hidden layer with regularization\n  layer_dense(units = 20, \n              activation = \"relu\",\n              kernel_regularizer = regularizer_l1()) %>% \n  # output layer, one output neuron for one response\n  # and no activation function\n  layer_dense(units = 1)\n\n# print architecture\nsummary(DNN)\n\nModel: \"sequential_2\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_8 (Dense)                    (None, 10)                      40          \n dense_7 (Dense)                    (None, 20)                      220         \n dense_6 (Dense)                    (None, 1)                       21          \n================================================================================\nTotal params: 281\nTrainable params: 281\nNon-trainable params: 0\n________________________________________________________________________________\n\n# add loss function and optimizer\nDNN %>% \n  compile(loss = loss_mean_squared_error,\n          optimizer = optimizer_adamax(0.01))\n\n# train model\nDNN %>% \n  fit(X, YT, epochs = 50, verbose = 0)\n\nMake predictions:\n\nhead(predict(DNN, X), n = 3)\n\n          [,1]\n[1,] 0.3332931\n[2,] 0.3334758\n[3,] 0.3333959\n\n\n\n\n6.2.2 Python\n\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\niris = datasets.load_iris()\ndata = iris.data\nX = scale(data[:,1:4])\nY = data[:,0]\n\nDNN = keras.Sequential()\n  # first hidden layer\nDNN.add(Dense(\n  input_shape=[X.shape[1]], \n  units = 10, \n  activation = \"relu\")) \n  # second hidden layer with regularization\nDNN.add(Dense(\n  units = 20, \n  activation = \"relu\",\n  kernel_regularizer = keras.regularizers.l1()))\n  # output layer, 3 output neurons for our three classes\n  # and softmax activation to get quasi probabilities \n  # that sum up to 1 for each observation\nDNN.add(Dense(\n  units = 1, \n  activation = None))\n\n# print architecture\nDNN.summary()\n\n# add loss function and optimizer\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_9 (Dense)             (None, 10)                40        \n                                                                 \n dense_10 (Dense)            (None, 20)                220       \n                                                                 \n dense_11 (Dense)            (None, 1)                 21        \n                                                                 \n=================================================================\nTotal params: 281\nTrainable params: 281\nNon-trainable params: 0\n_________________________________________________________________\n\nDNN.compile(loss = keras.losses.mean_squared_error,\n            optimizer = keras.optimizers.Adamax(0.01))\n\n# train model\nDNN.fit(X, Y, epochs = 50, verbose = 0)\n\n<keras.callbacks.History object at 0x7ff7e61e5ac8>\n\n\nMake predictions:\n\nDNN.predict(X)[0:10]\n\n\n1/5 [=====>........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n5/5 [==============================] - 0s 1ms/step\narray([[5.0288253],\n       [4.8119473],\n       [4.9024477],\n       [4.8307614],\n       [5.158363 ],\n       [5.384384 ],\n       [4.8593917],\n       [4.946008 ],\n       [4.780525 ],\n       [4.9478073]], dtype=float32)\n\n\n\n\n\n\n\n\nAbadi, Martı́n, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. 2016. “Tensorflow: A System for Large-Scale Machine Learning.” In 12th \\(\\{\\)USENIX\\(\\}\\) Symposium on Operating Systems Design and Implementation (\\(\\{\\)OSDI\\(\\}\\) 16), 265–83.\n\n\nAllaire, JJ, and François Chollet. 2022. Keras: R Interface to ’Keras’. https://CRAN.R-project.org/package=keras.\n\n\nChollet, Francois et al. 2015. “Keras.” GitHub. 2015. https://github.com/fchollet/keras."
  }
]