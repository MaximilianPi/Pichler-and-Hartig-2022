# Ridge, LASSO, and elastic-net regression

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
reticulate::use_condaenv("r-sjsdm", required = TRUE)
#reticulate::use_python("/home/maxpichler/miniconda3/envs/r-sjsdm/bin/python", required = TRUE)
```


We can use the 'glmnet' R package (@glmnet) for Ridge, LASSO, or elastic-net regularization. The 'glmnet' package supports different response families including 'gaussian', 'binomial' and 'Poisson'. The strength of the regularization is set by the 'lambda' argument ($\lambda$) and the weighting between Ridge and LASSO regularization by the 'alpha' parameter ($\alpha$):

$$
\lambda*[(1 - \alpha)\|\beta\|_1 + \alpha\|\beta||^2]
$$ Setting alpha = 0 turns off the LASSO and alpha = 1 the Ridge. Alphas between (0,1) will use both regularization types, turning the model into an elastic-net regularization.

When using regularization, it is important to scale all features otherwise effects for features that are on a larger scale are stronger regularized.


In python, the 'scikit-learn' package provides an interface for many different ML algorithms, including elastic-net regression models (@scikit-learn).

The 'MLJ' package provides a generic interface for different ML algorithms (@anthony_blaom_2019_3541506). Elastic-net regression models can be accessed via the 'MLJLinearModels' package within MLJ.


## Classification

Build models (for regularization it is important to scale the features):

::: panel-tabset
### R

```{r, message=FALSE}
library(glmnet)
X = scale(iris[,1:4])
Y = iris$Species

# Ridge:
ridge = glmnet(X, Y, family = "multinomial", alpha = 0, lambda = 0.01)

# LASSO:
lasso = glmnet(X, Y, family = "multinomial", alpha = 1, lambda = 0.01)

# Elastic-net:
elastic = glmnet(X, Y, family = "multinomial", alpha = 0.5, lambda = 0.01)
```

Make predictions (class probabilities):

```{r, message=FALSE}
head(predict(lasso, newx = X, type = "response")[,,1], n = 3)
```

### Python

In the sklearn implementation the regularization strength parameter 'C' corresponds to the lambda parameter from glmnet:

```{r, include=FALSE}
Sys.setenv(PYTHONWARNINGS="ignore")
```

```{python, warning=FALSE, message=FALSE}
from sklearn.linear_model import LogisticRegression
from sklearn import datasets
from sklearn.preprocessing import scale
iris = datasets.load_iris()
X = scale(iris.data)
Y = iris.target

```

Models:

```{python}

# Ridge:
ridge = LogisticRegression(multi_class='multinomial', 
                           penalty = "l2", 
                           C = 0.01, 
                           solver="saga")
ridge.fit(X, Y)

# LASSO:
lasso = LogisticRegression(multi_class='multinomial', 
                           penalty = "l1", 
                           C = 0.01, 
                           solver="saga")
lasso.fit(X, Y)

# Elastic-net:
elastic = LogisticRegression(multi_class='multinomial', 
                             penalty = "elasticnet", 
                             C = 0.01, 
                             l1_ratio=0.5, 
                             solver="saga")
elastic.fit(X, Y)

```

Predictions (probabilities):

```{python}

lasso.predict_proba(X)[0:5,:]

```

### Julia


```{julia}
import StatsBase;
using MLJ;
using MLJLinearModels;
@load MultinomialClassifier pkg=MLJLinearModels;
using RDatasets;
using StatsBase;
using DataFrames;
```

```{julia}
iris = dataset("datasets", "iris");
X = mapcols(StatsBase.zscore, iris[:, 1:4]);
Y = iris[:, 5];

```

Models:

```{julia}

# Ridge
ridge = fit!(machine(MultinomialClassifier(lambda = 0.01, penalty = "l2"), X, Y));

# Lasso
lasso = fit!(machine(MultinomialClassifier(lambda = 0.01, penalty = "l1"), X, Y));


# Elastic-net
elastic = fit!(machine(MultinomialClassifier(lambda = 0.01, gamma = 0.01, penalty = "en"), X, Y));


```

Predictions:

```{julia}
MLJ.predict(lasso, X)[1:5]
```


:::

## Regression

::: panel-tabset
### R

```{r, message=FALSE}
X = scale(iris[,2:4])
Y = iris[,1]

# Ridge:
ridge = glmnet(X, Y, family = gaussian(), alpha = 0, lambda = 0.01)

# LASSO:
lasso = glmnet(X, Y, family = gaussian(), alpha = 1, lambda = 0.01)

# Elastic-net:
elastic = glmnet(X, Y, family = gaussian(), alpha = 0.5, lambda = 0.01)
```

Make predictions (class probabilities):

```{r, message=FALSE}
head(predict(lasso, newx = X), n = 3)
```

### Python

For regressions we can use the ElasticNet model class, here, however, lambda corresponds to alpha and l1_ratio to the alpha parameter.

```{python, warning=FALSE, message=FALSE}
from sklearn.linear_model import ElasticNet
from sklearn import datasets
from sklearn.preprocessing import scale
iris = datasets.load_iris()
data = iris.data
X = scale(data[:,1:4])
Y = data[:,0]


# Ridge:
ridge = ElasticNet(alpha = 0.01,
                   l1_ratio = 0.011)
ridge.fit(X, Y)

# LASSO:
lasso = ElasticNet(alpha = 0.01,
                   l1_ratio = 1.0)
lasso.fit(X, Y)

# Elastic-net:
elastic = ElasticNet(alpha = 0.01,
                     l1_ratio = 0.5)
elastic.fit(X, Y)

# Make predictions:

lasso.predict(X)[0:10]

```

### Julia

```{julia}
import StatsBase;
using MLJ;
using MLJLinearModels;
@load LassoRegressor pkg=MLJLinearModels;
@load RidgeRegressor pkg=MLJLinearModels;
@load ElasticNetRegressor pkg=MLJLinearModels;
using RDatasets;
using DataFrames;
```

```{julia}
iris = dataset("datasets", "iris");
X = mapcols(StatsBase.zscore, iris[:, 2:4]);
Y = iris[:, 1];
```

Models:

```{julia}
# Ridge
ridge = fit!(machine(RidgeRegressor(lambda = 0.01), X, Y));

# Lasso
lasso = fit!(machine(LassoRegressor(lambda = 0.01), X, Y));


# Elastic-net
elastic = fit!(machine(ElasticNetRegressor(lambda = 0.01, gamma = 0.01), X, Y));
```

Predictions (probabilities):

```{julia}
MLJ.predict(lasso, X)[1:5]
```


:::
