---
title: "Supporting information S1 for Pichler & Hartig – Machine Learning and Deep Learning – A review for Ecologists"
author: Pichler & Hartig, 2022
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
```

## 1 Trend analysis


```{r, echo=FALSE}
library(flextable)
set_flextable_defaults(fonts_ignore=TRUE)
queries = c(
  '("artificial neural network" OR "deep neural network" OR 
  "multi-layer perceptron" OR "fully connected neural network")',
  '("convolutional neural network" OR "object detection")',
  '("recurrent neural network")',
  '("graph neural network" OR "graph convolutional")',
  '("random forest")',
  '("boosted regression tree" OR "boosted reg" OR 
  "gradient boosting" OR "adaboost")',
  '("k-nearest-neighbor")',
  '("ridge regression" OR "lasso regression" OR 
  "elastic-net" OR "elastic net")',
  '("support vector machine" OR "support vector")'
)
algorithms = c(
  "Deep neural network (ANN)",
  "Convolutional neural network (CNN)",
  "Recurrent neural network (RNN)",
  "Graph neural network (GNN)",
  "Random Forest (RF)",
  "Boosted Regression Trees (BRT)",
  "k-nearest neighbor (kNN)",
  "Ridge, lasso, or elastic-net regression",
  "Support vector machine (SVM)"
)
data = data.frame(cbind(queries, algorithms))
colnames(data) = c("Queries", "ML and DL algorithm")
fl = flextable(data)
fl = set_caption(fl, caption = "")
fl = theme_zebra(fl)
fl = set_table_properties(fl, width = .3, layout = "autofit")
fl
```



### Classification

Build models (for regularization it is important to scale the features):
```{r, message=FALSE}
library(glmnet)
X = scale(iris[,1:4])
Y = iris$Species

# Ridge:
ridge = glmnet(X, Y, family = "multinomial", alpha = 0, lambda = 0.01)

# LASSO:
lasso = glmnet(X, Y, family = "multinomial", alpha = 1, lambda = 0.01)

# Elastic-net:
elastic = glmnet(X, Y, family = "multinomial", alpha = 0.5, lambda = 0.01)
```

Make predictions (class probabilities):
```{r, message=FALSE}
head(predict(lasso, newx = X, type = "response")[,,1], n = 3)
```

### Regression
```{r, message=FALSE}
X = scale(iris[,2:4])
Y = iris[,1]

# Ridge:
ridge = glmnet(X, Y, family = gaussian(), alpha = 0, lambda = 0.01)

# LASSO:
lasso = glmnet(X, Y, family = gaussian(), alpha = 1, lambda = 0.01)

# Elastic-net:
elastic = glmnet(X, Y, family = gaussian(), alpha = 0.5, lambda = 0.01)
```

Make predictions (class probabilities):
```{r, message=FALSE}
head(predict(lasso, newx = X), n = 3)
```

## 3 Support Vector Machines

### Classification:
```{r, message=FALSE}
library(e1071)
X = scale(iris[,1:4])
Y = iris$Species

sv = svm(X, Y, probability = TRUE) 
summary(sv)
```

Make predictions (class probabilities):
```{r, message=FALSE}
head(attr(predict(sv, newdata = X, probability = TRUE), "probabilities"), n = 3)
```


### Regression:
```{r, message=FALSE}
library(e1071)
X = scale(iris[,2:4])
Y = iris[,1]

sv = svm(X, Y) 
summary(sv)
```

Make predictions (class probabilities):
```{r, message=FALSE}
head(predict(sv, newdata = X), n = 3)
```



## 4 k-nearest-neighbor

### Classification:
```{r, message=FALSE}
library(kknn)
X = scale(iris[,1:4])
Y = iris[,5,drop=FALSE]
data = cbind(Y, X)

knn = kknn(Species~., train = data, test = data) 
```

Make predictions (class probabilities):
```{r, message=FALSE}
head(knn$prob, n = 3)
```


### Regression:
```{r, message=FALSE}
library(e1071)
X = scale(iris[,2:4])
data = cbind(iris[,1,drop=FALSE], X)

knn = kknn(Sepal.Length~., train = data, test = data) 
```

Make predictions (class probabilities):
```{r, message=FALSE}
head(predict(knn), n = 3)
```



## 5 Random forest

### Classification:
```{r, message=FALSE}
library(ranger)
X = iris[,1:4]
Y = iris[,5,drop=FALSE]
data = cbind(Y, X)

rf = ranger(Species~., data = data, probability = TRUE, importance = "impurity")
```

Show feature importances:
```{r, message=FALSE}
importance(rf)
```

Make predictions (class probabilities):
```{r, message=FALSE}
head(predict(rf, data = data)$predictions, n = 3)
```


### Regression:
```{r, message=FALSE}
library(ranger)
X = iris[,2:4]
data = cbind(iris[,1,drop=FALSE], X)

rf = ranger(Sepal.Length~., data = data, importance = "impurity")
```


Show feature importances:
```{r, message=FALSE}
importance(rf)
```

Make predictions (class probabilities):
```{r, message=FALSE}
head(predict(rf, data = data)$predictions, n = 3)
```



## 6 Boosted gradient trees

### Classification:
```{r, message=FALSE}
library(xgboost)
X = as.matrix(iris[,1:4])
Y = as.integer(iris[,5]) - 1 # classes must be integers starting from 0

xgdata = xgb.DMatrix(X, label = Y)

# nrounds = number of trees in the ensemble
brt = xgboost(data = xgdata, 
              objective="multi:softprob", 
              nrounds = 50, 
              num_class = 3,
              verbose = 0)
```

Show feature importances:
```{r, message=FALSE}
xgb.importance(model = brt)
```

Make predictions (class probabilities):
```{r, message=FALSE}
head(matrix(predict(brt, newdata = xgb.DMatrix(X)), ncol =3), n = 3)
```


### Regression:
```{r, message=FALSE}
library(xgboost)
X = as.matrix(iris[,2:4])
Y = iris[,1]

xgdata = xgb.DMatrix(X, label = Y)

# nrounds = number of trees in the ensemble
brt = xgboost(data = xgdata, 
              objective="reg:squarederror", 
              nrounds = 50, 
              verbose = 0)
```

Show feature importances:
```{r, message=FALSE}
xgb.importance(model = brt)
```

Make predictions:
```{r, message=FALSE}
head(predict(brt, newdata = xgb.DMatrix(X), n = 3))
```



## 7 Deep neural networks

### Classification:
```{r, message=FALSE,eval=TRUE}
library(keras)
X = scale(as.matrix(iris[,1:4]))
Y = as.integer(iris$Species)
# We need to one hot encode our response classes
YT = k_one_hot(Y-1L, num_classes = 3)

DNN = keras_model_sequential() %>% 
  # first hidden layer
  layer_dense(input_shape = ncol(X), 
              units = 10, 
              activation = "relu") %>% 
  # second hidden layer with regularization
  layer_dense(units = 20, 
              activation = "relu",
              kernel_regularizer = regularizer_l1()) %>% 
  # output layer, 3 output neurons for our three classes
  # and softmax activation to get quasi probabilities 
  # that sum up to 1 for each observation
  layer_dense(units = 3, 
              activation = "softmax")

# print architecture
summary(DNN)

# add loss function and optimizer
DNN %>% 
  compile(loss = loss_categorical_crossentropy,
          optimizer = optimizer_adamax(0.01))

# train model
DNN %>% 
  fit(X, YT, epochs = 50, verbose = 0)
```

Make predictions (class probabilities):
```{r, message=FALSE,eval=TRUE}
head(predict(DNN, X), n = 3)
```


### Regression:
```{r, message=FALSE,eval=TRUE}
library(keras)
X = scale(as.matrix(iris[,2:4]))
Y = as.matrix(iris[,1,drop=FALSE])

DNN = keras_model_sequential() %>% 
  # first hidden layer
  layer_dense(input_shape = ncol(X), 
              units = 10, 
              activation = "relu") %>% 
  # second hidden layer with regularization
  layer_dense(units = 20, 
              activation = "relu",
              kernel_regularizer = regularizer_l1()) %>% 
  # output layer, one output neuron for one response
  # and no activation function
  layer_dense(units = 1)

# print architecture
summary(DNN)

# add loss function and optimizer
DNN %>% 
  compile(loss = loss_mean_squared_error,
          optimizer = optimizer_adamax(0.01))

# train model
DNN %>% 
  fit(X, YT, epochs = 50, verbose = 0)
```

Make predictions:
```{r, message=FALSE,eval=TRUE}
head(predict(DNN, X), n = 3)
```


## 8 Convolutional neural networks
```{r,eval=TRUE}
library(keras)
data = keras::dataset_mnist()
train = data$train
X = train$x/255
# we have to add a dimension that 
# informs the network about the channels
# of the images
X = array(X, dim = c(dim(X), 1))
YT = k_one_hot(train$y, num_classes = 10)



CNN = 
  keras_model_sequential() %>% 
  # first hidden layer
  layer_conv_2d(input_shape = list(28, 28, 1), 
                filters = 16,
                kernel_size = c(2, 2),
                activation = "relu") %>%
  layer_average_pooling_2d() %>% 
  # second hidden layer with regularization
  layer_conv_2d(filters = 8,
                kernel_size = c(2, 2),
                activation = "relu") %>%
  # we use a normal DNN on top of the CNN:
  # the layer flatten will remove the additional 
  # dimensions
  layer_flatten() %>% 
  layer_dense(units = 20, 
              activation = "relu") %>%
  # 10 output neurons for 10 classes
  layer_dense(units = 10, 
              activation = "softmax")


# print architecture
summary(CNN)

# add loss function and optimizer
CNN %>% 
  compile(loss = loss_categorical_crossentropy,
          optimizer = optimizer_adamax(0.01))

CNN %>% 
  fit(X, YT, epochs = 3, verbose = 1, batch_size = 125)
```

Make predictions (class probabilites):
```{r, eval=TRUE}
head(predict(CNN, X[1:100,,,,drop=FALSE]), n = 3)
```


## 9 Recurrent neural networks

```{r, eval=TRUE}
## RNNs
library(keras)
data = as.matrix(arima.sim(n = 1000, list(ar = c(0.3, -0.7)) ))
# We use here a simplified way to create X and Y 
# since the focus is on creating the RNNs
data = matrix(data, ncol = 10L, byrow = TRUE)
X = array(data[seq(1, 100, by = 2), ], dim = c(50, 10, 1))
Y = data[seq(2, 100, by = 2), ]

RNN = 
  keras_model_sequential() %>% 
  # first hidden layer
  layer_gru(input_shape = list(10L, 1L),
            units = 50, 
            activation = "relu") %>%
  # we want to predict the next 10 time steps
  layer_dense(units = 10)


# add loss function and optimizer
RNN %>% 
  compile(loss = loss_mean_squared_error,
          optimizer = optimizer_adamax(0.01))

RNN %>% 
  fit(X, Y, epochs = 50, verbose = 0)
```

Make predictions:
```{r, eval=TRUE}
head(predict(RNN, X), n = 3)
```


## 10 Graph (convolutional) neural networks


```{r}
library(reticulate)
# Load python packages torch and torch_geometric via the reticulate R package
torch = import("torch") 
torch_geometric = import("torch_geometric")

# helper functions from the torch_geometric modules
GCNConv = torch_geometric$nn$GCNConv
global_mean_pool = torch_geometric$nn$global_mean_pool


# Download the MUTAG TUDataset
dataset = torch_geometric$datasets$TUDataset(root='data/TUDataset', 
                                             name='MUTAG')
dataloader = torch_geometric$loader$DataLoader(dataset, 
                                               batch_size=64L,
                                               shuffle=TRUE)

# Create the model with a python class
# There are two classes in the response variable
GCN = PyClass(
  "GCN", 
   inherit = torch$nn$Module, 
   defs = list(
       `__init__` = function(self, hidden_channels) {
         super()$`__init__`()
         torch$manual_seed(42L)
         self$conv = GCNConv(dataset$num_node_features, hidden_channels)
         self$linear = torch$nn$Linear(hidden_channels, dataset$num_classes)
         NULL
       },
       forward = function(self, x, edge_index, batch) {
         x = self$conv(x, edge_index)
         x = x$relu()
         x = global_mean_pool(x, batch)
         
         x = torch$nn$functional$dropout(x, p = 0.5, training=self$training)
         x = self$linear(x)
         return(x)
       }
   ))
```

Training loop:
```{r}
# create model object
model = GCN(hidden_channels = 64L)

# get optimizer and loss function
optimizer = torch$optim$Adamax(model$parameters(), lr = 0.01)
loss_func = torch$nn$CrossEntropyLoss()

# set model into training mode (because of the dropout layer)
model$train()

# train model
for(e in 1:50) {
  iterator = reticulate::as_iterator(dataloader)
  coro::loop(for (b in iterator) { 
     pred = model(b$x, b$edge_index, b$batch)
     loss = loss_func(pred, b$y)
     loss$backward()
     optimizer$step()
     optimizer$zero_grad()
  })
  if(e %% 10 ==0) cat(paste0("Epoch: ",e," Loss: ", round(loss$item()[1], 4), "\n"))
}
```


Make predictions:
```{r}
preds = list()
test = torch_geometric$loader$DataLoader(dataset, batch_size=64L,shuffle=FALSE)
iterator = reticulate::as_iterator(test)
model$eval()
counter = 1
coro::loop(for (b in iterator) {
  preds[[counter]] = model(b$x, b$edge_index, b$batch)
  counter <<- counter + 1
  })
head(torch$concat(preds)$sigmoid()$data$cpu()$numpy(), n = 3)
```

